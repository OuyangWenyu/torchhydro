{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#torchhydro","title":"Torchhydro","text":"<ul> <li>License: BSD license</li> <li>Documentation: https://OuyangWenyu.github.io/torchhydro</li> </ul> <p><code>torchhydro</code> provides datasets and models for applying deep learning to hydrological modeling.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#for-users","title":"For Users","text":"<p>You can install <code>torchhydro</code> using <code>pip</code> or <code>uv</code> (which is faster).</p> <p><pre><code>pip install torchhydro\n</code></pre> or <pre><code>uv pip install torchhydro\n</code></pre></p>"},{"location":"#for-developers","title":"For Developers","text":"<p>If you want to contribute to the project, we recommend using <code>uv</code> for environment management.</p> <pre><code># Clone the repository\ngit clone https://github.com/OuyangWenyu/torchhydro.git\ncd torchhydro\n\n# Create a virtual environment and install all dependencies\nuv sync --all-extras\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"#1-configure-data-path","title":"1. Configure Data Path","text":"<p>Before running any examples, you need to tell <code>torchhydro</code> where your data is located.</p> <p>Create a file named <code>hydro_setting.yml</code> in your user home directory (<code>C:\\Users\\YourUsername</code> on Windows or <code>~/</code> on Linux/macOS). Then, add the following content, pointing to your data folders:</p> <pre><code>local_data_path:\n  root: 'D:/data/waterism' # Update with your root data directory\n  datasets-origin: 'D:/data/waterism/datasets-origin'\n  datasets-interim: 'D:/data/waterism/datasets-interim'\n  cache: 'D:/data/waterism/cache'\n</code></pre> <p>The examples use the CAMELS dataset. If you don't have it, <code>torchhydro</code> will automatically call hydrodataset to download it.</p>"},{"location":"#2-run-examples","title":"2. Run Examples","text":"<p>We provide standalone scripts in the <code>examples/</code> directory to help you get started.</p> <ul> <li><code>examples/lstm_camels_example.py</code>: A basic example of training a standard LSTM model on the CAMELS dataset.</li> <li><code>examples/dpl_xaj_example.py</code>: An advanced example of training a differentiable model based on the Xinanjiang (XAJ) hydrological model.</li> </ul> <p>To run an example: <pre><code>python examples/lstm_camels_example.py\n</code></pre></p> <p>Feel free to modify these scripts to experiment with different models, datasets, and parameters.</p>"},{"location":"#explore-more-features","title":"Explore More Features","text":"<p>The examples above cover two primary use cases, but <code>torchhydro</code> is much more flexible. We support a variety of models, datasets, and data sources out of the box. Explore the full public API to see all available components:</p> <ul> <li>Models API: Discover all available model architectures.</li> <li>Datasets API: See all dataset classes, data sources, and samplers.</li> <li>Trainers API: Understand the core training and evaluation pipeline.</li> </ul> <p>We are continuously working to expand the documentation with more examples.</p>"},{"location":"#main-modules","title":"Main Modules","text":"<p>The project is organized into several key modules:</p> <ul> <li>Trainers: Manages the end-to-end training and evaluation pipeline. The core <code>DeepHydro</code> class handles data loading, model initialization, training loops, and evaluation. It is designed to be extensible for various learning paradigms like transfer learning or multi-task learning.</li> <li>Models: Contains all available model architectures, including standard neural networks (e.g., LSTM) and differentiable models. A central dictionary allows for easy configuration and selection of models and loss functions.</li> <li>Datasets: Provides data handling capabilities. It interfaces with data source libraries like hydrodataset (for public datasets like CAMELS) and hydrodatasource (for custom data) to create <code>torch.utils.data.Dataset</code> objects suitable for training.</li> <li>Configs: Manages all experiment configurations, including settings for the model, data (time periods, variables), training (epochs, batch size), and evaluation.</li> </ul>"},{"location":"#why-torchhydro","title":"Why Torchhydro?","text":"<p>While mature tools like NeuralHydrology exist, <code>torchhydro</code> was developed with a different architectural philosophy:</p> <ol> <li>Decoupled Data Sources: We believe data handling, especially for complex or private datasets, requires a separate abstraction layer. Our approach uses <code>hydrodataset</code> and <code>hydrodatasource</code> to manage data access first, which then feeds into a PyTorch <code>Dataset</code>. This modularity promotes code reuse and allows the data source tools to be used even without a deep learning model.</li> <li>Flexible Learning Paradigms: The framework is explicitly designed to support not just standard supervised learning, but also more complex modes like transfer learning, multi-task learning, and federated learning from the ground up.</li> <li>Deep Configuration: We provide fine-grained control over many aspects of the pipeline, including data traversal, normalization methods, batch sampling strategies, and advanced dropout techniques, allowing for greater flexibility in experimentation.</li> <li>Extensibility: The core design principle is to externalize as much configuration as possible, enabling flexible matching and calling of different data sources and models.</li> </ol>"},{"location":"#additional-information","title":"Additional Information","text":"<p>This package was inspired by:</p> <ul> <li>TorchGeo</li> <li>NeuralHydrology</li> <li>hydroDL</li> </ul> <p>This package was created using the Cookiecutter and the giswqs/pypackage project template.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v008-2025-08-19-203429","title":"v0.0.8 - 2025-08-19 20:34:29","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/OuyangWenyu/torchhydro/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>torchhydro could always use more documentation, whether as part of the official torchhydro docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/OuyangWenyu/torchhydro/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up torchhydro for local development.</p> <ol> <li> <p>Fork the torchhydro repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/torchhydro.git\n</code></pre> </li> <li> <p>Install your local copy into a conda virtual environment.      this is how you set up your fork for local development:</p> <pre><code>$ conda create -n torchhydro\n$ conda activate torchhydro\n$ conda install  -c pytorch pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=10.2\n$ conda install -c conda-forge mamba\n$ conda install -c pyg pytorch-scatter\n$ mamba install -c conda-forge numpy xarray netcdf4 geopandas scikit-learn tensorboard tqdm pytest black flake8 pip\n$ pip install tbparse setuptools wheel twine hydroutils hydrodataset \n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 torchhydro tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and     for PyPy. Check https://github.com/OuyangWenyu/torchhydro/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"fabric_debug_guide/","title":"Lightning Fabric \u8c03\u8bd5\u4e0e\u5206\u5e03\u5f0f\u8bad\u7ec3\u6307\u5357","text":""},{"location":"fabric_debug_guide/#_1","title":"\u95ee\u9898\u63cf\u8ff0","text":"<p>\u5728\u4f7f\u7528 Lightning Fabric \u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u65f6\uff0c\u60a8\u53ef\u80fd\u4f1a\u9047\u5230\u4ee5\u4e0b\u95ee\u9898\uff1a</p> <ol> <li>\u8c03\u8bd5\u56f0\u96be\uff1a\u5206\u5e03\u5f0f\u8bad\u7ec3\u4f1a\u589e\u52a0\u8c03\u8bd5\u7684\u590d\u6742\u6027</li> <li>\u5f00\u53d1\u6548\u7387\u4f4e\uff1a\u6bcf\u6b21\u4fee\u6539\u4ee3\u7801\u90fd\u9700\u8981\u542f\u52a8\u5206\u5e03\u5f0f\u8bad\u7ec3</li> <li>\u7075\u6d3b\u6027\u4e0d\u8db3\uff1a\u65e0\u6cd5\u8f7b\u677e\u5730\u5728\u5355\u673a\u8c03\u8bd5\u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e4b\u95f4\u5207\u6362</li> </ol>"},{"location":"fabric_debug_guide/#_2","title":"\u89e3\u51b3\u65b9\u6848","text":"<p>\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684 <code>FabricWrapper</code> \u7c7b\uff0c\u8ba9\u60a8\u53ef\u4ee5\u5728\u8c03\u8bd5\u6a21\u5f0f\u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u6a21\u5f0f\u4e4b\u95f4\u65e0\u7f1d\u5207\u6362\u3002</p>"},{"location":"fabric_debug_guide/#_3","title":"\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"fabric_debug_guide/#1","title":"1. \u914d\u7f6e\u8c03\u8bd5\u6a21\u5f0f","text":"<pre><code># \u5728\u60a8\u7684\u8bad\u7ec3\u914d\u7f6e\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u53c2\u6570\nconfig_data[\"training_cfgs\"][\"debug_mode\"] = True\nconfig_data[\"training_cfgs\"][\"use_fabric\"] = False\n</code></pre>"},{"location":"fabric_debug_guide/#2","title":"2. \u914d\u7f6e\u5206\u5e03\u5f0f\u8bad\u7ec3\u6a21\u5f0f","text":"<pre><code># \u5728\u60a8\u7684\u8bad\u7ec3\u914d\u7f6e\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u53c2\u6570\nconfig_data[\"training_cfgs\"][\"debug_mode\"] = False\nconfig_data[\"training_cfgs\"][\"use_fabric\"] = True\nconfig_data[\"training_cfgs\"][\"force_fabric\"] = True\n</code></pre>"},{"location":"fabric_debug_guide/#3","title":"3. \u81ea\u52a8\u6a21\u5f0f","text":"<p>\u7cfb\u7edf\u4f1a\u6839\u636e\u4ee5\u4e0b\u6761\u4ef6\u81ea\u52a8\u9009\u62e9\u6a21\u5f0f\uff1a</p> <ul> <li>\u5982\u679c\u53ea\u6709\u4e00\u4e2a GPU\uff0c\u5efa\u8bae\u4f7f\u7528\u8c03\u8bd5\u6a21\u5f0f</li> <li>\u5982\u679c\u6709\u591a\u4e2a GPU\uff0c\u5efa\u8bae\u4f7f\u7528\u5206\u5e03\u5f0f\u8bad\u7ec3\u6a21\u5f0f</li> <li>\u5982\u679c\u8bbe\u7f6e\u4e86 <code>DEBUG_MODE=true</code> \u73af\u5883\u53d8\u91cf\uff0c\u5f3a\u5236\u4f7f\u7528\u8c03\u8bd5\u6a21\u5f0f</li> </ul>"},{"location":"fabric_debug_guide/#_4","title":"\u4e3b\u8981\u7279\u6027","text":""},{"location":"fabric_debug_guide/#debug_modetrue","title":"\u8c03\u8bd5\u6a21\u5f0f (debug_mode=True)","text":"<ul> <li>\u2705 \u4f7f\u7528\u666e\u901a\u7684 PyTorch \u64cd\u4f5c</li> <li>\u2705 \u652f\u6301\u65ad\u70b9\u8c03\u8bd5</li> <li>\u2705 \u7b80\u5355\u7684\u9519\u8bef\u4fe1\u606f</li> <li>\u2705 \u5feb\u901f\u542f\u52a8</li> <li>\u2705 \u5355GPU\u8bad\u7ec3</li> </ul>"},{"location":"fabric_debug_guide/#use_fabrictrue","title":"\u5206\u5e03\u5f0f\u8bad\u7ec3\u6a21\u5f0f (use_fabric=True)","text":"<ul> <li>\u2705 \u4f7f\u7528 Lightning Fabric</li> <li>\u2705 \u591aGPU\u5e76\u884c\u8bad\u7ec3</li> <li>\u2705 \u81ea\u52a8\u5904\u7406\u5206\u5e03\u5f0f\u7ec6\u8282</li> <li>\u2705 \u66f4\u597d\u7684\u6027\u80fd</li> <li>\u2705 \u652f\u6301\u591a\u8282\u70b9\u8bad\u7ec3</li> </ul>"},{"location":"fabric_debug_guide/#_5","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"fabric_debug_guide/#1_1","title":"\u65b9\u5f0f1\uff1a\u901a\u8fc7\u914d\u7f6e\u6587\u4ef6","text":"<pre><code>from torchhydro.configs.config import default_config_file, update_cfg, cmd\nfrom torchhydro.trainers.trainer import train_and_evaluate\n\n# \u521b\u5efa\u8c03\u8bd5\u914d\u7f6e\nconfig_data = default_config_file()\nargs = cmd(\n    sub=\"debug_example\",\n    ctx=[0],  # \u5355GPU\n    model_name=\"LSTM\",\n    # ... \u5176\u4ed6\u53c2\u6570\n)\nupdate_cfg(config_data, args)\n\n# \u542f\u7528\u8c03\u8bd5\u6a21\u5f0f\nconfig_data[\"training_cfgs\"][\"debug_mode\"] = True\nconfig_data[\"training_cfgs\"][\"use_fabric\"] = False\n\n# \u5f00\u59cb\u8bad\u7ec3\ntrain_and_evaluate(config_data)\n</code></pre>"},{"location":"fabric_debug_guide/#2_1","title":"\u65b9\u5f0f2\uff1a\u901a\u8fc7\u73af\u5883\u53d8\u91cf","text":"<pre><code># \u8c03\u8bd5\u6a21\u5f0f\nexport DEBUG_MODE=true\nexport CUDA_VISIBLE_DEVICES=0\npython your_training_script.py\n\n# \u5206\u5e03\u5f0f\u8bad\u7ec3\u6a21\u5f0f\nexport DEBUG_MODE=false\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\npython your_training_script.py\n</code></pre>"},{"location":"fabric_debug_guide/#3_1","title":"\u65b9\u5f0f3\uff1a\u901a\u8fc7\u547d\u4ee4\u884c\u53c2\u6570","text":"<pre><code># \u8c03\u8bd5\u6a21\u5f0f\npython examples/debug_vs_distributed_training.py --mode debug\n\n# \u5206\u5e03\u5f0f\u8bad\u7ec3\u6a21\u5f0f\npython examples/debug_vs_distributed_training.py --mode distributed\n\n# \u81ea\u52a8\u6a21\u5f0f\npython examples/debug_vs_distributed_training.py --mode auto\n</code></pre>"},{"location":"fabric_debug_guide/#_6","title":"\u5de5\u4f5c\u6d41\u7a0b\u5efa\u8bae","text":""},{"location":"fabric_debug_guide/#_7","title":"\u5f00\u53d1\u9636\u6bb5","text":"<ol> <li>\u4f7f\u7528\u8c03\u8bd5\u6a21\u5f0f\u8fdb\u884c\u4ee3\u7801\u5f00\u53d1\u548c\u8c03\u8bd5</li> <li>\u4f7f\u7528\u5c11\u91cf\u6570\u636e\u548c\u5c11\u91cfepochs\u8fdb\u884c\u5feb\u901f\u9a8c\u8bc1</li> <li>\u4f7f\u7528\u65ad\u70b9\u548cprint\u8bed\u53e5\u8fdb\u884c\u8c03\u8bd5</li> </ol> <pre><code># \u8c03\u8bd5\u914d\u7f6e\u793a\u4f8b\nconfig_data[\"training_cfgs\"][\"debug_mode\"] = True\nconfig_data[\"training_cfgs\"][\"use_fabric\"] = False\nconfig_data[\"training_cfgs\"][\"epochs\"] = 5  # \u5c11\u91cfepochs\nconfig_data[\"data_cfgs\"][\"batch_size\"] = 32  # \u5c0fbatch size\n</code></pre>"},{"location":"fabric_debug_guide/#_8","title":"\u9a8c\u8bc1\u9636\u6bb5","text":"<ol> <li>\u4f7f\u7528\u5355GPU\u8fdb\u884c\u4e2d\u7b49\u89c4\u6a21\u7684\u9a8c\u8bc1</li> <li>\u786e\u4fdd\u6a21\u578b\u6b63\u5e38\u5de5\u4f5c</li> <li>\u68c0\u67e5\u6027\u80fd\u6307\u6807</li> </ol>"},{"location":"fabric_debug_guide/#_9","title":"\u751f\u4ea7\u9636\u6bb5","text":"<ol> <li>\u5207\u6362\u5230\u5206\u5e03\u5f0f\u8bad\u7ec3\u6a21\u5f0f</li> <li>\u4f7f\u7528\u591aGPU\u8fdb\u884c\u5927\u89c4\u6a21\u8bad\u7ec3</li> <li>\u4f7f\u7528\u5b8c\u6574\u7684\u6570\u636e\u96c6\u548cepochs</li> </ol> <pre><code># \u751f\u4ea7\u914d\u7f6e\u793a\u4f8b\nconfig_data[\"training_cfgs\"][\"debug_mode\"] = False\nconfig_data[\"training_cfgs\"][\"use_fabric\"] = True\nconfig_data[\"training_cfgs\"][\"force_fabric\"] = True\nconfig_data[\"training_cfgs\"][\"epochs\"] = 100  # \u5b8c\u6574epochs\nconfig_data[\"data_cfgs\"][\"batch_size\"] = 256  # \u5927batch size\n</code></pre>"},{"location":"fabric_debug_guide/#_10","title":"\u914d\u7f6e\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u8bf4\u660e <code>debug_mode</code> bool False \u662f\u5426\u542f\u7528\u8c03\u8bd5\u6a21\u5f0f <code>use_fabric</code> bool True \u662f\u5426\u4f7f\u7528Lightning Fabric <code>force_fabric</code> bool False \u5f3a\u5236\u4f7f\u7528Fabric\uff08\u5373\u4f7f\u5355GPU\uff09 <code>strategy</code> str \"auto\" \u5206\u5e03\u5f0f\u7b56\u7565 (ddp, fsdp, auto) <code>precision</code> str \"32-true\" \u8bad\u7ec3\u7cbe\u5ea6 <code>accelerator</code> str \"auto\" \u52a0\u901f\u5668\u7c7b\u578b"},{"location":"fabric_debug_guide/#_11","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ol> <li>\u8c03\u8bd5\u6a21\u5f0f\u4e0b\u7684\u9650\u5236\uff1a</li> <li>\u53ea\u652f\u6301\u5355GPU\u8bad\u7ec3</li> <li>\u4e0d\u652f\u6301\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3</li> <li> <p>\u4e0d\u652f\u6301\u6a21\u578b\u5e76\u884c</p> </li> <li> <p>\u5206\u5e03\u5f0f\u6a21\u5f0f\u4e0b\u7684\u9650\u5236\uff1a</p> </li> <li>\u8c03\u8bd5\u76f8\u5bf9\u590d\u6742</li> <li>\u542f\u52a8\u65f6\u95f4\u8f83\u957f</li> <li> <p>\u9700\u8981\u66f4\u591a\u5185\u5b58</p> </li> <li> <p>\u8fc1\u79fb\u6ce8\u610f\u4e8b\u9879\uff1a</p> </li> <li>\u73b0\u6709\u4ee3\u7801\u4e2d\u7684 <code>total_fab</code> \u8c03\u7528\u4f1a\u81ea\u52a8\u9002\u914d</li> <li>\u65e0\u9700\u4fee\u6539\u73b0\u6709\u7684\u8bad\u7ec3\u903b\u8f91</li> <li>\u914d\u7f6e\u6587\u4ef6\u5411\u540e\u517c\u5bb9</li> </ol>"},{"location":"fabric_debug_guide/#_12","title":"\u6545\u969c\u6392\u9664","text":""},{"location":"fabric_debug_guide/#_13","title":"\u5e38\u89c1\u95ee\u9898","text":"<ol> <li> <p>\u95ee\u9898\uff1a\u5728\u8c03\u8bd5\u6a21\u5f0f\u4e0b\u51fa\u73b0 \"fabric not found\" \u9519\u8bef    \u89e3\u51b3\uff1a\u68c0\u67e5\u662f\u5426\u6b63\u786e\u8bbe\u7f6e\u4e86 <code>debug_mode=True</code></p> </li> <li> <p>\u95ee\u9898\uff1a\u5206\u5e03\u5f0f\u8bad\u7ec3\u65e0\u6cd5\u542f\u52a8    \u89e3\u51b3\uff1a\u68c0\u67e5 CUDA_VISIBLE_DEVICES \u548c strategy \u8bbe\u7f6e</p> </li> <li> <p>\u95ee\u9898\uff1a\u6a21\u578b\u5728\u4e0d\u540c\u6a21\u5f0f\u4e0b\u8868\u73b0\u4e0d\u4e00\u81f4    \u89e3\u51b3\uff1a\u68c0\u67e5 batch_size \u548c learning_rate \u8bbe\u7f6e</p> </li> </ol>"},{"location":"fabric_debug_guide/#_14","title":"\u65e5\u5fd7\u793a\u4f8b","text":"<pre><code>\ud83d\udc1b Debug mode enabled - disabling Lightning Fabric\n\u2705 Normal PyTorch initialized, using device: cuda:0\n\ud83d\udc1b Debug mode configuration created\n   - Single device: [0]\n   - Lightning Fabric: False\n   - Debug mode: True\n</code></pre> <pre><code>\u2705 Lightning Fabric initialized successfully\n\ud83d\ude80 Distributed training configuration created\n   - Devices: [0, 1]\n   - Strategy: ddp\n   - Lightning Fabric: True\n   - Debug mode: False\n</code></pre>"},{"location":"fabric_debug_guide/#_15","title":"\u603b\u7ed3","text":"<p>\u901a\u8fc7\u4f7f\u7528\u6211\u4eec\u7684 <code>FabricWrapper</code> \u7cfb\u7edf\uff0c\u60a8\u53ef\u4ee5\uff1a</p> <ol> <li>\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff1a\u5728\u8c03\u8bd5\u65f6\u4f7f\u7528\u7b80\u5355\u7684PyTorch\uff0c\u5728\u751f\u4ea7\u65f6\u4f7f\u7528\u5206\u5e03\u5f0f\u8bad\u7ec3</li> <li>\u964d\u4f4e\u8c03\u8bd5\u96be\u5ea6\uff1a\u907f\u514d\u5206\u5e03\u5f0f\u8bad\u7ec3\u5e26\u6765\u7684\u8c03\u8bd5\u590d\u6742\u6027</li> <li>\u4fdd\u6301\u4ee3\u7801\u4e00\u81f4\u6027\uff1a\u65e0\u9700\u4fee\u6539\u73b0\u6709\u4ee3\u7801\uff0c\u53ea\u9700\u66f4\u6539\u914d\u7f6e</li> <li>\u7075\u6d3b\u5207\u6362\uff1a\u6839\u636e\u9700\u8981\u5728\u4e0d\u540c\u6a21\u5f0f\u4e4b\u95f4\u5207\u6362</li> </ol> <p>\u8fd9\u4e2a\u89e3\u51b3\u65b9\u6848\u5b8c\u7f8e\u5730\u5e73\u8861\u4e86\u5f00\u53d1\u6548\u7387\u548c\u8bad\u7ec3\u6027\u80fd\uff0c\u8ba9\u60a8\u80fd\u591f\u4e13\u6ce8\u4e8e\u6a21\u578b\u5f00\u53d1\u800c\u4e0d\u662f\u57fa\u7840\u8bbe\u65bd\u95ee\u9898\u3002 </p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#1-installation-related","title":"1. Installation Related","text":""},{"location":"faq/#q-what-should-i-do-if-i-encounter-cuda-related-errors-during-installation","title":"Q: What should I do if I encounter CUDA-related errors during installation?","text":"<p>A: First, ensure your CUDA version is compatible with your PyTorch version. You can check the compatibility table on the PyTorch website. If issues persist, try installing the corresponding PyTorch version first, then install torchhydro.</p>"},{"location":"faq/#q-is-gpu-required","title":"Q: Is GPU required?","text":"<p>A: No, torchhydro can run on CPU environments, but using a GPU will significantly speed up training.</p>"},{"location":"faq/#2-data-related","title":"2. Data Related","text":""},{"location":"faq/#q-what-data-formats-are-supported","title":"Q: What data formats are supported?","text":"<p>A: Currently, we support CAMELS dataset format, NetCDF format, and common CSV formats. For other formats, you can refer to the documentation to create custom data loaders.</p>"},{"location":"faq/#q-how-to-handle-missing-values","title":"Q: How to handle missing values?","text":"<p>A: torchhydro provides various missing value handling strategies, including mean filling, forward filling, and interpolation. You can use the <code>MissingValueFiller</code> transformer to handle missing values.</p>"},{"location":"faq/#3-model-related","title":"3. Model Related","text":""},{"location":"faq/#q-what-should-i-do-if-the-model-loss-doesnt-converge","title":"Q: What should I do if the model loss doesn't converge?","text":"<p>A: Try the following approaches: - Adjust the learning rate - Check if the data preprocessing is appropriate - Use gradient clipping - Try different model architectures</p>"},{"location":"faq/#q-how-to-choose-the-appropriate-model-architecture","title":"Q: How to choose the appropriate model architecture?","text":"<p>A: It depends on your specific task. LSTM is suitable for time series data, CNN for spatial data, and Transformer excels in long sequence tasks. We recommend starting with simpler models.</p>"},{"location":"faq/#4-performance-related","title":"4. Performance Related","text":""},{"location":"faq/#q-how-to-improve-training-speed","title":"Q: How to improve training speed?","text":"<p>A: Consider: - Using GPU training - Increasing batch size - Using data preloading - Optimizing data preprocessing pipeline</p>"},{"location":"faq/#q-how-to-handle-out-of-memory-issues","title":"Q: How to handle out-of-memory issues?","text":"<p>A: Try: - Reducing batch size - Using data generators - Reducing model complexity - Using gradient accumulation</p>"},{"location":"faq/#5-other-questions","title":"5. Other Questions","text":""},{"location":"faq/#q-how-can-i-contribute-to-the-project","title":"Q: How can I contribute to the project?","text":"<p>A: Please refer to our contribution guide. We welcome all forms of contributions, including code, documentation, and issue reporting.</p>"},{"location":"faq/#q-where-can-i-get-help","title":"Q: Where can I get help?","text":"<p>A: You can: - Check the detailed documentation - Submit an Issue on GitHub - Join our community discussions</p>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.11</li> <li>PyTorch &gt;= 2.6.0</li> <li>CUDA (optional, for GPU acceleration)</li> </ul>"},{"location":"installation/#stable-release","title":"Stable Release","text":"<p>The recommended way to install the latest stable version is using pip:</p> <pre><code>pip install torchhydro\n</code></pre> <p>If you don't have pip installed, you can follow this Python installation guide.</p>"},{"location":"installation/#development-version","title":"Development Version","text":"<p>For the latest development version, you can install directly from the GitHub repository:</p> <pre><code>git clone https://github.com/OuyangWenyu/torchhydro.git\ncd torchhydro\npip install -e .\n</code></pre>"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>For specific functionality, you might need to install the following optional dependencies:</p> <pre><code># For data visualization\npip install matplotlib seaborn\n\n# For data processing\npip install pandas numpy\n\n# For geographic data processing\npip install geopandas rasterio\n</code></pre>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<p>After installation, you can verify it by running the following code in your Python environment:</p> <pre><code>import torchhydro\nprint(torchhydro.__version__)\n</code></pre>"},{"location":"torchhydro/","title":"API Reference","text":""},{"location":"torchhydro/#data-module-torchhydrodata","title":"Data Module (<code>torchhydro.data</code>)","text":""},{"location":"torchhydro/#datasets","title":"Datasets","text":"<ul> <li><code>CamelsDataset</code>: CAMELS dataset loader for hydrological data</li> <li><code>HydroDataset</code>: Base class for hydrological datasets</li> <li><code>TimeSeriesDataset</code>: Generic time series dataset class</li> </ul>"},{"location":"torchhydro/#data-processing","title":"Data Processing","text":"<ul> <li><code>DataProcessor</code>: Base class for data processing</li> <li><code>Scaler</code>: Data scaling and normalization utilities</li> <li><code>TimeSeriesProcessor</code>: Time series specific processing utilities</li> </ul>"},{"location":"torchhydro/#models-torchhydromodels","title":"Models (<code>torchhydro.models</code>)","text":""},{"location":"torchhydro/#base-models","title":"Base Models","text":"<ul> <li><code>BaseModel</code>: Abstract base class for all models</li> <li><code>LSTM</code>: Long Short-Term Memory network for time series</li> <li><code>GRU</code>: Gated Recurrent Unit network</li> <li><code>Transformer</code>: Transformer model for hydrological forecasting</li> </ul>"},{"location":"torchhydro/#pre-trained-models","title":"Pre-trained Models","text":"<ul> <li><code>load_pretrained</code>: Load pre-trained model utilities</li> <li><code>save_model</code>: Model saving utilities</li> </ul>"},{"location":"torchhydro/#metrics-torchhydrometrics","title":"Metrics (<code>torchhydro.metrics</code>)","text":""},{"location":"torchhydro/#hydrological-metrics","title":"Hydrological Metrics","text":"<ul> <li><code>NSE</code>: Nash-Sutcliffe Efficiency coefficient</li> <li><code>KGE</code>: Kling-Gupta Efficiency</li> <li><code>RMSE</code>: Root Mean Square Error</li> <li><code>MAE</code>: Mean Absolute Error</li> <li><code>PBias</code>: Percent Bias</li> </ul>"},{"location":"torchhydro/#transforms-torchhydrotransforms","title":"Transforms (<code>torchhydro.transforms</code>)","text":""},{"location":"torchhydro/#data-transformations","title":"Data Transformations","text":"<ul> <li><code>Normalize</code>: Data normalization</li> <li><code>MissingValueFiller</code>: Handle missing values</li> <li><code>TimeSeriesAugmentation</code>: Time series augmentation techniques</li> <li><code>TemporalDownsampling</code>: Temporal resolution adjustment</li> </ul>"},{"location":"torchhydro/#utils-torchhydroutils","title":"Utils (<code>torchhydro.utils</code>)","text":""},{"location":"torchhydro/#utility-functions","title":"Utility Functions","text":"<ul> <li><code>hydro_utils</code>: Common hydrological utilities</li> <li><code>data_utils</code>: Data handling utilities</li> <li><code>plot_utils</code>: Plotting utilities</li> <li><code>time_utils</code>: Time series handling utilities</li> </ul>"},{"location":"torchhydro/#configuration-torchhydroconfig","title":"Configuration (<code>torchhydro.config</code>)","text":""},{"location":"torchhydro/#configuration","title":"Configuration","text":"<ul> <li><code>HydroConfig</code>: Configuration class for model and training</li> <li><code>DataConfig</code>: Configuration for data processing</li> <li><code>TrainingConfig</code>: Training specific configuration</li> </ul>"},{"location":"usage/","title":"Usage Guide","text":"<p>This guide demonstrates how to use the <code>torchhydro</code> library to configure and run hydrological models. The core workflow revolves around defining a set of parameters, updating a default configuration, and launching the training and evaluation process with a single function.</p>"},{"location":"usage/#core-concept","title":"Core Concept","text":"<p>The main workflow consists of three steps:</p> <ol> <li>Define Parameters: You start by defining all your experiment's parameters (like model choice, dataset, variables, and hyperparameters) using the <code>torchhydro.configs.config.cmd</code> function. This creates a parameter object.</li> <li>Update Configuration: The default configuration is loaded using <code>torchhydro.configs.config.default_config_file()</code>. Then, your custom parameters are merged into it using <code>torchhydro.configs.config.update_cfg()</code>.</li> <li>Train and Evaluate: Finally, you pass the consolidated configuration dictionary to the <code>torchhydro.trainers.trainer.train_and_evaluate()</code> function, which handles the entire pipeline: data loading, model building, training, and evaluation.</li> </ol> <pre><code>from torchhydro.configs.config import cmd, default_config_file, update_cfg\nfrom torchhydro.trainers.trainer import train_and_evaluate\n\n# 1. Define parameters for your experiment\nargs = cmd(...) \n\n# 2. Load default config and update it with your parameters\nconfig_data = default_config_file()\nupdate_cfg(config_data, args)\n\n# 3. Run the training and evaluation pipeline\ntrain_and_evaluate(config_data)\n</code></pre> <p>Below are two practical examples demonstrating this workflow.</p>"},{"location":"usage/#example-1-training-a-standard-lstm-on-camels-data","title":"Example 1: Training a Standard LSTM on CAMELS Data","text":"<p>This example shows how to train a standard LSTM model for streamflow prediction using the CAMELS-US dataset.</p>"},{"location":"usage/#step-1-define-parameters","title":"Step 1: Define Parameters","text":"<p>First, we define all the necessary parameters for our experiment. This includes data source, model type, variables, time periods, and training settings.</p> <pre><code>import os\nfrom torchhydro.configs.config import cmd\nfrom torchhydro import SETTING\n\n# It's recommended to set your local data path in SETTING\n# For example: SETTING[\"local_data_path\"][\"datasets-origin\"] = \"/path/to/your/data\"\nsource_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\n\nargs = cmd(\n    # Experiment name and output directory\n    sub=\"test_camels/exp1\",\n\n    # Data source configuration\n    source_cfgs={\n        \"source_name\": \"camels_us\",\n        \"source_path\": source_path,\n    },\n\n    # Use CPU for this example\n    ctx=[-1],\n\n    # Model selection and hyperparameters\n    model_name=\"CpuLSTM\",\n    model_hyperparam={\n        \"n_input_features\": 23,\n        \"n_output_features\": 1,\n        \"n_hidden_states\": 256,\n    },\n\n    # Basin IDs for training and evaluation\n    gage_id=[\n        \"01013500\", \"01022500\", \"01030500\", \"01031500\", \"01047000\",\n        \"01052500\", \"01054200\", \"01055000\", \"01057000\", \"01170100\",\n    ],\n\n    # Training settings\n    batch_size=8,\n    train_epoch=2,\n    save_epoch=1,\n\n    # Sequence lengths\n    hindcast_length=0,\n    forecast_length=20,\n\n    # Time settings\n    min_time_unit=\"D\",\n    min_time_interval=\"1\",\n\n    # Input and output variables\n    var_t=[\n        \"precipitation\", \"daylight_duration\", \"solar_radiation\",\n        \"temperature_max\", \"temperature_min\", \"vapor_pressure\",\n    ],\n    var_out=[\"streamflow\"],\n\n    # Data components\n    dataset=\"StreamflowDataset\",\n    sampler=\"KuaiSampler\",\n    scaler=\"DapengScaler\",\n\n    # Model loading configuration for evaluation\n    model_loader={\"load_way\": \"specified\", \"test_epoch\": 2},\n\n    # Date ranges for training, validation, and testing\n    train_period=[\"2000-10-01\", \"2001-10-01\"],\n    valid_period=[\"2001-10-01\", \"2002-10-01\"],\n    test_period=[\"2002-10-01\", \"2003-10-01\"],\n\n    # Loss function and optimizer\n    loss_func=\"RMSESum\",\n    opt=\"Adam\",\n    lr_scheduler={0: 1, 1: 0.5, 2: 0.2},\n\n    # Tensor layout\n    which_first_tensor=\"sequence\",\n)\n</code></pre>"},{"location":"usage/#step-2-run-the-pipeline","title":"Step 2: Run the Pipeline","text":"<p>With the parameters defined, we simply call the update and train functions.</p> <pre><code>from torchhydro.configs.config import default_config_file, update_cfg\nfrom torchhydro.trainers.trainer import train_and_evaluate\n\n# Load default config and update it\nconfig_data = default_config_file()\nupdate_cfg(config_data, args)\n\n# Run training and evaluation\ntrain_and_evaluate(config_data)\n</code></pre> <p>The library will now handle data loading, preprocessing, model training, and finally, evaluation on the test set. Results will be saved in the directory specified by the <code>sub</code> parameter (e.g., <code>results/test_camels/exp1</code>).</p>"},{"location":"usage/#example-2-training-a-physics-informed-model-dpl-xaj","title":"Example 2: Training a Physics-Informed Model (DPL-XAJ)","text":"<p>This example demonstrates a more advanced use case: training a Differentiable Parameter Learning (DPL) model. Here, an LSTM is coupled with the Xinanjiang (XAJ) hydrological model. The network learns to output the parameters of the XAJ model.</p>"},{"location":"usage/#step-1-define-dpl-parameters","title":"Step 1: Define DPL Parameters","text":"<p>The parameter definition is similar, but we specify a different model (<code>DplAttrXaj</code>), dataset (<code>DplDataset</code>), and some additional hyperparameters specific to the physics-informed approach.</p> <pre><code>import os\nfrom torchhydro.configs.config import cmd\nfrom hydrodataset.hydro_dataset import StandardVariable\nfrom torchhydro import SETTING\n\nsource_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\n\ndpl_args = cmd(\n    sub=\"test_camels/expdpl001\",\n    source_cfgs={\n        \"source_name\": \"camels_us\",\n        \"source_path\": source_path,\n    },\n    ctx=[0],  # Use GPU 0\n\n    # DPL model selection\n    model_name=\"DplAttrXaj\",\n    model_hyperparam={\n        \"n_input_features\": 17,\n        \"n_output_features\": 15, # Number of XAJ model parameters\n        \"n_hidden_states\": 256,\n        \"kernel_size\": 15,\n        \"warmup_length\": 30, # Warm-up period for the hydrological model\n        \"param_limit_func\": \"clamp\",\n    },\n\n    # DPL models often require a specialized dataset\n    dataset=\"DplDataset\",\n\n    # Use a hybrid loss function for multiple outputs\n    loss_func=\"MultiOutLoss\",\n    loss_param={\n        \"loss_funcs\": \"RMSESum\",\n        \"data_gap\": [0, 0],\n        \"device\": [0],\n        \"item_weight\": [1, 0], # Weight for streamflow vs. other outputs\n        \"limit_part\": [1],\n    },\n\n    # Special scaler settings for physical models\n    scaler=\"DapengScaler\",\n    scaler_params={\n        \"prcp_norm_cols\": [\"streamflow\"],\n        \"gamma_norm_cols\": [\n            StandardVariable.PRECIPITATION,\n            StandardVariable.POTENTIAL_EVAPOTRANSPIRATION,\n        ],\n        \"pbm_norm\": True,\n    },\n\n    gage_id=[\"01013500\", \"01022500\", \"01030500\", \"01031500\"],\n    train_period=[\"1985-10-01\", \"1986-04-01\"],\n    test_period=[\"2000-10-01\", \"2001-10-01\"],\n    valid_period=None,\n\n    batch_size=50,\n    forecast_length=60,\n    warmup_length=30,\n\n    # Input variables for the neural network part\n    var_t=[\n        StandardVariable.PRECIPITATION,\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION,\n    ],\n    # Output variables (streamflow from XAJ, plus a dummy variable)\n    var_out=[StandardVariable.STREAMFLOW, StandardVariable.EVAPOTRANSPIRATION],\n    n_output=2,\n\n    train_epoch=2,\n    opt=\"Adadelta\",\n)\n</code></pre>"},{"location":"usage/#step-2-run-the-dpl-pipeline","title":"Step 2: Run the DPL Pipeline","text":"<p>The execution step is identical.</p> <pre><code>from torchhydro.configs.config import default_config_file, update_cfg\nfrom torchhydro.trainers.trainer import train_and_evaluate\n\n# Load default config and update it\nconfig_data = default_config_file()\nupdate_cfg(config_data, dpl_args)\n\n# Run training and evaluation\ntrain_and_evaluate(config_data)\n</code></pre>"},{"location":"usage/#future-development","title":"Future Development","text":"<p>This guide provides a starting point. <code>torchhydro</code> is designed to be flexible and supports a growing number of models, datasets, and data sources.</p> <ul> <li>The full list of available models can be found in <code>torchhydro/models/model_dict_function.py</code>.</li> <li>The full list of available dataset types is in <code>torchhydro/datasets/data_dict.py</code>.</li> <li>The supported data sources are defined in <code>torchhydro/datasets/data_sources.py</code>.</li> </ul> <p>Future versions of this documentation will provide a comprehensive API reference and more detailed examples for all supported components.</p>"},{"location":"api/configs/","title":"Configs API","text":""},{"location":"api/configs/#torchhydro.configs.config","title":"<code>config</code>","text":"<p>Author: Wenyu Ouyang Date: 2021-12-31 11:08:29 LastEditTime: 2025-10-29 14:34:08 LastEditors: Wenyu Ouyang Description: Config for hydroDL FilePath:       orchhydro       orchhydro\\configs\\config.py Copyright (c) 2021-2022 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/configs/#torchhydro.configs.config.cmd","title":"<code>cmd(project_dir=None, sub=None, source_cfgs=None, scaler=None, scaler_params=None, dataset=None, model_loader=None, variable_length_cfgs=None, sampler=None, fl_sample=None, fl_num_users=None, fl_local_ep=None, fl_local_bs=None, fl_frac=None, master_addr=None, port=None, ctx=None, rs=None, gage_id_file=None, gage_id=None, train_period=None, valid_period=None, test_period=None, opt=None, lr_scheduler=None, opt_param=None, batch_size=None, warmup_length=None, forecast_history=None, hindcast_length=None, forecast_length=None, lead_time_type=None, lead_time_start=None, train_mode=None, train_epoch=None, save_epoch=None, save_iter=None, model_type=None, model_name=None, weight_path=None, continue_train=None, var_c=None, c_rm_nan=None, var_t=None, t_rm_nan=None, n_output=None, loss_func=None, model_hyperparam=None, dropout=None, weight_path_add=None, var_t_type=None, var_f=None, f_rm_nan=None, feature_mapping=None, var_g=None, var_out=None, var_to_source_map=None, out_rm_nan=None, target_as_input=None, constant_only=None, gage_id_screen=None, loss_param=None, metrics=None, fill_nan=None, explainer=None, rolling=None, hrwin=None, frwin=None, calc_metrics=None, start_epoch=None, stat_dict_file=None, num_workers=None, pin_memory=None, which_first_tensor=None, ensemble=None, ensemble_items=None, early_stopping=None, patience=None, min_time_unit=None, min_time_interval=None, valid_batch_mode=None, evaluator=None, fabric_strategy=None, station_cfgs=None)</code>","text":"<p>input args from cmd</p> Source code in <code>torchhydro/configs/config.py</code> <pre><code>def cmd(\n    project_dir: Optional[str] = None,\n    sub: Optional[str] = None,\n    source_cfgs: Optional[Dict[str, Any]] = None,\n    scaler: Optional[str] = None,\n    scaler_params: Optional[Dict[str, Any]] = None,\n    dataset: Optional[str] = None,\n    model_loader: Optional[int] = None,\n    variable_length_cfgs: Optional[Dict[str, Any]] = None,\n    sampler: Optional[str] = None,\n    fl_sample: Optional[str] = None,\n    fl_num_users: Optional[int] = None,\n    fl_local_ep: Optional[int] = None,\n    fl_local_bs: Optional[float] = None,\n    fl_frac: Optional[float] = None,\n    master_addr: Optional[List[str]] = None,\n    port: Optional[List[str]] = None,\n    ctx: Optional[List[str]] = None,\n    rs: Optional[int] = None,\n    gage_id_file: Optional[str] = None,\n    gage_id: Optional[List[str]] = None,\n    train_period: Optional[List[str]] = None,\n    valid_period: Optional[List[str]] = None,\n    test_period: Optional[List[str]] = None,\n    opt: Optional[str] = None,\n    lr_scheduler: Optional[Dict[str, Any]] = None,\n    opt_param: Optional[Dict[str, Any]] = None,\n    batch_size: Optional[int] = None,\n    warmup_length: Optional[int] = None,\n    # forecast_history will be deprecated in the future\n    forecast_history: Optional[int] = None,\n    hindcast_length: Optional[int] = None,\n    forecast_length: Optional[int] = None,\n    lead_time_type: Optional[str] = None,\n    lead_time_start: Optional[int] = None,\n    train_mode: Optional[int] = None,\n    train_epoch: Optional[int] = None,\n    save_epoch: Optional[int] = None,\n    save_iter: Optional[int] = None,\n    model_type: Optional[str] = None,\n    model_name: Optional[str] = None,\n    weight_path: Optional[str] = None,\n    continue_train: Optional[int] = None,\n    var_c: Optional[List[str]] = None,\n    c_rm_nan: Optional[int] = None,\n    var_t: Optional[List[str]] = None,\n    t_rm_nan: Optional[int] = None,\n    n_output: Optional[int] = None,\n    loss_func: Optional[str] = None,\n    model_hyperparam: Optional[Dict[str, Any]] = None,\n    dropout: Optional[float] = None,\n    weight_path_add: Optional[Dict[str, Any]] = None,\n    var_t_type: Optional[List[str]] = None,\n    var_f: Optional[Dict[str, Any]] = None,\n    f_rm_nan: Optional[int] = None,\n    feature_mapping: Optional[Dict[str, Any]] = None,\n    var_g: Optional[Dict[str, Any]] = None,\n    var_out: Optional[List[str]] = None,\n    var_to_source_map: Optional[Dict[str, Any]] = None,\n    out_rm_nan: Optional[int] = None,\n    target_as_input: Optional[int] = None,\n    constant_only: Optional[int] = None,\n    gage_id_screen: Optional[Dict[str, Any]] = None,\n    loss_param: Optional[Dict[str, Any]] = None,\n    metrics: Optional[List[str]] = None,\n    fill_nan: Optional[List[str]] = None,\n    explainer: Optional[List[str]] = None,\n    rolling: Optional[int] = None,\n    hrwin: Optional[int] = None,\n    frwin: Optional[int] = None,\n    calc_metrics: Optional[bool] = None,\n    start_epoch: Optional[int] = None,\n    stat_dict_file: Optional[str] = None,\n    num_workers: Optional[int] = None,\n    pin_memory: Optional[bool] = None,\n    which_first_tensor: Optional[str] = None,\n    ensemble: Optional[int] = None,\n    ensemble_items: Optional[Dict[str, Any]] = None,\n    early_stopping: Optional[bool] = None,\n    patience: Optional[int] = None,\n    min_time_unit: Optional[str] = None,\n    min_time_interval: Optional[int] = None,\n    valid_batch_mode: Optional[str] = None,\n    evaluator: Optional[Dict[str, Any]] = None,\n    fabric_strategy: Optional[str] = None,\n    # station data configurations for GNN models\n    station_cfgs: Optional[Dict[str, Any]] = None,\n) -&gt; argparse.Namespace:\n    \"\"\"input args from cmd\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Train a Time-Series Deep Learning Model for Basins\"\n    )\n    parser.add_argument(\n        \"--project_dir\",\n        dest=\"project_dir\",\n        help=\"the project directory where you put your results in\",\n        default=project_dir,\n        type=str,\n    )\n    parser.add_argument(\n        \"--sub\", dest=\"sub\", help=\"subset and sub experiment\", default=sub, type=str\n    )\n    parser.add_argument(\n        \"--source_cfgs\",\n        dest=\"source_cfgs\",\n        help=\"configs for data sources\",\n        default=source_cfgs,\n        type=json.loads,\n    )\n    parser.add_argument(\n        \"--scaler\",\n        dest=\"scaler\",\n        help=\"Choose a Scaler function\",\n        default=scaler,\n        type=str,\n    )\n    parser.add_argument(\n        \"--scaler_params\",\n        dest=\"scaler_params\",\n        help=\"Parameters of the chosen Scaler function\",\n        default=scaler_params,\n        type=json.loads,\n    )\n    parser.add_argument(\n        \"--dataset\",\n        dest=\"dataset\",\n        help=\"Choose a dataset class for PyTorch\",\n        default=dataset,\n        type=str,\n    )\n    parser.add_argument(\n        \"--sampler\",\n        dest=\"sampler\",\n        help=\"None or KuaiSampler\",\n        default=sampler,\n        type=str,\n    )\n    parser.add_argument(\n        \"--fl_sample\",\n        dest=\"fl_sample\",\n        help=\"sampling method for federated learning\",\n        default=fl_sample,\n        type=str,\n    )\n    parser.add_argument(\n        \"--fl_num_users\",\n        dest=\"fl_num_users\",\n        help=\"number of users for federated learning\",\n        default=fl_num_users,\n        type=int,\n    )\n    parser.add_argument(\n        \"--fl_local_ep\",\n        dest=\"fl_local_ep\",\n        help=\"number of local epochs for federated learning\",\n        default=fl_local_ep,\n        type=int,\n    )\n    parser.add_argument(\n        \"--fl_local_bs\",\n        dest=\"fl_local_bs\",\n        help=\"local batch size for federated learning\",\n        default=fl_local_bs,\n        type=float,\n    )\n    parser.add_argument(\n        \"--fl_frac\",\n        dest=\"fl_frac\",\n        help=\"the fraction of clients for federated learning\",\n        default=fl_frac,\n        type=float,\n    )\n    parser.add_argument(\n        \"--master_addr\",\n        dest=\"master_addr\",\n        help=\"Running Context --default is localhost if you only train model on your computer\",\n        default=master_addr,\n        nargs=\"+\",\n    )\n    parser.add_argument(\n        \"--port\",\n        dest=\"port\",\n        help=\"Running Context -- which port do you want to use when using DistributedDataParallel\",\n        default=port,\n        nargs=\"+\",\n    )\n    parser.add_argument(\n        \"--ctx\",\n        dest=\"ctx\",\n        help=\"Running Context -- gpu num or cpu. E.g `--ctx 0 1` means run code in gpu 0 and 1; -1 means cpu\",\n        default=ctx,\n        nargs=\"+\",\n    )\n    parser.add_argument(\"--rs\", dest=\"rs\", help=\"random seed\", default=rs, type=int)\n    # There is something wrong with \"bool\", so I used 1 as True, 0 as False\n    parser.add_argument(\n        \"--train_mode\",\n        dest=\"train_mode\",\n        help=\"If 0, no train or test, just read data; else train + test\",\n        default=train_mode,\n        type=int,\n    )\n    parser.add_argument(\n        \"--train_epoch\",\n        dest=\"train_epoch\",\n        help=\"epoches of training period\",\n        default=train_epoch,\n        type=int,\n    )\n    parser.add_argument(\n        \"--save_epoch\",\n        dest=\"save_epoch\",\n        help=\"save for every save_epoch epoches\",\n        default=save_epoch,\n        type=int,\n    )\n    parser.add_argument(\n        \"--save_iter\",\n        dest=\"save_iter\",\n        help=\"save for every save_iter in save_epoches\",\n        default=save_iter,\n        type=int,\n    )\n    parser.add_argument(\n        \"--loss_func\",\n        dest=\"loss_func\",\n        help=\"choose loss function\",\n        default=loss_func,\n        type=str,\n    )\n    parser.add_argument(\n        \"--loss_param\",\n        dest=\"loss_param\",\n        help=\"choose parameters of loss function\",\n        default=loss_param,\n        type=json.loads,\n    )\n    parser.add_argument(\n        \"--train_period\",\n        dest=\"train_period\",\n        help=\"The training period\",\n        default=train_period,\n        nargs=\"+\",\n    )\n    parser.add_argument(\n        \"--valid_period\",\n        dest=\"valid_period\",\n        help=\"The validating period\",\n        default=valid_period,\n        nargs=\"+\",\n    )\n    parser.add_argument(\n        \"--test_period\",\n        dest=\"test_period\",\n        help=\"The test period\",\n        default=test_period,\n        nargs=\"+\",\n    )\n    parser.add_argument(\n        \"--batch_size\",\n        dest=\"batch_size\",\n        help=\"batch_size\",\n        default=batch_size,\n        type=int,\n    )\n    parser.add_argument(\n        \"--dropout\",\n        dest=\"dropout\",\n        help=\"dropout\",\n        default=dropout,\n        type=float,\n    )\n    parser.add_argument(\n        \"--warmup_length\",\n        dest=\"warmup_length\",\n        help=\"Physical hydro models need warmup\",\n        default=warmup_length,\n        type=int,\n    )\n    parser.add_argument(\n        \"--hindcast_length\",\n        dest=\"hindcast_length\",\n        help=\"length of time sequence when training in encoder part, for decoder-only models, hindcast_length=0\",\n        default=hindcast_length,\n        type=int,\n    )\n    parser.add_argument(\n        \"--forecast_history\",\n        dest=\"forecast_history\",\n        help=\"length of time sequence when training in encoder part, for decoder-only models, hindcast_length=0\",\n        default=forecast_history,\n        type=int,\n    )\n    parser.add_argument(\n        \"--forecast_length\",\n        dest=\"forecast_length\",\n        help=\"length of time sequence when training in decoder part\",\n        default=forecast_length,\n        type=int,\n    )\n    parser.add_argument(\n        \"--lead_time_type\",\n        dest=\"lead_time_type\",\n        help=\"fixed or increasing\",\n        default=lead_time_type,\n        type=str,\n    )\n    parser.add_argument(\n        \"--lead_time_start\",\n        dest=\"lead_time_start\",\n        help=\"the start lead time\",\n        default=lead_time_start,\n        type=int,\n    )\n    parser.add_argument(\n        \"--model_type\",\n        dest=\"model_type\",\n        help=\"The type of DL model\",\n        default=model_type,\n        type=str,\n    )\n    parser.add_argument(\n        \"--model_name\",\n        dest=\"model_name\",\n        help=\"The name of DL model. now in the zoo\",\n        default=model_name,\n        type=str,\n    )\n    parser.add_argument(\n        \"--weight_path\",\n        dest=\"weight_path\",\n        help=\"The weights of trained model\",\n        default=weight_path,\n        type=str,\n    )\n    parser.add_argument(\n        \"--weight_path_add\",\n        dest=\"weight_path_add\",\n        help=\"More info about the weights of trained model\",\n        default=weight_path_add,\n        type=json.loads,\n    )\n    parser.add_argument(\n        \"--continue_train\",\n        dest=\"continue_train\",\n        help=\"Continue to train the model from weight_path when continue_train&gt;0\",\n        default=continue_train,\n        type=int,\n    )\n    parser.add_argument(\n        \"--gage_id\",\n        dest=\"gage_id\",\n        help=\"just select some sites\",\n        default=gage_id,\n        nargs=\"+\",\n    )\n    parser.add_argument(\n        \"--gage_id_screen\",\n        dest=\"gage_id_screen\",\n        help=\"the criterion to chose some gages\",\n        default=gage_id_screen,\n        type=json.loads,\n    )\n    parser.add_argument(\n        \"--gage_id_file\",\n        dest=\"gage_id_file\",\n        help=\"choose some sites from a file\",\n        default=gage_id_file,\n        type=str,\n    )\n    parser.add_argument(\n        \"--opt\", dest=\"opt\", help=\"choose an optimizer\", default=opt, type=str\n    )\n    parser.add_argument(\n        \"--opt_param\",\n        dest=\"opt_param\",\n        help=\"the optimizer parameters\",\n        default=opt_param,\n        type=json.loads,\n    )\n    parser.add_argument(\n        \"--var_c\", dest=\"var_c\", help=\"types of attributes\", default=var_c, nargs=\"+\"\n    )\n    parser.add_argument(\n        \"--c_rm_nan\",\n        dest=\"c_rm_nan\",\n        help=\"if true, we remove NaN value for var_c data when scaling\",\n        default=c_rm_nan,\n        type=int,\n    )\n    parser.add_argument(\n        \"--var_t\", dest=\"var_t\", help=\"types of forcing\", default=var_t, nargs=\"+\"\n    )\n    parser.add_argument(\n        \"--t_rm_nan\",\n        dest=\"t_rm_nan\",\n        help=\"if true, we remove NaN value for var_t data when scaling\",\n        default=t_rm_nan,\n        type=int,\n    )\n    parser.add_argument(\n        \"--var_t_type\",\n        dest=\"var_t_type\",\n        help=\"types of forcing data_source\",\n        default=var_t_type,\n        nargs=\"+\",\n    )\n    parser.add_argument(\n        \"--var_f\",\n        dest=\"var_f\",\n        help=\"forecast variables such as precipitation from GFS\",\n        default=var_f,\n        type=json.loads,\n    )\n    parser.add_argument(\n        \"--feature_mapping\",\n        type=json.loads,\n        help=\"same variables from obs and forecast\",\n        default=feature_mapping,\n    )\n    parser.add_argument(\n        \"--var_g\",\n        dest=\"var_g\",\n        help=\"global variables such as ENSO indicators\",\n        default=var_g,\n        type=json.loads,\n    )\n    parser.add_argument(\n        \"--var_out\", dest=\"var_out\", help=\"type of outputs\", default=var_out, nargs=\"+\"\n    )\n    parser.add_argument(\n        \"--var_to_source_map\",\n        dest=\"var_to_source_map\",\n        help=\"var_to_source_map\",\n        default=var_to_source_map,\n        type=json.loads,\n    )\n    parser.add_argument(\n        \"--out_rm_nan\",\n        dest=\"out_rm_nan\",\n        help=\"if true, we remove NaN value for var_out data when scaling\",\n        default=out_rm_nan,\n        type=int,\n    )\n    parser.add_argument(\n        \"--f_rm_nan\",\n        dest=\"f_rm_nan\",\n        help=\"if true, we remove NaN value for var_f data when scaling\",\n        default=f_rm_nan,\n        type=int,\n    )\n    parser.add_argument(\n        \"--target_as_input\",\n        dest=\"target_as_input\",\n        help=\"if true, we will use target data as input for data assimilation or physics-based models\",\n        default=target_as_input,\n        type=int,\n    )\n    parser.add_argument(\n        \"--constant_only\",\n        dest=\"constant_only\",\n        help=\"if true, we will only use attribute data as input for deep learning models; \"\n        \"now it is only for dpl models and it is only used when target_as_input is False\",\n        default=constant_only,\n        type=int,\n    )\n    parser.add_argument(\n        \"--n_output\",\n        dest=\"n_output\",\n        help=\"the number of output features\",\n        default=n_output,\n        type=int,\n    )\n    parser.add_argument(\n        \"--model_hyperparam\",\n        dest=\"model_hyperparam\",\n        help=\"the model_hyperparam in model_cfgs\",\n        default=model_hyperparam,\n        type=json.loads,\n    )\n    parser.add_argument(\n        \"--metrics\",\n        dest=\"metrics\",\n        help=\"The evaluating metrics\",\n        default=metrics,\n        nargs=\"+\",\n    )\n    parser.add_argument(\n        \"--fill_nan\",\n        dest=\"fill_nan\",\n        help=\"how to fill nan values when evaluating\",\n        default=fill_nan,\n        nargs=\"+\",\n    )\n    parser.add_argument(\n        \"--explainer\",\n        dest=\"explainer\",\n        help=\"explainer what when evaluating\",\n        default=explainer,\n        nargs=\"+\",\n    )\n    parser.add_argument(\n        \"--rolling\",\n        dest=\"rolling\",\n        help=\"0 means no rolling; rolling&gt;0, such as 1, means perform forecasting once after 1 period. For example, at 8:00am we perform one forecasting and our time-step is 3h, rolling=1 means 11:00, 14:00, 17:00 ..., we will perform forecasting\",\n        default=rolling,\n        type=int,\n    )\n    parser.add_argument(\n        \"--hrwin\",\n        dest=\"hrwin\",\n        help=\"hrwin\",\n        default=hrwin,\n        type=int,\n    )\n    parser.add_argument(\n        \"--frwin\",\n        dest=\"frwin\",\n        help=\"frwin\",\n        default=frwin,\n        type=int,\n    )\n    parser.add_argument(\n        \"--model_loader\",\n        dest=\"model_loader\",\n        help=\"the way to load weights of trained model\",\n        default=model_loader,\n        type=int,\n    )\n    parser.add_argument(\n        \"--variable_length_cfgs\",\n        dest=\"variable_length_cfgs\",\n        help=\"variable length training configuration\",\n        default=variable_length_cfgs,\n        type=json.loads,\n    )\n    parser.add_argument(\n        \"--calc_metrics\",\n        dest=\"calc_metrics\",\n        help=\"if False, calculate valid loss only\",\n        default=calc_metrics,\n        type=bool,\n    )\n    parser.add_argument(\n        \"--start_epoch\",\n        dest=\"start_epoch\",\n        help=\"The index of epoch when starting training, default is 1. \"\n        \"When retraining after an interrupt, it will be larger than 1\",\n        default=start_epoch,\n        type=int,\n    )\n    parser.add_argument(\n        \"--stat_dict_file\",\n        dest=\"stat_dict_file\",\n        help=\"for testing sometimes such as pub cases, we need stat_dict_file from trained dataset\",\n        default=stat_dict_file,\n        type=str,\n    )\n    parser.add_argument(\n        \"--num_workers\",\n        dest=\"num_workers\",\n        help=\"The number of workers used in Dataloader\",\n        default=num_workers,\n        type=int,\n    )\n    parser.add_argument(\n        \"--pin_memory\",\n        dest=\"pin_memory\",\n        help=\"use pin_memory parameter in Pytorch DataLoader of not\",\n        default=pin_memory,\n        type=bool,\n    )\n    parser.add_argument(\n        \"--which_first_tensor\",\n        dest=\"which_first_tensor\",\n        help=\"sequence_first or batch_first\",\n        default=which_first_tensor,\n        type=str,\n    )\n    parser.add_argument(\n        \"--lr_scheduler\",\n        dest=\"lr_scheduler\",\n        help=\"The learning rate scheduler\",\n        default=lr_scheduler,\n        type=json.loads,\n    )\n    parser.add_argument(\n        \"--ensemble\",\n        dest=\"ensemble\",\n        help=\"ensemble config\",\n        default=ensemble,\n        type=int,\n    )\n    parser.add_argument(\n        \"--ensemble_items\",\n        dest=\"ensemble_items\",\n        help=\"ensemble config\",\n        default=ensemble_items,\n        type=json.loads,\n    )\n    parser.add_argument(\n        \"--early_stopping\",\n        dest=\"early_stopping\",\n        help=\"early_stopping config\",\n        default=early_stopping,\n        type=bool,\n    )\n    parser.add_argument(\n        \"--patience\",\n        dest=\"patience\",\n        help=\"patience config\",\n        default=patience,\n        type=int,\n    )\n    parser.add_argument(\n        \"--min_time_unit\",\n        dest=\"min_time_unit\",\n        help=\"The min time type of the input data\",\n        default=min_time_unit,\n        type=str,\n    )\n    parser.add_argument(\n        \"--min_time_interval\",\n        dest=\"min_time_interval\",\n        help=\"The min time interval of the input data\",\n        default=min_time_interval,\n        type=int,\n    )\n    parser.add_argument(\n        \"--valid_batch_mode\",\n        dest=\"valid_batch_mode\",\n        help=\"The batch organization mode of valid data, train means same as train; test means same as test\",\n        default=valid_batch_mode,\n    )\n    parser.add_argument(\n        \"--evaluator\",\n        dest=\"evaluator\",\n        help=\"evaluation way\",\n        default=evaluator,\n        type=json.loads,\n    )\n    parser.add_argument(\n        \"--fabric_strategy\",\n        dest=\"fabric_strategy\",\n        help=\"fabric strategy\",\n        default=fabric_strategy,\n        type=str,\n    )\n    # Station data configurations for GNN models\n    parser.add_argument(\n        \"--station_cfgs\",\n        dest=\"station_cfgs\",\n        help=\"station data configurations for GNN models (as JSON)\",\n        default=station_cfgs,\n        type=json.loads,\n    )\n    # To make pytest work in PyCharm, here we use the following code instead of \"args = parser.parse_args()\":\n    # https://blog.csdn.net/u014742995/article/details/100119905\n    args, unknown = parser.parse_known_args()\n    return args\n</code></pre>"},{"location":"api/configs/#torchhydro.configs.config.default_config_file","title":"<code>default_config_file()</code>","text":"<p>Default config file for all models/data/training parameters in this repo</p>"},{"location":"api/configs/#torchhydro.configs.config.default_config_file--returns","title":"Returns","text":"<p>dict     configurations</p> Source code in <code>torchhydro/configs/config.py</code> <pre><code>def default_config_file() -&gt; Dict[str, Any]:\n    \"\"\"\n    Default config file for all models/data/training parameters in this repo\n\n    Returns\n    -------\n    dict\n        configurations\n    \"\"\"\n\n    return {\n        \"model_cfgs\": {\n            # model_type including normal deep learning (Normal), federated learning (FedLearn), transfer learing (TransLearn), multi-task learning (MTL), etc.\n            \"model_type\": \"Normal\",\n            # supported models can be seen in torchhydro/models/model_dict_function.py\n            \"model_name\": \"LSTM\",\n            # the details of model parameters for the \"model_name\" model\n            \"model_hyperparam\": {\n                # &lt;- warmup -&gt;&lt;- hindcast_length -&gt;&lt;- forecast_length -&gt;\n                \"hindcast_length\": 30,\n                \"forecast_length\": 30,\n                # the size of input (feature number)\n                \"input_size\": 24,\n                # the length of output time-sequence (feature number)\n                \"output_size\": 1,\n                \"hidden_size\": 20,\n                \"num_layers\": 1,\n                \"bias\": True,\n                \"batch_size\": 100,\n                \"dropout\": 0.2,\n            },\n            \"weight_path\": None,\n            \"continue_train\": True,\n            # federated learning parameters\n            \"fl_hyperparam\": {\n                # sampling for federated learning\n                \"fl_sample\": \"basin\",\n                # number of users for federated learning\n                # TODO: we don't use this parameter now, but we may use it in the future\n                \"fl_num_users\": 10,\n                # the number of local epochs\n                \"fl_local_ep\": 5,\n                # local batch size\n                \"fl_local_bs\": 6,\n                # the fraction of clients\n                \"fl_frac\": 0.1,\n            },\n            \"tl_hyperparam\": {\n                # part of transfer learning in a model: a list of layers' names, such as [\"lstm\"]\n                \"tl_part\": None,\n            },\n        },\n        \"data_cfgs\": {\n            \"source_cfgs\": {\n                # the name of data source, such as CAMELS\n                \"source_names\": [\"CAMELS\"],\n                \"source_paths\": [\"../../example/camels_us\"],\n            },\n            \"case_dir\": None,\n            # the min time step of the input data\n            \"min_time_unit\": \"D\",\n            # the min time interval of the input data\n            \"min_time_interval\": 1,\n            # modeled objects\n            \"object_ids\": \"ALL\",\n            # modeling time range\n            \"t_range_train\": [\"1992-01-01\", \"1993-01-01\"],\n            \"t_range_valid\": None,\n            \"t_range_test\": [\"1993-01-01\", \"1994-01-01\"],\n            # the output\n            \"target_cols\": [Q_CAMELS_US_NAME],\n            \"target_rm_nan\": False,\n            # only for cases in which target data will be used as input:\n            # data assimilation -- use streamflow from period 0 to t-1 (TODO: not included now)\n            # for physics-based model -- use streamflow to calibrate models\n            \"target_as_input\": False,\n            # the time series input\n            # TODO: now we only support one forcing type\n            \"relevant_types\": [DAYMET_NAME],\n            \"relevant_cols\": [\n                \"dayl\",\n                PRCP_DAYMET_NAME,\n                \"srad\",\n                \"swe\",\n                \"tmax\",\n                \"tmin\",\n                \"vp\",\n            ],\n            \"relevant_rm_nan\": True,\n            # the attribute input\n            \"constant_cols\": [\n                \"elev_mean\",\n                \"slope_mean\",\n                \"area\",\n                \"frac_forest\",\n                \"lai_max\",\n                \"lai_diff\",\n                \"dom_land_cover_frac\",\n                \"dom_land_cover\",\n                \"root_depth_50\",\n                \"soil_depth_statsgo\",\n                \"soil_porosity\",\n                \"soil_conductivity\",\n                \"max_water_content\",\n                \"geol_1st_class\",\n                \"geol_2nd_class\",\n                \"geol_porostiy\",\n                \"geol_permeability\",\n            ],\n            # for forecast variables such as data from GFS\n            # for each period, they have multiple forecast data with different lead time\n            # hence we list them as a seperate type\n            \"forecast_cols\": None,\n            \"forecast_rm_nan\": True,\n            # same variable but different names for obs and forecast\n            # key is obs and value is forecast\n            \"feature_mapping\": {\n                \"total_precipitation_hourly\": \"total_precipitation_surface\",\n            },\n            # global variables such as ENSO indictors are used in some long term models\n            \"global_cols\": None,\n            # specify the data source of each variable\n            \"var_to_source_map\": None,\n            # {\n            #     \"temperature\": \"nldas4camels\",\n            #     \"specific_humidity\": \"nldas4camels\",\n            #     \"usgsFlow\": \"camels_us\",\n            #     \"ET\": \"modiset4camels\",\n            # },\n            \"constant_rm_nan\": True,\n            # if constant_only, we will only use constant data as DL models' input: this is only for dpl models now\n            \"constant_only\": False,\n            # only numerical scaler: for categorical vars, they are transformed to numerical vars when reading them\n            \"scaler\": \"StandardScaler\",\n            # Some parameters for the chosen scaler function, default is DapengScaler's\n            \"scaler_params\": {\n                \"prcp_norm_cols\": [\n                    Q_CAMELS_US_NAME,\n                    \"streamflow\",\n                    Q_CAMELS_CC_NAME,\n                    \"qobs\",\n                    \"qobs_mm_per_hour\",\n                ],\n                \"gamma_norm_cols\": [\n                    StandardVariable.PRECIPITATION,\n                    PRCP_DAYMET_NAME,\n                    \"pr\",\n                    # PRCP_ERA5LAND_NAME is same as PRCP_NLDAS_NAME\n                    PRCP_NLDAS_NAME,\n                    \"pre\",\n                    StandardVariable.POTENTIAL_EVAPOTRANSPIRATION,\n                    StandardVariable.EVAPOTRANSPIRATION,\n                    # pet may be negative, but we set negative as 0 because of gamma_norm_cols\n                    # https://earthscience.stackexchange.com/questions/12031/does-negative-reference-evapotranspiration-make-sense-using-fao-penman-monteith\n                    \"pet\",\n                    # PET_ERA5LAND_NAME is same as PET_NLDAS_NAME\n                    PET_NLDAS_NAME,\n                    ET_MODIS_NAME,\n                    \"LE\",\n                    PET_MODIS_NAME,\n                    \"PLE\",\n                    \"GPP\",\n                    \"Ec\",\n                    \"Es\",\n                    \"Ei\",\n                    \"ET_water\",\n                    \"ET_sum\",\n                    SSM_SMAP_NAME,\n                    \"susm\",\n                    \"smp\",\n                    \"ssma\",\n                    \"susma\",\n                ],\n                # NOTE: pbm_norm is True means norm and denorm for differentiable models; if you use pure data-driven models, you should set it as False\n                \"pbm_norm\": False,\n            },\n            # For scaler from sklearn, we need to specify the stat_dict_file for three different parts:\n            # target_cols, relevant_vars and constant_cols, and the sequence must be target_cols, relevant_cols, constant_cols\n            # the seperator of three stat_dict_file is \";\"\n            # for example: \"stat_dict_file\": \"target_stat_dict_file;relevant_stat_dict_file;constant_stat_dict_file\"\n            \"stat_dict_file\": None,\n            # dataset for pytorch dataset\n            \"dataset\": \"StreamflowDataset\",\n            # sampler for pytorch dataloader, here we mainly use it for Kuai Fang's sampler in all his DL papers\n            \"sampler\": None,\n            # station data configurations for GNN models\n            \"station_cfgs\": {\n                # \u7ad9\u70b9\u6570\u636e\u914d\u7f6e - \u4f7f\u75283h\u6570\u636e\u4e2d\u5b9e\u9645\u5b58\u5728\u7684\u53d8\u91cf\n                \"station_cols\": [\"DRP\"],  # TM=\u6e29\u5ea6, \u4ece\u7ad9\u70b9\u6570\u636e\u4e2d\u9009\u62e9\u53d8\u91cf\n                \"station_rm_nan\": True,\n                \"station_time_units\": [\"3h\"],\n                \"station_scaler_type\": \"DapengScaler\",\n                # \u90bb\u63a5\u77e9\u9635\u914d\u7f6e\n                \"use_adjacency\": True,\n                \"adjacency_src_col\": \"ID\",\n                \"adjacency_dst_col\": \"NEXTDOWNID\",\n                \"adjacency_edge_attr_cols\": [\"dist_hdn\", \"elev_diff\", \"strm_slope\"],\n                \"adjacency_weight_col\": None,  # \u4e0d\u4f7f\u7528\u6743\u91cd\uff0c\u8fd4\u56de\u8fb9\u5c5e\u6027\n                \"return_edge_weight\": False,\n            },\n        },\n        \"training_cfgs\": {\n            # for distributed training, we use lightning fabric, it has some different strategies\n            # you can see the following link for more details:\n            # https://lightning.ai/docs/fabric/stable/api/fabric_args.html#strategy\n            # such as \"dp\", \"ddp\", \"ddp_spawn\", \"ddp_find_unused_parameters_true\", \"xla\", \"deepspeed\", \"fsdp\"\n            # here we use None as default, which means we don't use fabric\n            \"fabric_strategy\": None,\n            \"master_addr\": \"localhost\",\n            \"port\": \"12335\",\n            # if train_mode is False, don't train and evaluate\n            \"train_mode\": True,\n            \"batch_size\": 100,\n            # we generally have three times: [warmup, hindcast_length, forecast_length]\n            # warmup period means no observation will be used to calculate loss for it.\n            # For physics-based models, we generally need warmup to get a better initial state\n            # its default is 0 as DL models generally don't need it\n            \"warmup_length\": 0,\n            # the length of the history data to forecast\n            \"hindcast_length\": 30,\n            # the length of the forecast data\n            \"forecast_length\": 1,\n            # for each batch, we fix length of hindcast and forecast length.\n            # data from different lead time with a number representing the lead time,\n            # for example, now is 2020-09-30, our min_time_interval is 1 day, hindcast length is 30 and forecast length is 1,\n            # lead_time = 3 means 2020-09-01 to 2020-09-30, and the forecast data is 2020-10-01 forecast-performed at 2020-09-28\n            # for forecast data, we have two different configurations:\n            # 1st \"fixed\", we can set a same lead time for all forecast time\n            # 2020-09-30now, 30hindcast, 2forecast, 3leadtime means 2020-09-01 to 2020-09-30 obs concatenate with 2020-10-01 forecast data from 2020-09-28 and 2020-10-02 forecast data from 2020-09-29\n            # 2nd \"increasing\", we can set a increasing lead time for each forecast time\n            # 2020-09-30now, 30hindcast, 2forecast, [1, 2]leadtime means 2020-09-01 to 2020-09-30 obs concatenate with 2020-10-01 to 2010-10-02 forecast data from 2020-09-30\n            \"lead_time_type\": \"fixed\",  # must be fixed or increasing\n            \"lead_time_start\": 1,\n            # Variable length training configuration (unified mask_cfgs and multi_length_training)\n            \"variable_length_cfgs\": {\n                # whether to use variable length training\n                \"use_variable_length\": False,\n                # variable length type:\n                # - \"fixed\": use predefined lengths (replaces old multi_length_training)\n                # - \"dynamic\": automatic padding with mask (replaces old mask_cfgs)\n                \"variable_length_type\": \"dynamic\",\n                # for \"fixed\" type: specify exact sequence lengths to use\n                \"fixed_lengths\": [\n                    365,\n                    1095,\n                    1825,\n                ],\n                # Pad strategy: \"Pad\" or \"multi_table\" (multi_table not fully tested yet)\n                \"pad_strategy\": \"Pad\",\n            },\n            # valid batch can be organized as same way with training or testing\n            \"valid_batch_mode\": \"test\",\n            \"criterion\": \"RMSE\",\n            \"criterion_params\": None,\n            # \"weight_decay\": None, a regularization term in loss func\n            \"optimizer\": \"Adam\",\n            # \"optim_params\": {\"lr\": 0.001} means the initial learning rate is 0.001\n            \"optim_params\": {},\n            \"lr_scheduler\": {\n                # 1st opt config, all epochs use this lr,\n                # this setting will cover the lr setting in \"optim_params\"\n                \"lr\": 0.001,\n                # 2nd opt config, piecewise constant learning rate:\n                # diff epoch uses diff lr, key is epoch, start from 0,\n                # each value means the decay rate. For example,\n                # if initial lr is 0.001, then 1: 0.5 means the lr of 2nd epoch is 0.001*0.5=0.0005\n                # and the next following epoch keep this lr until the next key\n                # \"lr_scheduler\": {0: 1, 1: 0.5, 10: 0.2},\n                # 3rd opt config, initial lr need to be set in \"optim_params\" or it will use default one\n                # lr_factor as an exponential decay factor\n                # \"lr_factor\": 0.1,\n                # 4th opt config, initial lr need to be set in \"optim_params\" or it will use default one\n                # lr_patience represent how many epochs without opt (we watch val_loss) could be tolerated\n                # if lr_patience is satisfied, then lr will be decayed by lr_factor by a linear way\n                # \"lr_factor\": 0.1, \"lr_patience\": 1,\n            },\n            \"early_stopping\": False,\n            \"patience\": 1,\n            \"epochs\": 20,\n            # save_epoch ==0 means only save once in the final epoch\n            \"save_epoch\": 0,\n            # save_iter ==0 means we don't save model during training in a epoch\n            \"save_iter\": 0,\n            # when we train a model for long time, some accidents may interrupt our training.\n            # Then we need retrain the model with saved weights, and the start_epoch is not 1 yet.\n            \"start_epoch\": 1,\n            \"random_seed\": 1234,\n            \"device\": [0, 1, 2],\n            \"multi_targets\": 1,\n            \"num_workers\": 0,\n            \"pin_memory\": False,\n            \"which_first_tensor\": \"sequence\",\n            # if we calculate metrics such as RMSE, NSE, etc. during validation\n            # metrics will be samed as the config in evaluation_cfgs\n            \"calc_metrics\": True,\n            # for ensemble exp:\n            # basically we set kfold/seeds/hyper_params for trianing such as batch_sizes\n            \"ensemble\": False,\n            \"ensemble_items\": {\n                # kfold means a time cross validation,\n                # concatenate train ,valid, and test data together,\n                # then split them into k folds\n                \"kfold\": None,\n                \"batch_sizes\": None,\n                # if seeds is not None,\n                # we will use different seeds for different sub-exps\n                \"seeds\": None,\n                \"patience\": None,\n                \"early_stopping\": None,\n                \"kfold_continuous\": True,\n            },\n        },\n        # For evaluation\n        \"evaluation_cfgs\": {\n            # there are some different loading way of trained model weights\n            # 'epoch' means we load the weights of the specified epoch;\n            # 'best' means we load the best weights during training especially for early stopping\n            # 'latest' means we load the latest weights during training\n            # 'pth' means we load the weights from the specified path\n            \"model_loader\": {\"load_way\": \"specified\", \"test_epoch\": 20},\n            # \"model_loader\": {\"load_way\": \"best\"},\n            # \"model_loader\": {\"load_way\": \"latest\"},\n            # \"model_loader\": {\"load_way\": \"pth\", \"pth_path\": \"path/to/weights\"},\n            \"metrics\": [\"NSE\", \"RMSE\", \"R2\", \"KGE\", \"FHV\", \"FLV\"],\n            \"fill_nan\": \"no\",\n            \"explainer\": None,\n            # rolling is stride, 0 means each period has only one prediction.\n            # when rolling&gt;0, such as 1, means perform forecasting each step after 1 period.\n            # For example, at 8:00am we perform one forecasting and our time-step is 3h,\n            # rolling=1 means 11:00, 14:00, 17:00 ..., we will perform forecasting\n            # when rolling&gt;0, we will perform rolling forecast, for each forecasting,\n            # a rolling window (rwin) can be chosen:\n            # hindcast (hrwin) and forecast (frwin) length in rwin need to be chosen\n            # and then rwin = hrwin + frwin\n            # when rolling is -1, we will use the each flood event sequence as one sample for testing\n            \"rolling\": 0,\n            \"hrwin\": None,\n            \"frwin\": None,\n            # we provide some different evaluators:\n            # 1st -- once: for each time each var and each basin, only one result is evaluated\n            # stride means if rolling is true, after evaluating, we need a stride to skip some periods\n            # 2nd -- 1pace: we only chose one pace from results to evaluate\n            # -1 means we chose the final result of each sample which will be used in hindcast-only/forecast-only model inference\n            # 1 means we chose the first result of each sample which will be used in hindcast-forecast model inference\n            # 3rd -- rolling: we perform evaluation for each sample of each basin,\n            # stride means we will perform evaluation for each sample after stride periods\n            \"evaluator\": {\n                \"eval_way\": \"once\",\n                \"stride\": 0,\n            },\n            # \"evaluator\": {\n            #     \"eval_way\": \"1pace\",\n            #     \"pace_idx\": -1,\n            # },\n            # \"evaluator\": {\n            # \"eval_way\": \"rolling\",\n            # \"stride\": 1,\n            # },\n        },\n    }\n</code></pre>"},{"location":"api/configs/#torchhydro.configs.config.update_cfg","title":"<code>update_cfg(cfg_file, new_args)</code>","text":"<p>Update default config with new arguments</p>"},{"location":"api/configs/#torchhydro.configs.config.update_cfg--parameters","title":"Parameters","text":"<p>cfg_file     default config new_args     new arguments</p>"},{"location":"api/configs/#torchhydro.configs.config.update_cfg--returns","title":"Returns","text":"<p>None     in-place operation for cfg_file</p> Source code in <code>torchhydro/configs/config.py</code> <pre><code>def update_cfg(cfg_file: Dict[str, Any], new_args: argparse.Namespace) -&gt; None:\n    \"\"\"\n    Update default config with new arguments\n\n    Parameters\n    ----------\n    cfg_file\n        default config\n    new_args\n        new arguments\n\n    Returns\n    -------\n    None\n        in-place operation for cfg_file\n    \"\"\"\n    print(\"update config file\")\n    if new_args.project_dir is not None:\n        project_dir = new_args.project_dir\n    else:\n        project_dir = os.getcwd()\n    result_dir = os.path.join(project_dir, \"results\")\n    if os.path.exists(result_dir) is False:\n        os.makedirs(result_dir)\n    if new_args.sub is not None:\n        subset, subexp = new_args.sub.split(os.sep)\n        cfg_file[\"data_cfgs\"][\"case_dir\"] = os.path.join(result_dir, subset, subexp)\n    if new_args.source_cfgs is not None:\n        cfg_file[\"data_cfgs\"][\"source_cfgs\"] = new_args.source_cfgs\n    if new_args.scaler is not None:\n        cfg_file[\"data_cfgs\"][\"scaler\"] = new_args.scaler\n    if new_args.scaler_params is not None:\n        cfg_file[\"data_cfgs\"][\"scaler_params\"] = new_args.scaler_params\n    if new_args.dataset is not None:\n        cfg_file[\"data_cfgs\"][\"dataset\"] = new_args.dataset\n    if new_args.sampler is not None:\n        cfg_file[\"data_cfgs\"][\"sampler\"] = new_args.sampler\n    if new_args.fl_sample is not None:\n        if new_args.fl_sample not in [\"basin\", \"region\"]:\n            # basin means each client is a basin\n            raise ValueError(\"fl_sample must be 'basin' or 'region'\")\n        cfg_file[\"model_cfgs\"][\"fl_hyperparam\"][\"fl_sample\"] = new_args.fl_sample\n    if new_args.fl_num_users is not None:\n        cfg_file[\"model_cfgs\"][\"fl_hyperparam\"][\"fl_num_users\"] = new_args.fl_num_users\n    if new_args.fl_local_ep is not None:\n        cfg_file[\"model_cfgs\"][\"fl_hyperparam\"][\"fl_local_ep\"] = new_args.fl_local_ep\n    if new_args.fl_local_bs is not None:\n        cfg_file[\"model_cfgs\"][\"fl_hyperparam\"][\"fl_local_bs\"] = new_args.fl_local_bs\n    if new_args.fl_frac is not None:\n        cfg_file[\"model_cfgs1\"][\"fl_hyperparam\"][\"fl_frac\"] = new_args.fl_frac\n    if new_args.master_addr is not None:\n        cfg_file[\"training_cfgs\"][\"master_addr\"] = new_args.master_addr\n    if new_args.port is not None:\n        cfg_file[\"training_cfgs\"][\"port\"] = new_args.port\n    if new_args.ctx is not None:\n        cfg_file[\"training_cfgs\"][\"device\"] = new_args.ctx\n    if new_args.rs is not None:\n        cfg_file[\"training_cfgs\"][\"random_seed\"] = new_args.rs\n    if new_args.train_mode is not None:\n        cfg_file[\"training_cfgs\"][\"train_mode\"] = bool(new_args.train_mode &gt; 0)\n    if new_args.loss_func is not None:\n        cfg_file[\"training_cfgs\"][\"criterion\"] = new_args.loss_func\n        if new_args.loss_param is not None:\n            cfg_file[\"training_cfgs\"][\"criterion_params\"] = new_args.loss_param\n    if new_args.train_period is not None:\n        cfg_file[\"data_cfgs\"][\"t_range_train\"] = new_args.train_period\n    if new_args.valid_period is not None:\n        cfg_file[\"data_cfgs\"][\"t_range_valid\"] = new_args.valid_period\n    if new_args.test_period is not None:\n        cfg_file[\"data_cfgs\"][\"t_range_test\"] = new_args.test_period\n    if (\n        new_args.gage_id is not None\n        and new_args.gage_id_file is not None\n        or new_args.gage_id is None\n        and new_args.gage_id_file is not None\n    ):\n        gage_id_lst = (\n            pd.read_csv(new_args.gage_id_file, dtype={0: str}).iloc[:, 0].values\n        )\n        cfg_file[\"data_cfgs\"][\"object_ids\"] = gage_id_lst.tolist()\n    elif new_args.gage_id is not None:\n        cfg_file[\"data_cfgs\"][\"object_ids\"] = new_args.gage_id\n    if new_args.opt is not None:\n        cfg_file[\"training_cfgs\"][\"optimizer\"] = new_args.opt\n        if new_args.opt_param is not None:\n            cfg_file[\"training_cfgs\"][\"optim_params\"] = new_args.opt_param\n        else:\n            cfg_file[\"training_cfgs\"][\"optim_params\"] = {}\n    if new_args.var_c is not None:\n        # I don't find a method to receive empty list for argparse, so if we input \"None\" or \"\" or \" \", we treat it as []\n        if new_args.var_c in [[\"None\"], [\"\"], [\" \"]]:\n            cfg_file[\"data_cfgs\"][\"constant_cols\"] = []\n        else:\n            cfg_file[\"data_cfgs\"][\"constant_cols\"] = new_args.var_c\n    if new_args.c_rm_nan is not None:\n        cfg_file[\"data_cfgs\"][\"constant_rm_nan\"] = bool(new_args.c_rm_nan &gt; 0)\n    if new_args.var_t is not None:\n        cfg_file[\"data_cfgs\"][\"relevant_cols\"] = new_args.var_t\n        print(\n            \"!!!!!!NOTE!!!!!!!!\\n-------Please make sure the PRECIPITATION variable is in the 1st location in var_t setting!!---------\"\n        )\n        print(\"If you have POTENTIAL_EVAPOTRANSPIRATION, please set it the 2nd!!!-\")\n    if new_args.var_t_type is not None:\n        cfg_file[\"data_cfgs\"][\"relevant_types\"] = new_args.var_t_type\n    if new_args.t_rm_nan is not None:\n        cfg_file[\"data_cfgs\"][\"relevant_rm_nan\"] = bool(new_args.t_rm_nan &gt; 0)\n    if new_args.var_f is not None:\n        cfg_file[\"data_cfgs\"][\"forecast_cols\"] = new_args.var_f\n    if new_args.var_g is not None:\n        cfg_file[\"data_cfgs\"][\"global_cols\"] = new_args.var_g\n    if new_args.var_out is not None:\n        cfg_file[\"data_cfgs\"][\"target_cols\"] = new_args.var_out\n        print(\n            \"!!!!!!NOTE!!!!!!!!\\n-------Please make sure the STREAMFLOW variable is in the 1st location in var_out setting!!---------\"\n        )\n    if new_args.var_to_source_map is not None:\n        cfg_file[\"data_cfgs\"][\"var_to_source_map\"] = new_args.var_to_source_map\n    if new_args.out_rm_nan is not None:\n        cfg_file[\"data_cfgs\"][\"target_rm_nan\"] = bool(new_args.out_rm_nan &gt; 0)\n    if new_args.f_rm_nan is not None:\n        cfg_file[\"data_cfgs\"][\"forecast_rm_nan\"] = bool(new_args.f_rm_nan &gt; 0)\n    if new_args.feature_mapping is not None:\n        cfg_file[\"data_cfgs\"][\"feature_mapping\"] = new_args.feature_mapping\n    if new_args.target_as_input is not None:\n        cfg_file[\"data_cfgs\"][\"target_as_input\"] = new_args.target_as_input &gt; 0\n    if new_args.constant_only is not None:\n        cfg_file[\"data_cfgs\"][\"constant_only\"] = new_args.constant_only &gt; 0\n    if new_args.calc_metrics is not None:\n        cfg_file[\"training_cfgs\"][\"calc_metrics\"] = new_args.calc_metrics\n    if new_args.train_epoch is not None:\n        cfg_file[\"training_cfgs\"][\"epochs\"] = new_args.train_epoch\n    if new_args.save_epoch is not None:\n        cfg_file[\"training_cfgs\"][\"save_epoch\"] = new_args.save_epoch\n    if new_args.save_iter is not None:\n        cfg_file[\"training_cfgs\"][\"save_iter\"] = new_args.save_iter\n    if new_args.fabric_strategy is not None:\n        cfg_file[\"training_cfgs\"][\"fabric_strategy\"] = new_args.fabric_strategy\n    if new_args.model_type is not None:\n        cfg_file[\"model_cfgs\"][\"model_type\"] = new_args.model_type\n    if new_args.model_name is not None:\n        cfg_file[\"model_cfgs\"][\"model_name\"] = new_args.model_name\n    if new_args.weight_path is not None:\n        cfg_file[\"model_cfgs\"][\"weight_path\"] = new_args.weight_path\n        continue_train = bool(\n            new_args.continue_train is not None and new_args.continue_train &gt; 0\n        )\n        cfg_file[\"model_cfgs\"][\"continue_train\"] = continue_train\n    if new_args.weight_path_add is not None:\n        cfg_file[\"model_cfgs\"][\"weight_path_add\"] = new_args.weight_path_add\n    if new_args.n_output is not None:\n        cfg_file[\"training_cfgs\"][\"multi_targets\"] = new_args.n_output\n        if len(cfg_file[\"data_cfgs\"][\"target_cols\"]) != new_args.n_output:\n            raise AttributeError(\n                \"Please make sure size of vars in data_cfgs/target_cols is same as n_output\"\n            )\n    if new_args.variable_length_cfgs is not None:\n        cfg_file[\"training_cfgs\"][\n            \"variable_length_cfgs\"\n        ] = new_args.variable_length_cfgs\n\n    if new_args.model_hyperparam is not None:\n        # raise AttributeError(\"Please set the model_hyperparam!!!\")\n        cfg_file[\"model_cfgs\"][\"model_hyperparam\"] = new_args.model_hyperparam\n        if (\n            \"batch_size\" in new_args.model_hyperparam.keys()\n            and new_args.model_hyperparam[\"batch_size\"] != new_args.batch_size\n        ):\n            raise RuntimeError(\n                \"Please set same batch_size in model_cfgs and training_cfgs\"\n            )\n        if (\n            \"forecast_length\" in new_args.model_hyperparam.keys()\n            and new_args.forecast_length != new_args.model_hyperparam[\"forecast_length\"]\n        ):\n            raise RuntimeError(\n                \"Please set same forecast_length in model_cfgs and training_cfgs\"\n            )\n        # The following two configurations are for encoder-decoder models' seq2seqdataset\n        if \"hindcast_output_window\" in new_args.model_hyperparam.keys():\n            cfg_file[\"data_cfgs\"][\"hindcast_output_window\"] = new_args.model_hyperparam[\n                \"hindcast_output_window\"\n            ]\n        else:\n            cfg_file[\"data_cfgs\"][\"hindcast_output_window\"] = 0\n    if new_args.batch_size is not None:\n        cfg_file[\"training_cfgs\"][\"batch_size\"] = new_args.batch_size\n    if new_args.min_time_unit is not None:\n        if new_args.min_time_unit not in [\"h\", \"D\"]:\n            raise ValueError(\"min_time_unit must be 'h' (HOURLY) or 'D' (DAILY)\")\n        cfg_file[\"data_cfgs\"][\"min_time_unit\"] = new_args.min_time_unit\n    if new_args.min_time_interval is not None:\n        cfg_file[\"data_cfgs\"][\"min_time_interval\"] = new_args.min_time_interval\n    if new_args.metrics is not None:\n        cfg_file[\"evaluation_cfgs\"][\"metrics\"] = new_args.metrics\n    if new_args.fill_nan is not None:\n        cfg_file[\"evaluation_cfgs\"][\"fill_nan\"] = new_args.fill_nan\n    if new_args.explainer is not None:\n        cfg_file[\"evaluation_cfgs\"][\"explainer\"] = new_args.explainer\n    if new_args.rolling is not None:\n        cfg_file[\"evaluation_cfgs\"][\"rolling\"] = new_args.rolling\n    if new_args.hrwin is not None:\n        cfg_file[\"evaluation_cfgs\"][\"hrwin\"] = new_args.hrwin\n    if new_args.frwin is not None:\n        cfg_file[\"evaluation_cfgs\"][\"frwin\"] = new_args.frwin\n    if new_args.model_loader is not None:\n        cfg_file[\"evaluation_cfgs\"][\"model_loader\"] = new_args.model_loader\n    if new_args.warmup_length is not None:\n        cfg_file[\"training_cfgs\"][\"warmup_length\"] = new_args.warmup_length\n        if (\n            \"warmup_length\" in new_args.model_hyperparam.keys()\n            and new_args.warmup_length != new_args.model_hyperparam[\"warmup_length\"]\n        ):\n            raise RuntimeError(\n                \"Please set same warmup_length in model_cfgs and data_cfgs\"\n            )\n    if new_args.hindcast_length is not None:\n        cfg_file[\"training_cfgs\"][\"hindcast_length\"] = new_args.hindcast_length\n    if new_args.hindcast_length is None and new_args.forecast_history is not None:\n        # forecast_history will be deprecated in the future\n        warnings.warn(\n            \"forecast_history will be deprecated in the future, please use hindcast_length instead\"\n        )\n        cfg_file[\"training_cfgs\"][\"hindcast_length\"] = new_args.forecast_history\n    if new_args.forecast_length is not None:\n        cfg_file[\"training_cfgs\"][\"forecast_length\"] = new_args.forecast_length\n    if new_args.lead_time_type is not None:\n        if new_args.lead_time_type not in [\"fixed\", \"increasing\"]:\n            raise ValueError(\"lead_time_type must be 'fixed' or 'increasing'\")\n        cfg_file[\"training_cfgs\"][\"lead_time_type\"] = new_args.lead_time_type\n        if new_args.lead_time_start is None:\n            raise ValueError(\"lead_time_start must be set when lead_time_type is set\")\n        cfg_file[\"training_cfgs\"][\"lead_time_start\"] = new_args.lead_time_start\n    if new_args.start_epoch is not None:\n        cfg_file[\"training_cfgs\"][\"start_epoch\"] = new_args.start_epoch\n    if new_args.stat_dict_file is not None:\n        stat_dict_file = new_args.stat_dict_file\n        if len(stat_dict_file.split(\";\")) &gt; 1:\n            target_, relevant_, constant_ = stat_dict_file.split(\";\")\n            stat_dict_file = {\n                \"target_cols\": target_,\n                \"relevant_cols\": relevant_,\n                \"constant_cols\": constant_,\n            }\n        cfg_file[\"data_cfgs\"][\"stat_dict_file\"] = stat_dict_file\n    if new_args.num_workers is not None and new_args.num_workers &gt; 0:\n        cfg_file[\"training_cfgs\"][\"num_workers\"] = new_args.num_workers\n    if new_args.pin_memory is not None:\n        cfg_file[\"training_cfgs\"][\"pin_memory\"] = new_args.pin_memory\n    if new_args.which_first_tensor is not None:\n        cfg_file[\"training_cfgs\"][\"which_first_tensor\"] = new_args.which_first_tensor\n    if new_args.ensemble == 0:\n        cfg_file[\"training_cfgs\"][\"ensemble\"] = False\n    else:\n        cfg_file[\"training_cfgs\"][\"ensemble\"] = True\n    if new_args.ensemble_items is not None:\n        cfg_file[\"training_cfgs\"][\"ensemble_items\"] = new_args.ensemble_items\n    if new_args.patience is not None:\n        cfg_file[\"training_cfgs\"][\"patience\"] = new_args.patience\n    if new_args.early_stopping is not None:\n        cfg_file[\"training_cfgs\"][\"early_stopping\"] = new_args.early_stopping\n    if new_args.lr_scheduler is not None:\n        cfg_file[\"training_cfgs\"][\"lr_scheduler\"] = new_args.lr_scheduler\n    if new_args.valid_batch_mode is not None:\n        cfg_file[\"training_cfgs\"][\"valid_batch_mode\"] = new_args.valid_batch_mode\n    if new_args.evaluator is not None:\n        cfg_file[\"evaluation_cfgs\"][\"evaluator\"] = new_args.evaluator\n\n    # Station data configurations for GNN models\n    if new_args.station_cfgs is not None:\n        cfg_file[\"data_cfgs\"][\"station_cfgs\"] = new_args.station_cfgs\n\n    # print(\"the updated config:\\n\", json.dumps(cfg_file, indent=4, ensure_ascii=False))\n</code></pre>"},{"location":"api/configs/#torchhydro.configs.config.update_nested_dict","title":"<code>update_nested_dict(d, keys, value)</code>","text":"<p>update nested dict</p>"},{"location":"api/configs/#torchhydro.configs.config.update_nested_dict--parameters","title":"Parameters","text":"<p>d     the dict to be updated keys     the keys of the dict value     the updated value of the dict</p> Source code in <code>torchhydro/configs/config.py</code> <pre><code>def update_nested_dict(d: Dict[str, Any], keys: List[str], value: Any) -&gt; None:\n    \"\"\"update nested dict\n\n    Parameters\n    ----------\n    d\n        the dict to be updated\n    keys\n        the keys of the dict\n    value\n        the updated value of the dict\n    \"\"\"\n    if len(keys) == 1:\n        d[keys[0]] = value\n    else:\n        update_nested_dict(d[keys[0]], keys[1:], value)\n</code></pre>"},{"location":"api/configs/#torchhydro.configs.model_config","title":"<code>model_config</code>","text":"<p>Author: Wenyu Ouyang Date: 2022-10-25 21:16:22 LastEditTime: 2024-11-17 14:43:54 LastEditors: Wenyu Ouyang Description: some basic config for hydrological models FilePath:       orchhydro       orchhydro\\configs\\model_config.py Copyright (c) 2021-2022 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/datasets/","title":"Datasets API","text":""},{"location":"api/datasets/#torchhydro.datasets.data_dict","title":"<code>data_dict</code>","text":"<p>Author: Wenyu Ouyang Date: 2021-12-31 11:08:29 LastEditTime: 2025-07-13 15:40:07 LastEditors: Wenyu Ouyang Description: A dict used for data source and data loader FilePath:       orchhydro       orchhydro\\datasets\\data_dict.py Copyright (c) 2021-2022 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers","title":"<code>data_scalers</code>","text":"<p>Author: Wenyu Ouyang Date: 2024-04-08 18:17:44 LastEditTime: 2025-10-29 08:53:29 LastEditors: Wenyu Ouyang Description: normalize the data FilePath:       orchhydro       orchhydro\\datasets\\data_scalers.py Copyright (c) 2024-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.DapengScaler","title":"<code> DapengScaler        </code>","text":"Source code in <code>torchhydro/datasets/data_scalers.py</code> <pre><code>class DapengScaler(object):\n    def __init__(\n        self,\n        vars_data,\n        data_cfgs: dict,\n        is_tra_val_te: str,\n        other_vars: Optional[dict] = None,\n        prcp_norm_cols=None,\n        gamma_norm_cols=None,\n        pbm_norm=False,\n        data_source: object = None,\n    ):\n        \"\"\"\n        The normalization and denormalization methods from Dapeng's 1st WRR paper.\n        Some use StandardScaler, and some use special norm methods\n\n        Parameters\n        ----------\n        vars_data: dict\n            data for all variables used\n        data_cfgs\n            data parameter config in data source\n        is_tra_val_te\n            train/valid/test\n        other_vars\n            if more input are needed, list them in other_vars\n        prcp_norm_cols\n            data items which use _prcp_norm method to normalize\n        gamma_norm_cols\n            data items which use log(\\sqrt(x)+.1) method to normalize\n        pbm_norm\n            if true, use pbm_norm method to normalize; the output of pbms is not normalized data, so its inverse is different.\n        \"\"\"\n        if prcp_norm_cols is None:\n            prcp_norm_cols = [\n                \"streamflow\",\n            ]\n        if gamma_norm_cols is None:\n            gamma_norm_cols = [\n                \"gpm_tp\",\n                \"sta_tp\",\n                \"total_precipitation_hourly\",\n                \"temperature_2m\",\n                \"dewpoint_temperature_2m\",\n                \"surface_net_solar_radiation\",\n                \"sm_surface\",\n                \"sm_rootzone\",\n            ]\n        self.data_target = vars_data[\"target_cols\"]\n        self.data_cfgs = data_cfgs\n        self.t_s_dict = wrap_t_s_dict(data_cfgs, is_tra_val_te)\n        self.data_other = other_vars\n        self.prcp_norm_cols = prcp_norm_cols\n        self.gamma_norm_cols = gamma_norm_cols\n        # both prcp_norm_cols and gamma_norm_cols use log(\\sqrt(x)+.1) method to normalize\n        self.log_norm_cols = gamma_norm_cols + prcp_norm_cols\n        self.pbm_norm = pbm_norm\n        self.data_source = data_source\n        # save stat_dict of training period in case_dir for valid/test\n        stat_file = os.path.join(data_cfgs[\"case_dir\"], \"dapengscaler_stat.json\")\n        # for testing sometimes such as pub cases, we need stat_dict_file from trained dataset\n        if is_tra_val_te == \"train\" and data_cfgs[\"stat_dict_file\"] is None:\n            self.stat_dict = self.cal_stat_all(vars_data)\n            with open(stat_file, \"w\") as fp:\n                json.dump(self.stat_dict, fp)\n        else:\n            # for valid/test, we need to load stat_dict from train\n            if data_cfgs[\"stat_dict_file\"] is not None:\n                # we used a assigned stat file, typically for PUB exps\n                # shutil.copy(data_cfgs[\"stat_dict_file\"], stat_file)\n                try:\n                    shutil.copy(data_cfgs[\"stat_dict_file\"], stat_file)\n                except SameFileError:\n                    print(\n                        f\"The source file and the target file are the same: {data_cfgs['stat_dict_file']}, skipping the copy operation.\"\n                    )\n                except Exception as e:\n                    print(f\"Error: {e}\")\n            assert os.path.isfile(stat_file)\n            with open(stat_file, \"r\") as fp:\n                self.stat_dict = json.load(fp)\n\n    @property\n    def mean_prcp(self):\n        \"\"\"This property is used to be divided by streamflow to normalize streamflow,\n        hence, its unit is same as streamflow\n\n        Returns\n        -------\n        np.ndarray\n            mean_prcp with the same unit as streamflow\n        \"\"\"\n        # Get the first target variable (usually flow variable) instead of hardcoding \"streamflow\"\n        flow_var_name = self.data_cfgs[\"target_cols\"][0]\n        final_unit = self.data_target.attrs[\"units\"][flow_var_name]\n        mean_prcp = self.data_source.read_mean_prcp(\n            self.t_s_dict[\"sites_id\"], unit=final_unit\n        )\n        return mean_prcp.to_array().transpose(\"basin\", \"variable\").to_numpy()\n\n    def inverse_transform(self, target_values):\n        \"\"\"\n        Denormalization for output variables\n\n        Parameters\n        ----------\n        target_values\n            output variables\n\n        Returns\n        -------\n        np.array\n            denormalized predictions\n        \"\"\"\n        stat_dict = self.stat_dict\n        target_vars = self.data_cfgs[\"target_cols\"]\n        if self.pbm_norm:\n            # for (differentiable models) pbm's output, its unit is mm/day, so we don't need to recover its unit\n            pred = target_values\n        else:\n            pred = _trans_norm(\n                target_values,\n                target_vars,\n                stat_dict,\n                log_norm_cols=self.log_norm_cols,\n                to_norm=False,\n            )\n            for i in range(len(self.data_cfgs[\"target_cols\"])):\n                var = self.data_cfgs[\"target_cols\"][i]\n                if var in self.prcp_norm_cols:\n                    pred.loc[dict(variable=var)] = _prcp_norm(\n                        pred.sel(variable=var).to_numpy(),\n                        self.mean_prcp,\n                        to_norm=False,\n                    )\n                else:\n                    pred.loc[dict(variable=var)] = pred.sel(variable=var)\n        # add attrs for units\n        pred.attrs.update(self.data_target.attrs)\n        return pred.to_dataset(dim=\"variable\")\n\n    def cal_stat_all(self, vars_data):\n        \"\"\"\n        Calculate statistics of outputs(streamflow etc), and inputs(forcing and attributes)\n        Parameters\n        ----------\n        vars_data: dict\n            data for all variables used\n\n        Returns\n        -------\n        dict\n            a dict with statistic values\n        \"\"\"\n        stat_dict = {}\n        for k, v in vars_data.items():\n            if v is None:\n                continue\n            for i in range(len(v.coords[\"variable\"].values)):\n                var_name = v.coords[\"variable\"].values[i]\n                if var_name in self.prcp_norm_cols:\n                    stat_dict[var_name] = cal_stat_prcp_norm(\n                        v.sel(variable=var_name).to_numpy(),\n                        self.mean_prcp,\n                    )\n                elif var_name in self.gamma_norm_cols:\n                    stat_dict[var_name] = cal_stat_gamma(\n                        v.sel(variable=var_name).to_numpy()\n                    )\n                else:\n                    stat_dict[var_name] = cal_stat(v.sel(variable=var_name).to_numpy())\n\n        return stat_dict\n\n    def get_data_norm(self, data, to_norm: bool = True) -&gt; np.ndarray:\n        \"\"\"\n        Get normalized values\n\n        Parameters\n        ----------\n        data\n            origin data\n        to_norm\n            if true, perform normalization\n            if false, perform denormalization\n\n        Returns\n        -------\n        np.array\n            the output value for modeling\n        \"\"\"\n        stat_dict = self.stat_dict\n        out = xr.full_like(data, np.nan)\n        # if we don't set a copy() here, the attrs of data will be changed, which is not our wish\n        out.attrs = copy.deepcopy(data.attrs)\n        _vars = data.coords[\"variable\"].values\n        if \"units\" not in out.attrs:\n            Warning(\"The attrs of output data does not contain units\")\n            out.attrs[\"units\"] = {}\n        for i in range(len(_vars)):\n            var = _vars[i]\n            if var in self.prcp_norm_cols:\n                out.loc[dict(variable=var)] = _prcp_norm(\n                    data.sel(variable=var).to_numpy(),\n                    self.mean_prcp,\n                    to_norm=True,\n                )\n            else:\n                out.loc[dict(variable=var)] = data.sel(variable=var).to_numpy()\n            out.attrs[\"units\"][var] = \"dimensionless\"\n        out = _trans_norm(\n            out,\n            _vars,\n            stat_dict,\n            log_norm_cols=self.log_norm_cols,\n            to_norm=to_norm,\n        )\n        return out\n\n    def load_norm_data(self, vars_data):\n        \"\"\"\n        Read data and perform normalization for DL models\n        Parameters\n        ----------\n        vars_data: dict\n            data for all variables used\n\n        Returns\n        -------\n        tuple\n            x: 3-d  gages_num*time_num*var_num\n            y: 3-d  gages_num*time_num*1\n            c: 2-d  gages_num*var_num\n        \"\"\"\n        if vars_data is None:\n            return None\n        return {\n            k: self.get_data_norm(v) if v is not None else None\n            for k, v in vars_data.items()\n        }\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.DapengScaler.mean_prcp","title":"<code>mean_prcp</code>  <code>property</code> <code>readonly</code>","text":"<p>This property is used to be divided by streamflow to normalize streamflow, hence, its unit is same as streamflow</p>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.DapengScaler.mean_prcp--returns","title":"Returns","text":"<p>np.ndarray     mean_prcp with the same unit as streamflow</p>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.DapengScaler.__init__","title":"<code>__init__(self, vars_data, data_cfgs, is_tra_val_te, other_vars=None, prcp_norm_cols=None, gamma_norm_cols=None, pbm_norm=False, data_source=None)</code>  <code>special</code>","text":"<p>The normalization and denormalization methods from Dapeng's 1st WRR paper. Some use StandardScaler, and some use special norm methods</p>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.DapengScaler.__init__--parameters","title":"Parameters","text":"<p>!!! vars_data \"dict\"     data for all variables used data_cfgs     data parameter config in data source is_tra_val_te     train/valid/test other_vars     if more input are needed, list them in other_vars prcp_norm_cols     data items which use _prcp_norm method to normalize gamma_norm_cols     data items which use log(\\sqrt(x)+.1) method to normalize pbm_norm     if true, use pbm_norm method to normalize; the output of pbms is not normalized data, so its inverse is different.</p> Source code in <code>torchhydro/datasets/data_scalers.py</code> <pre><code>def __init__(\n    self,\n    vars_data,\n    data_cfgs: dict,\n    is_tra_val_te: str,\n    other_vars: Optional[dict] = None,\n    prcp_norm_cols=None,\n    gamma_norm_cols=None,\n    pbm_norm=False,\n    data_source: object = None,\n):\n    \"\"\"\n    The normalization and denormalization methods from Dapeng's 1st WRR paper.\n    Some use StandardScaler, and some use special norm methods\n\n    Parameters\n    ----------\n    vars_data: dict\n        data for all variables used\n    data_cfgs\n        data parameter config in data source\n    is_tra_val_te\n        train/valid/test\n    other_vars\n        if more input are needed, list them in other_vars\n    prcp_norm_cols\n        data items which use _prcp_norm method to normalize\n    gamma_norm_cols\n        data items which use log(\\sqrt(x)+.1) method to normalize\n    pbm_norm\n        if true, use pbm_norm method to normalize; the output of pbms is not normalized data, so its inverse is different.\n    \"\"\"\n    if prcp_norm_cols is None:\n        prcp_norm_cols = [\n            \"streamflow\",\n        ]\n    if gamma_norm_cols is None:\n        gamma_norm_cols = [\n            \"gpm_tp\",\n            \"sta_tp\",\n            \"total_precipitation_hourly\",\n            \"temperature_2m\",\n            \"dewpoint_temperature_2m\",\n            \"surface_net_solar_radiation\",\n            \"sm_surface\",\n            \"sm_rootzone\",\n        ]\n    self.data_target = vars_data[\"target_cols\"]\n    self.data_cfgs = data_cfgs\n    self.t_s_dict = wrap_t_s_dict(data_cfgs, is_tra_val_te)\n    self.data_other = other_vars\n    self.prcp_norm_cols = prcp_norm_cols\n    self.gamma_norm_cols = gamma_norm_cols\n    # both prcp_norm_cols and gamma_norm_cols use log(\\sqrt(x)+.1) method to normalize\n    self.log_norm_cols = gamma_norm_cols + prcp_norm_cols\n    self.pbm_norm = pbm_norm\n    self.data_source = data_source\n    # save stat_dict of training period in case_dir for valid/test\n    stat_file = os.path.join(data_cfgs[\"case_dir\"], \"dapengscaler_stat.json\")\n    # for testing sometimes such as pub cases, we need stat_dict_file from trained dataset\n    if is_tra_val_te == \"train\" and data_cfgs[\"stat_dict_file\"] is None:\n        self.stat_dict = self.cal_stat_all(vars_data)\n        with open(stat_file, \"w\") as fp:\n            json.dump(self.stat_dict, fp)\n    else:\n        # for valid/test, we need to load stat_dict from train\n        if data_cfgs[\"stat_dict_file\"] is not None:\n            # we used a assigned stat file, typically for PUB exps\n            # shutil.copy(data_cfgs[\"stat_dict_file\"], stat_file)\n            try:\n                shutil.copy(data_cfgs[\"stat_dict_file\"], stat_file)\n            except SameFileError:\n                print(\n                    f\"The source file and the target file are the same: {data_cfgs['stat_dict_file']}, skipping the copy operation.\"\n                )\n            except Exception as e:\n                print(f\"Error: {e}\")\n        assert os.path.isfile(stat_file)\n        with open(stat_file, \"r\") as fp:\n            self.stat_dict = json.load(fp)\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.DapengScaler.cal_stat_all","title":"<code>cal_stat_all(self, vars_data)</code>","text":"<p>Calculate statistics of outputs(streamflow etc), and inputs(forcing and attributes) Parameters</p> <p>!!! vars_data \"dict\"     data for all variables used</p>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.DapengScaler.cal_stat_all--returns","title":"Returns","text":"<p>dict     a dict with statistic values</p> Source code in <code>torchhydro/datasets/data_scalers.py</code> <pre><code>def cal_stat_all(self, vars_data):\n    \"\"\"\n    Calculate statistics of outputs(streamflow etc), and inputs(forcing and attributes)\n    Parameters\n    ----------\n    vars_data: dict\n        data for all variables used\n\n    Returns\n    -------\n    dict\n        a dict with statistic values\n    \"\"\"\n    stat_dict = {}\n    for k, v in vars_data.items():\n        if v is None:\n            continue\n        for i in range(len(v.coords[\"variable\"].values)):\n            var_name = v.coords[\"variable\"].values[i]\n            if var_name in self.prcp_norm_cols:\n                stat_dict[var_name] = cal_stat_prcp_norm(\n                    v.sel(variable=var_name).to_numpy(),\n                    self.mean_prcp,\n                )\n            elif var_name in self.gamma_norm_cols:\n                stat_dict[var_name] = cal_stat_gamma(\n                    v.sel(variable=var_name).to_numpy()\n                )\n            else:\n                stat_dict[var_name] = cal_stat(v.sel(variable=var_name).to_numpy())\n\n    return stat_dict\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.DapengScaler.get_data_norm","title":"<code>get_data_norm(self, data, to_norm=True)</code>","text":"<p>Get normalized values</p>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.DapengScaler.get_data_norm--parameters","title":"Parameters","text":"<p>data     origin data to_norm     if true, perform normalization     if false, perform denormalization</p>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.DapengScaler.get_data_norm--returns","title":"Returns","text":"<p>np.array     the output value for modeling</p> Source code in <code>torchhydro/datasets/data_scalers.py</code> <pre><code>def get_data_norm(self, data, to_norm: bool = True) -&gt; np.ndarray:\n    \"\"\"\n    Get normalized values\n\n    Parameters\n    ----------\n    data\n        origin data\n    to_norm\n        if true, perform normalization\n        if false, perform denormalization\n\n    Returns\n    -------\n    np.array\n        the output value for modeling\n    \"\"\"\n    stat_dict = self.stat_dict\n    out = xr.full_like(data, np.nan)\n    # if we don't set a copy() here, the attrs of data will be changed, which is not our wish\n    out.attrs = copy.deepcopy(data.attrs)\n    _vars = data.coords[\"variable\"].values\n    if \"units\" not in out.attrs:\n        Warning(\"The attrs of output data does not contain units\")\n        out.attrs[\"units\"] = {}\n    for i in range(len(_vars)):\n        var = _vars[i]\n        if var in self.prcp_norm_cols:\n            out.loc[dict(variable=var)] = _prcp_norm(\n                data.sel(variable=var).to_numpy(),\n                self.mean_prcp,\n                to_norm=True,\n            )\n        else:\n            out.loc[dict(variable=var)] = data.sel(variable=var).to_numpy()\n        out.attrs[\"units\"][var] = \"dimensionless\"\n    out = _trans_norm(\n        out,\n        _vars,\n        stat_dict,\n        log_norm_cols=self.log_norm_cols,\n        to_norm=to_norm,\n    )\n    return out\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.DapengScaler.inverse_transform","title":"<code>inverse_transform(self, target_values)</code>","text":"<p>Denormalization for output variables</p>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.DapengScaler.inverse_transform--parameters","title":"Parameters","text":"<p>target_values     output variables</p>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.DapengScaler.inverse_transform--returns","title":"Returns","text":"<p>np.array     denormalized predictions</p> Source code in <code>torchhydro/datasets/data_scalers.py</code> <pre><code>def inverse_transform(self, target_values):\n    \"\"\"\n    Denormalization for output variables\n\n    Parameters\n    ----------\n    target_values\n        output variables\n\n    Returns\n    -------\n    np.array\n        denormalized predictions\n    \"\"\"\n    stat_dict = self.stat_dict\n    target_vars = self.data_cfgs[\"target_cols\"]\n    if self.pbm_norm:\n        # for (differentiable models) pbm's output, its unit is mm/day, so we don't need to recover its unit\n        pred = target_values\n    else:\n        pred = _trans_norm(\n            target_values,\n            target_vars,\n            stat_dict,\n            log_norm_cols=self.log_norm_cols,\n            to_norm=False,\n        )\n        for i in range(len(self.data_cfgs[\"target_cols\"])):\n            var = self.data_cfgs[\"target_cols\"][i]\n            if var in self.prcp_norm_cols:\n                pred.loc[dict(variable=var)] = _prcp_norm(\n                    pred.sel(variable=var).to_numpy(),\n                    self.mean_prcp,\n                    to_norm=False,\n                )\n            else:\n                pred.loc[dict(variable=var)] = pred.sel(variable=var)\n    # add attrs for units\n    pred.attrs.update(self.data_target.attrs)\n    return pred.to_dataset(dim=\"variable\")\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.DapengScaler.load_norm_data","title":"<code>load_norm_data(self, vars_data)</code>","text":"<p>Read data and perform normalization for DL models Parameters</p> <p>!!! vars_data \"dict\"     data for all variables used</p>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.DapengScaler.load_norm_data--returns","title":"Returns","text":"<p>tuple     x: 3-d  gages_numtime_numvar_num     y: 3-d  gages_numtime_num1     c: 2-d  gages_num*var_num</p> Source code in <code>torchhydro/datasets/data_scalers.py</code> <pre><code>def load_norm_data(self, vars_data):\n    \"\"\"\n    Read data and perform normalization for DL models\n    Parameters\n    ----------\n    vars_data: dict\n        data for all variables used\n\n    Returns\n    -------\n    tuple\n        x: 3-d  gages_num*time_num*var_num\n        y: 3-d  gages_num*time_num*1\n        c: 2-d  gages_num*var_num\n    \"\"\"\n    if vars_data is None:\n        return None\n    return {\n        k: self.get_data_norm(v) if v is not None else None\n        for k, v in vars_data.items()\n    }\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.ScalerHub","title":"<code> ScalerHub        </code>","text":"<p>A class for Scaler</p> Source code in <code>torchhydro/datasets/data_scalers.py</code> <pre><code>class ScalerHub(object):\n    \"\"\"\n    A class for Scaler\n    \"\"\"\n\n    def __init__(\n        self,\n        vars_data,\n        data_cfgs=None,\n        is_tra_val_te=None,\n        data_source=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Perform normalization\n\n        Parameters\n        ----------\n        vars_data\n            data for all variables used.\n            the dim must be (basin, time, lead_step, var) for 4-d array;\n            the dim must be (basin, time, var) for 3-d array;\n            the dim must be (basin, time) for 2-d array;\n        data_cfgs\n            configs for reading data\n        is_tra_val_te\n            train, valid or test\n        data_source\n            data source to get original data info\n        kwargs\n            other optional parameters for ScalerHub\n        \"\"\"\n        self.data_cfgs = data_cfgs\n        scaler_type = data_cfgs[\"scaler\"]\n        pbm_norm = data_cfgs[\"scaler_params\"][\"pbm_norm\"]\n        if scaler_type == \"DapengScaler\":\n            gamma_norm_cols = data_cfgs[\"scaler_params\"][\"gamma_norm_cols\"]\n            prcp_norm_cols = data_cfgs[\"scaler_params\"][\"prcp_norm_cols\"]\n            scaler = DapengScaler(\n                vars_data,\n                data_cfgs,\n                is_tra_val_te,\n                prcp_norm_cols=prcp_norm_cols,\n                gamma_norm_cols=gamma_norm_cols,\n                pbm_norm=pbm_norm,\n                data_source=data_source,\n            )\n        elif scaler_type in SCALER_DICT.keys():\n            scaler = SklearnScaler(\n                vars_data,\n                data_cfgs,\n                is_tra_val_te,\n                pbm_norm=pbm_norm,\n            )\n        else:\n            raise NotImplementedError(\n                \"We don't provide this Scaler now!!! Please choose another one: DapengScaler or key in SCALER_DICT\"\n            )\n        self.norm_data = scaler.load_norm_data(vars_data)\n        # we will use target_scaler during denormalization\n        self.target_scaler = scaler\n        print(\"Finish Normalization\\n\")\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.ScalerHub.__init__","title":"<code>__init__(self, vars_data, data_cfgs=None, is_tra_val_te=None, data_source=None, **kwargs)</code>  <code>special</code>","text":"<p>Perform normalization</p>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.ScalerHub.__init__--parameters","title":"Parameters","text":"<p>vars_data     data for all variables used.     the dim must be (basin, time, lead_step, var) for 4-d array;     the dim must be (basin, time, var) for 3-d array;     the dim must be (basin, time) for 2-d array; data_cfgs     configs for reading data is_tra_val_te     train, valid or test data_source     data source to get original data info kwargs     other optional parameters for ScalerHub</p> Source code in <code>torchhydro/datasets/data_scalers.py</code> <pre><code>def __init__(\n    self,\n    vars_data,\n    data_cfgs=None,\n    is_tra_val_te=None,\n    data_source=None,\n    **kwargs,\n):\n    \"\"\"\n    Perform normalization\n\n    Parameters\n    ----------\n    vars_data\n        data for all variables used.\n        the dim must be (basin, time, lead_step, var) for 4-d array;\n        the dim must be (basin, time, var) for 3-d array;\n        the dim must be (basin, time) for 2-d array;\n    data_cfgs\n        configs for reading data\n    is_tra_val_te\n        train, valid or test\n    data_source\n        data source to get original data info\n    kwargs\n        other optional parameters for ScalerHub\n    \"\"\"\n    self.data_cfgs = data_cfgs\n    scaler_type = data_cfgs[\"scaler\"]\n    pbm_norm = data_cfgs[\"scaler_params\"][\"pbm_norm\"]\n    if scaler_type == \"DapengScaler\":\n        gamma_norm_cols = data_cfgs[\"scaler_params\"][\"gamma_norm_cols\"]\n        prcp_norm_cols = data_cfgs[\"scaler_params\"][\"prcp_norm_cols\"]\n        scaler = DapengScaler(\n            vars_data,\n            data_cfgs,\n            is_tra_val_te,\n            prcp_norm_cols=prcp_norm_cols,\n            gamma_norm_cols=gamma_norm_cols,\n            pbm_norm=pbm_norm,\n            data_source=data_source,\n        )\n    elif scaler_type in SCALER_DICT.keys():\n        scaler = SklearnScaler(\n            vars_data,\n            data_cfgs,\n            is_tra_val_te,\n            pbm_norm=pbm_norm,\n        )\n    else:\n        raise NotImplementedError(\n            \"We don't provide this Scaler now!!! Please choose another one: DapengScaler or key in SCALER_DICT\"\n        )\n    self.norm_data = scaler.load_norm_data(vars_data)\n    # we will use target_scaler during denormalization\n    self.target_scaler = scaler\n    print(\"Finish Normalization\\n\")\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.SklearnScaler","title":"<code> SklearnScaler        </code>","text":"Source code in <code>torchhydro/datasets/data_scalers.py</code> <pre><code>class SklearnScaler(object):\n    def __init__(\n        self,\n        vars_data,\n        data_cfgs,\n        is_tra_val_te,\n        pbm_norm=False,\n    ):\n        \"\"\"_summary_\n\n        Parameters\n        ----------\n        vars_data : dict\n            vars data map\n        data_cfgs : _type_\n            _description_\n        is_tra_val_te : bool\n            _description_\n        pbm_norm : bool, optional\n            _description_, by default False\n        \"\"\"\n        # we will use data_target and target_scaler for denormalization\n        self.data_target = vars_data[\"target_cols\"]\n        self.target_scaler = None\n        self.data_cfgs = data_cfgs\n        self.is_tra_val_te = is_tra_val_te\n        self.pbm_norm = pbm_norm\n\n    def load_norm_data(self, vars_data):\n        # TODO: not fully tested for differentiable models\n        norm_dict = {}\n        scaler_type = self.data_cfgs[\"scaler\"]\n        for k, v in vars_data.items():\n            scaler = SCALER_DICT[scaler_type]()\n            if v.ndim == 3:\n                # for forcings and outputs\n                num_instances, num_time_steps, num_features = v.shape\n                v_np = v.to_numpy().reshape(-1, num_features)\n                scaler, data_norm = self._sklearn_scale(\n                    self.data_cfgs, self.is_tra_val_te, scaler, k, v_np\n                )\n                data_norm = data_norm.reshape(\n                    num_instances, num_time_steps, num_features\n                )\n                norm_xrarray = xr.DataArray(\n                    data_norm,\n                    coords={\n                        \"basin\": v.coords[\"basin\"],\n                        \"time\": v.coords[\"time\"],\n                        \"variable\": v.coords[\"variable\"],\n                    },\n                    dims=[\"basin\", \"time\", \"variable\"],\n                )\n            elif v.ndim == 2:\n                num_instances, num_features = v.shape\n                v_np = v.to_numpy().reshape(-1, num_features)\n                scaler, data_norm = self._sklearn_scale(\n                    self.data_cfgs, self.is_tra_val_te, scaler, k, v_np\n                )\n                # don't need to reshape data_norm again as it is 2-d\n                norm_xrarray = xr.DataArray(\n                    data_norm,\n                    coords={\n                        \"basin\": v.coords[\"basin\"],\n                        \"variable\": v.coords[\"variable\"],\n                    },\n                    dims=[\"basin\", \"variable\"],\n                )\n            elif v.ndim == 4:\n                # for forecast data\n                num_instances, num_time_steps, num_lead_steps, num_features = v.shape\n                v_np = v.to_numpy().reshape(-1, num_features)\n                scaler, data_norm = self._sklearn_scale(\n                    self.data_cfgs, self.is_tra_val_te, scaler, k, v_np\n                )\n                data_norm = data_norm.reshape(\n                    num_instances, num_time_steps, num_lead_steps, num_features\n                )\n                norm_xrarray = xr.DataArray(\n                    data_norm,\n                    coords={\n                        \"basin\": v.coords[\"basin\"],\n                        \"time\": v.coords[\"time\"],\n                        \"lead_step\": v.coords[\"lead_step\"],\n                        \"variable\": v.coords[\"variable\"],\n                    },\n                    dims=[\"basin\", \"time\", \"lead_step\", \"variable\"],\n                )\n            else:\n                raise NotImplementedError(\n                    \"Please check your data, the dim of data must be 2, 3 or 4\"\n                )\n\n            norm_dict[k] = norm_xrarray\n            if k == \"target_cols\":\n                # we need target cols scaler for denormalization\n                self.target_scaler = scaler\n        return norm_dict\n\n    def _sklearn_scale(self, data_cfgs, is_tra_val_te, scaler, norm_key, data):\n        save_file = os.path.join(data_cfgs[\"case_dir\"], f\"{norm_key}_scaler.pkl\")\n        if is_tra_val_te == \"train\" and data_cfgs[\"stat_dict_file\"] is None:\n            data_norm = scaler.fit_transform(data)\n            # Save scaler in case_dir for valid/test\n            with open(save_file, \"wb\") as outfile:\n                pkl.dump(scaler, outfile)\n        else:\n            if data_cfgs[\"stat_dict_file\"] is not None:\n                # NOTE: you need to set data_cfgs[\"stat_dict_file\"] as a str with \";\" as its seperator\n                # the sequence of the stat_dict_file must be same as the sequence of norm_keys\n                # for example: \"stat_dict_file\": \"target_stat_dict_file;relevant_stat_dict_file;constant_stat_dict_file\"\n                shutil.copy(data_cfgs[\"stat_dict_file\"][norm_key], save_file)\n            if not os.path.isfile(save_file):\n                raise FileNotFoundError(\"Please genereate xx_scaler.pkl file\")\n            with open(save_file, \"rb\") as infile:\n                scaler = pkl.load(infile)\n                data_norm = scaler.transform(data)\n        return scaler, data_norm\n\n    def inverse_transform(self, target_values):\n        \"\"\"\n        Denormalization for output variables\n\n        Parameters\n        ----------\n        target_values\n            output variables (xr.DataArray or np.ndarray)\n\n        Returns\n        -------\n        xr.Dataset\n            denormalized predictions or observations\n        \"\"\"\n        coords = self.data_target.coords\n        attrs = self.data_target.attrs\n        # input must be xr.DataArray\n        if not isinstance(target_values, xr.DataArray):\n            # the shape of target_values must be (basin, time, variable)\n            target_values = xr.DataArray(\n                target_values,\n                coords={\n                    \"basin\": coords[\"basin\"],\n                    \"time\": coords[\"time\"],\n                    \"variable\": coords[\"variable\"],\n                },\n                dims=[\"basin\", \"time\", \"variable\"],\n            )\n        # transform to numpy array for sklearn inverse_transform\n        shape = target_values.shape\n        arr = target_values.to_numpy().reshape(-1, shape[-1])\n        # sklearn inverse_transform\n        arr_inv = self.target_scaler.inverse_transform(arr)\n        # reshape to original shape\n        arr_inv = arr_inv.reshape(shape)\n        result = xr.DataArray(\n            arr_inv,\n            coords=target_values.coords,\n            dims=target_values.dims,\n            attrs=attrs,\n        )\n        # add attrs for units\n        result.attrs.update(self.data_target.attrs)\n        return result.to_dataset(dim=\"variable\")\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.SklearnScaler.__init__","title":"<code>__init__(self, vars_data, data_cfgs, is_tra_val_te, pbm_norm=False)</code>  <code>special</code>","text":"<p>summary</p>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.SklearnScaler.__init__--parameters","title":"Parameters","text":"<p>vars_data : dict     vars data map data_cfgs : type description is_tra_val_te : bool     description pbm_norm : bool, optional     description, by default False</p> Source code in <code>torchhydro/datasets/data_scalers.py</code> <pre><code>def __init__(\n    self,\n    vars_data,\n    data_cfgs,\n    is_tra_val_te,\n    pbm_norm=False,\n):\n    \"\"\"_summary_\n\n    Parameters\n    ----------\n    vars_data : dict\n        vars data map\n    data_cfgs : _type_\n        _description_\n    is_tra_val_te : bool\n        _description_\n    pbm_norm : bool, optional\n        _description_, by default False\n    \"\"\"\n    # we will use data_target and target_scaler for denormalization\n    self.data_target = vars_data[\"target_cols\"]\n    self.target_scaler = None\n    self.data_cfgs = data_cfgs\n    self.is_tra_val_te = is_tra_val_te\n    self.pbm_norm = pbm_norm\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.SklearnScaler.inverse_transform","title":"<code>inverse_transform(self, target_values)</code>","text":"<p>Denormalization for output variables</p>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.SklearnScaler.inverse_transform--parameters","title":"Parameters","text":"<p>target_values     output variables (xr.DataArray or np.ndarray)</p>"},{"location":"api/datasets/#torchhydro.datasets.data_scalers.SklearnScaler.inverse_transform--returns","title":"Returns","text":"<p>xr.Dataset     denormalized predictions or observations</p> Source code in <code>torchhydro/datasets/data_scalers.py</code> <pre><code>def inverse_transform(self, target_values):\n    \"\"\"\n    Denormalization for output variables\n\n    Parameters\n    ----------\n    target_values\n        output variables (xr.DataArray or np.ndarray)\n\n    Returns\n    -------\n    xr.Dataset\n        denormalized predictions or observations\n    \"\"\"\n    coords = self.data_target.coords\n    attrs = self.data_target.attrs\n    # input must be xr.DataArray\n    if not isinstance(target_values, xr.DataArray):\n        # the shape of target_values must be (basin, time, variable)\n        target_values = xr.DataArray(\n            target_values,\n            coords={\n                \"basin\": coords[\"basin\"],\n                \"time\": coords[\"time\"],\n                \"variable\": coords[\"variable\"],\n            },\n            dims=[\"basin\", \"time\", \"variable\"],\n        )\n    # transform to numpy array for sklearn inverse_transform\n    shape = target_values.shape\n    arr = target_values.to_numpy().reshape(-1, shape[-1])\n    # sklearn inverse_transform\n    arr_inv = self.target_scaler.inverse_transform(arr)\n    # reshape to original shape\n    arr_inv = arr_inv.reshape(shape)\n    result = xr.DataArray(\n        arr_inv,\n        coords=target_values.coords,\n        dims=target_values.dims,\n        attrs=attrs,\n    )\n    # add attrs for units\n    result.attrs.update(self.data_target.attrs)\n    return result.to_dataset(dim=\"variable\")\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets","title":"<code>data_sets</code>","text":"<p>Author: Wenyu Ouyang Date: 2024-04-08 18:16:53 LastEditTime: 2025-11-07 09:39:57 LastEditors: Wenyu Ouyang Description: A pytorch dataset class; references to https://github.com/neuralhydrology/neuralhydrology FilePath:       orchhydro       orchhydro\\datasets\\data_sets.py Copyright (c) 2024-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.AugmentedFloodEventDataset","title":"<code> AugmentedFloodEventDataset            (FloodEventDataset)         </code>","text":"<p>Dataset class for augmented flood event data with discontinuous time ranges.</p> <p>This dataset is designed to handle flood event data that includes augmented (generated) future data alongside historical data, where time ranges may be discontinuous (e.g., historical data 1990-2010, then augmented data 2026+).</p> <p>It connects to hydrodatasource.reader.floodevent.FloodEventDatasource and uses the read_ts_xrdataset_augmented method to read augmented data.</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>class AugmentedFloodEventDataset(FloodEventDataset):\n    \"\"\"Dataset class for augmented flood event data with discontinuous time ranges.\n\n    This dataset is designed to handle flood event data that includes augmented\n    (generated) future data alongside historical data, where time ranges may be\n    discontinuous (e.g., historical data 1990-2010, then augmented data 2026+).\n\n    It connects to hydrodatasource.reader.floodevent.FloodEventDatasource\n    and uses the read_ts_xrdataset_augmented method to read augmented data.\n    \"\"\"\n\n    def __init__(self, cfgs: dict, is_tra_val_te: str):\n        \"\"\"Initialize AugmentedFloodEventDataset\n\n        Parameters\n        ----------\n        cfgs : dict\n            Configuration dictionary containing data_cfgs, training_cfgs, evaluation_cfgs\n        is_tra_val_te : str\n            One of 'train', 'valid', or 'test'\n        \"\"\"\n        super(AugmentedFloodEventDataset, self).__init__(cfgs, is_tra_val_te)\n\n        if not hasattr(self.data_source, \"read_ts_xrdataset_augmented\"):\n            raise ValueError(\n                \"Data source must support read_ts_xrdataset_augmented method\"\n            )\n\n    def _read_xyc_period(self, start_date, end_date):\n        \"\"\"Override template method to read augmented flood event data for a specific period\n\n        This method leverages the parent class's multi-period handling while using\n        augmented data reading methods for generated future data.\n\n        Parameters\n        ----------\n        start_date : str\n            start time\n        end_date : str\n            end time\n\n        Returns\n        -------\n        dict\n            Dictionary containing relevant_cols, target_cols, and constant_cols data\n        \"\"\"\n        return self._read_xyc_specified_time_augmented(start_date, end_date)\n\n    def _read_xyc_specified_time_augmented(self, start_date, end_date):\n        \"\"\"Read x, y, c data from both historical and augmented data sources\n\n        This method reads both historical observed data (using read_ts_xrdataset)\n        and augmented future data (using read_ts_xrdataset_augmented), then\n        concatenates them along the time dimension to provide a complete dataset.\n\n        Parameters\n        ----------\n        start_date : str\n            start time\n        end_date : str\n            end time\n\n        Returns\n        -------\n        dict\n            Dictionary containing relevant_cols, target_cols, and constant_cols data\n        \"\"\"\n\n        # Read historical observed data using standard method\n        relevant_cols = self.data_cfgs.get(\"relevant_cols\", [\"rain\"])\n        target_cols = self.data_cfgs.get(\"target_cols\", [\"inflow\", \"flood_event\"])\n\n        try:\n            data_forcing_hist_ = self.data_source.read_ts_xrdataset(\n                self.t_s_dict[\"sites_id\"],\n                [start_date, end_date],\n                relevant_cols,\n            )\n            data_output_hist_ = self.data_source.read_ts_xrdataset(\n                self.t_s_dict[\"sites_id\"],\n                [start_date, end_date],\n                target_cols,\n            )\n\n            # Process historical data\n            data_forcing_hist_ = self._rm_timeunit_key(data_forcing_hist_)\n            data_output_hist_ = self._rm_timeunit_key(data_output_hist_)\n\n        except Exception as e:\n            LOGGER.info(f\"\u65e0\u6cd5\u8bfb\u53d6\u5386\u53f2\u6570\u636e\uff0c\u53ef\u80fd\u662f\u65f6\u95f4\u8303\u56f4\u4e0d\u5728\u5386\u53f2\u6570\u636e\u4e2d: {e}\")\n            data_forcing_hist_ = None\n            data_output_hist_ = None\n\n        # Read augmented data using augmented method\n        try:\n            data_forcing_aug_ = self.data_source.read_ts_xrdataset_augmented(\n                self.t_s_dict[\"sites_id\"],\n                [start_date, end_date],\n                relevant_cols,\n            )\n            data_output_aug_ = self.data_source.read_ts_xrdataset_augmented(\n                self.t_s_dict[\"sites_id\"],\n                [start_date, end_date],\n                target_cols,\n            )\n\n            # Process augmented data\n            data_forcing_aug_ = self._rm_timeunit_key(data_forcing_aug_)\n            data_output_aug_ = self._rm_timeunit_key(data_output_aug_)\n\n        except Exception as e:\n            LOGGER.info(f\"\u65e0\u6cd5\u8bfb\u53d6\u589e\u5f3a\u6570\u636e\uff0c\u53ef\u80fd\u662f\u65f6\u95f4\u8303\u56f4\u4e0d\u5728\u589e\u5f3a\u6570\u636e\u4e2d: {e}\")\n            data_forcing_aug_ = None\n            data_output_aug_ = None\n\n        # Combine historical and augmented data\n        data_forcing_ds = self._combine_historical_and_augmented_data(\n            data_forcing_hist_, data_forcing_aug_, \"forcing\"\n        )\n        data_output_ds = self._combine_historical_and_augmented_data(\n            data_output_hist_, data_output_aug_, \"target\"\n        )\n\n        # Check and process combined data\n        data_forcing_ds, data_output_ds = self._check_ts_xrds_unit(\n            data_forcing_ds, data_output_ds\n        )\n\n        # Read constant/attribute data (same as parent class)\n        data_attr_ds = self.data_source.read_attr_xrdataset(\n            self.t_s_dict[\"sites_id\"],\n            self.data_cfgs[\"constant_cols\"],\n            all_number=True,\n        )\n\n        # Convert to DataArray with units\n        x_origin, y_origin, c_origin = self._to_dataarray_with_unit(\n            data_forcing_ds, data_output_ds, data_attr_ds\n        )\n\n        return {\n            \"relevant_cols\": x_origin.transpose(\"basin\", \"time\", \"variable\"),\n            \"target_cols\": y_origin.transpose(\"basin\", \"time\", \"variable\"),\n            \"constant_cols\": (\n                c_origin.transpose(\"basin\", \"variable\")\n                if c_origin is not None\n                else None\n            ),\n        }\n\n    def _combine_historical_and_augmented_data(self, hist_data, aug_data, data_type):\n        \"\"\"Combine historical observed data and augmented generated data\n\n        This method concatenates historical and augmented data along the time dimension,\n        handling cases where data may be discontinuous or overlapping.\n\n        Parameters\n        ----------\n        hist_data : xr.Dataset or None\n            Historical observed data\n        aug_data : xr.Dataset or None\n            Augmented generated data\n        data_type : str\n            Type of data (\"forcing\" or \"target\") for logging purposes\n\n        Returns\n        -------\n        xr.Dataset\n            Combined dataset with historical and augmented data concatenated\n        \"\"\"\n        import xarray as xr\n\n        # Handle cases where one or both data sources are None\n        if hist_data is None and aug_data is None:\n            raise ValueError(f\"Both historical and augmented {data_type} data are None\")\n        elif hist_data is None:\n            LOGGER.info(\n                f\"No historical {data_type} data found, using only augmented data\"\n            )\n            return aug_data\n        elif aug_data is None:\n            LOGGER.info(\n                f\"No augmented {data_type} data found, using only historical data\"\n            )\n            return hist_data\n\n        # Both datasets exist - need to combine them\n        try:\n            # Check if there's time overlap between datasets\n            hist_times = hist_data.time.values if \"time\" in hist_data.dims else []\n            aug_times = aug_data.time.values if \"time\" in aug_data.dims else []\n\n            if len(hist_times) == 0:\n                LOGGER.info(\n                    f\"Historical {data_type} data has no time dimension, using only augmented data\"\n                )\n                return aug_data\n            elif len(aug_times) == 0:\n                LOGGER.info(\n                    f\"Augmented {data_type} data has no time dimension, using only historical data\"\n                )\n                return hist_data\n\n            # Find overlap period\n            hist_start, hist_end = hist_times[0], hist_times[-1]\n            aug_start, aug_end = aug_times[0], aug_times[-1]\n\n            # Check for overlap\n            if hist_end &lt; aug_start:\n                # No overlap - historical data ends before augmented data starts\n                LOGGER.info(\n                    f\"No temporal overlap for {data_type} data, concatenating sequentially\"\n                )\n                combined_data = xr.concat([hist_data, aug_data], dim=\"time\")\n            elif aug_end &lt; hist_start:\n                # No overlap - augmented data ends before historical data starts\n                LOGGER.info(\n                    f\"Augmented {data_type} data precedes historical data, concatenating\"\n                )\n                combined_data = xr.concat([aug_data, hist_data], dim=\"time\")\n            else:\n                # There is overlap - need to handle carefully\n                LOGGER.info(\n                    f\"Temporal overlap detected for {data_type} data, \"\n                    f\"merging with priority to historical data\"\n                )\n\n                # Create time index for the full range\n                all_times = sorted(set(list(hist_times) + list(aug_times)))\n\n                # Reindex both datasets to the full time range\n                hist_reindexed = hist_data.reindex(time=all_times, method=None)\n                aug_reindexed = aug_data.reindex(time=all_times, method=None)\n\n                # Combine: use historical data where available, fill with augmented data\n                combined_data = hist_reindexed.where(\n                    ~hist_reindexed.isnull(), aug_reindexed\n                )\n\n            # Sort by time to ensure proper ordering\n            combined_data = combined_data.sortby(\"time\")\n\n            LOGGER.info(\n                f\"Successfully combined {data_type} data: \"\n                f\"historical shape {hist_data.dims if hasattr(hist_data, 'dims') else 'N/A'}, \"\n                f\"augmented shape {aug_data.dims if hasattr(aug_data, 'dims') else 'N/A'}, \"\n                f\"combined shape {combined_data.dims}\"\n            )\n\n            return combined_data\n\n        except Exception as e:\n            LOGGER.error(f\"Failed to combine {data_type} data: {e}\")\n            # Fallback: prefer historical data if combination fails\n            LOGGER.warning(f\"Falling back to historical {data_type} data only\")\n            return hist_data\n\n    def _handle_discontinuous_time_ranges(self, data_dict, start_date, end_date):\n        \"\"\"Handle discontinuous time ranges by filling gaps with NaN values\n\n        This method creates a continuous time index and fills missing periods\n        with NaN values, handling cases such as training data covers 1990-2010,\n        augmented data starts from 2026+, and test data covers 2011-2025.\n\n        Parameters\n        ----------\n        data_dict : dict\n            Dictionary containing xarray data with keys 'relevant_cols',\n            'target_cols', 'constant_cols'\n        start_date : str\n            Overall start date for the continuous timeline\n        end_date : str\n            Overall end date for the continuous timeline\n\n        Returns\n        -------\n        dict\n            Dictionary with continuous time index and NaN-filled gaps\n        \"\"\"\n        # Create continuous daily time index from start_date to end_date\n        try:\n            continuous_time = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n        except Exception as e:\n            LOGGER.warning(f\"Failed to create continuous time index: {e}\")\n            return data_dict\n\n        # Process each data type (relevant_cols, target_cols)\n        processed_dict = {}\n\n        for data_key in [\"relevant_cols\", \"target_cols\"]:\n            if data_key in data_dict and data_dict[data_key] is not None:\n                original_data = data_dict[data_key]\n\n                try:\n                    # Check if data has time dimension\n                    if \"time\" not in original_data.dims:\n                        LOGGER.warning(\n                            f\"{data_key} has no time dimension, skipping time alignment\"\n                        )\n                        processed_dict[data_key] = original_data\n                        continue\n\n                    # Reindex to continuous time, filling gaps with NaN\n                    aligned_data = original_data.reindex(\n                        time=continuous_time,\n                        method=None,  # No interpolation, fill with NaN\n                        fill_value=float(\"nan\"),\n                    )\n\n                    processed_dict[data_key] = aligned_data\n\n                    # Log information about the alignment\n                    original_time_points = len(original_data.time)\n                    aligned_time_points = len(aligned_data.time)\n                    nan_points = aligned_data.isnull().sum().sum().values\n\n                    LOGGER.info(\n                        f\"{data_key}: aligned from {original_time_points} to \"\n                        f\"{aligned_time_points} time points, with {nan_points} \"\n                        f\"NaN values for discontinuous periods\"\n                    )\n\n                except Exception as e:\n                    LOGGER.error(\n                        f\"Failed to align {data_key} to continuous timeline: {e}\"\n                    )\n                    processed_dict[data_key] = original_data\n            else:\n                processed_dict[data_key] = data_dict.get(data_key)\n\n        # Constant cols don't need time alignment\n        processed_dict[\"constant_cols\"] = data_dict.get(\"constant_cols\")\n\n        return processed_dict\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.AugmentedFloodEventDataset.__init__","title":"<code>__init__(self, cfgs, is_tra_val_te)</code>  <code>special</code>","text":"<p>Initialize AugmentedFloodEventDataset</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.AugmentedFloodEventDataset.__init__--parameters","title":"Parameters","text":"<p>cfgs : dict     Configuration dictionary containing data_cfgs, training_cfgs, evaluation_cfgs is_tra_val_te : str     One of 'train', 'valid', or 'test'</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>def __init__(self, cfgs: dict, is_tra_val_te: str):\n    \"\"\"Initialize AugmentedFloodEventDataset\n\n    Parameters\n    ----------\n    cfgs : dict\n        Configuration dictionary containing data_cfgs, training_cfgs, evaluation_cfgs\n    is_tra_val_te : str\n        One of 'train', 'valid', or 'test'\n    \"\"\"\n    super(AugmentedFloodEventDataset, self).__init__(cfgs, is_tra_val_te)\n\n    if not hasattr(self.data_source, \"read_ts_xrdataset_augmented\"):\n        raise ValueError(\n            \"Data source must support read_ts_xrdataset_augmented method\"\n        )\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.BaseDataset","title":"<code> BaseDataset            (Dataset)         </code>","text":"<p>Base data set class to load and preprocess data (batch-first) using PyTorch's Dataset</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>class BaseDataset(Dataset):\n    \"\"\"Base data set class to load and preprocess data (batch-first) using PyTorch's Dataset\"\"\"\n\n    def __init__(self, cfgs: dict, is_tra_val_te: str):\n        \"\"\"\n        Parameters\n        ----------\n        cfgs\n            configs, including data and training + evaluation settings\n            which will be used for organizing batch data\n        is_tra_val_te\n            train, vaild or test\n        \"\"\"\n        super(BaseDataset, self).__init__()\n        self.data_cfgs = cfgs[\"data_cfgs\"]\n        self.training_cfgs = cfgs[\"training_cfgs\"]\n        self.evaluation_cfgs = cfgs[\"evaluation_cfgs\"]\n        self._pre_load_data(is_tra_val_te)\n        # load and preprocess data\n        self._load_data()\n\n    def _pre_load_data(self, is_tra_val_te):\n        \"\"\"\n        some preprocessing before loading data, such as\n        setting the way to organize batch data\n\n        Parameters\n        ----------\n        is_tra_val_te: bool\n            train, valid or test\n\n        Raises\n        ------\n        ValueError\n            _description_\n        \"\"\"\n        if is_tra_val_te in {\"train\", \"valid\", \"test\"}:\n            self.is_tra_val_te = is_tra_val_te\n        else:\n            raise ValueError(\n                \"'is_tra_val_te' must be one of 'train', 'valid' or 'test' \"\n            )\n        self.train_mode = self.is_tra_val_te == \"train\"\n        self.t_s_dict = wrap_t_s_dict(self.data_cfgs, self.is_tra_val_te)\n        self.rho = self.training_cfgs[\"hindcast_length\"]\n        self.warmup_length = self.training_cfgs[\"warmup_length\"]\n        self.horizon = self.training_cfgs[\"forecast_length\"]\n        valid_batch_mode = self.training_cfgs[\"valid_batch_mode\"]\n        # train + valid with valid_mode is train means we will use the same batch data for train and valid\n        self.is_new_batch_way = (\n            is_tra_val_te != \"valid\" or valid_batch_mode != \"train\"\n        ) and is_tra_val_te != \"train\"\n        rolling = self.evaluation_cfgs.get(\"rolling\", 0)\n        if self.evaluation_cfgs[\"hrwin\"] is None:\n            hrwin = self.rho\n        else:\n            hrwin = self.evaluation_cfgs[\"hrwin\"]\n        if self.evaluation_cfgs[\"frwin\"] is None:\n            frwin = self.horizon\n        else:\n            frwin = self.evaluation_cfgs[\"frwin\"]\n        if rolling == 0:\n            hrwin = 0 if hrwin is None else hrwin\n            frwin = self.nt - hrwin - self.warmup_length\n        if self.is_new_batch_way:\n            # we will set the batch data for valid and test\n            self.rolling = rolling\n            self.rho = hrwin\n            self.horizon = frwin\n\n    @property\n    def data_source(self):\n        source_name = self.data_cfgs[\"source_cfgs\"][\"source_name\"]\n        source_path = self.data_cfgs[\"source_cfgs\"][\"source_path\"]\n\n        # \u4f20\u9012\u9664\u4e86 source_name \u548c source_path \u4e4b\u5916\u7684\u6240\u6709\u53c2\u6570\n\n        # \u5148\u83b7\u53d6\u6240\u6709\u53c2\u6570\n        other_settings = self.data_cfgs[\"source_cfgs\"].get(\"other_settings\", {})\n\n        # \u6392\u9664 source_name, source_path\n        other_settings.pop(\"source_name\", None)\n        other_settings.pop(\"source_path\", None)\n\n        return data_sources_dict[source_name](source_path, **other_settings)\n\n    @property\n    def streamflow_name(self):\n        return self.data_cfgs[\"target_cols\"][0]\n\n    @property\n    def precipitation_name(self):\n        return self.data_cfgs[\"relevant_cols\"][0]\n\n    @property\n    def ngrid(self):\n        \"\"\"How many basins/grids in the dataset\n\n        Returns\n        -------\n        int\n            number of basins/grids\n        \"\"\"\n        return len(self.basins)\n\n    @property\n    def noutputvar(self):\n        \"\"\"How many output variables in the dataset\n        Used in evaluation.\n\n        Returns\n        -------\n        int\n            number of variables\n        \"\"\"\n        return len(self.data_cfgs[\"target_cols\"])\n\n    @property\n    def nt(self):\n        \"\"\"length of longest time series in all basins\n\n        Returns\n        -------\n        int\n            number of longest time steps\n        \"\"\"\n        if isinstance(self.t_s_dict[\"t_final_range\"][0], tuple):\n            trange_type_num = len(self.t_s_dict[\"t_final_range\"])\n            if trange_type_num not in [self.ngrid, 1]:\n                raise ValueError(\n                    \"The number of time ranges should be equal to the number of basins \"\n                    \"if you choose different time ranges for different basins\"\n                )\n            earliest_date = None\n            latest_date = None\n            for start_date_str, end_date_str in self.t_s_dict[\"t_final_range\"]:\n                date_format = detect_date_format(start_date_str)\n\n                start_date = datetime.strptime(start_date_str, date_format)\n                end_date = datetime.strptime(end_date_str, date_format)\n\n                if earliest_date is None or start_date &lt; earliest_date:\n                    earliest_date = start_date\n                if latest_date is None or end_date &gt; latest_date:\n                    latest_date = end_date\n            earliest_date = earliest_date.strftime(date_format)\n            latest_date = latest_date.strftime(date_format)\n        else:\n            trange_type_num = 1\n            earliest_date = self.t_s_dict[\"t_final_range\"][0]\n            latest_date = self.t_s_dict[\"t_final_range\"][1]\n        min_time_unit = self.data_cfgs[\"min_time_unit\"]\n        min_time_interval = self.data_cfgs[\"min_time_interval\"]\n\n        # \u8ba1\u7b97\u65f6\u95f4\u6b65\u957f\uff08\u4ee5\u5c0f\u65f6\u4e3a\u5355\u4f4d\uff09\n        unit_to_hours = {\n            \"h\": 1,\n            \"H\": 1,\n            \"d\": 24,\n            \"D\": 24,\n            \"m\": 1 / 60,\n            \"M\": 1 / 60,\n            \"s\": 1 / 3600,\n            \"S\": 1 / 3600,\n        }\n        hours_per_step = min_time_interval * unit_to_hours.get(min_time_unit, 1)\n\n        # \u89e3\u6790\u65f6\u95f4\u5b57\u7b26\u4e32\n        date_format = detect_date_format(\n            earliest_date[0]\n            if isinstance(earliest_date, (list, tuple))\n            else earliest_date\n        )\n\n        # \u83b7\u53d6\u5f00\u59cb\u548c\u7ed3\u675f\u65f6\u95f4\n        if isinstance(earliest_date, (list, tuple)):\n            s_date = datetime.strptime(\n                earliest_date[0], date_format\n            )  # \u4f7f\u7528\u7b2c\u4e00\u4e2a\u5143\u7d20\u4f5c\u4e3a\u5f00\u59cb\u65f6\u95f4\n        else:\n            s_date = datetime.strptime(earliest_date, date_format)\n\n        if isinstance(latest_date, (list, tuple)):\n            e_date = datetime.strptime(\n                latest_date[-1], date_format\n            )  # \u4f7f\u7528\u6700\u540e\u4e00\u4e2a\u5143\u7d20\u4f5c\u4e3a\u7ed3\u675f\u65f6\u95f4\n        else:\n            e_date = datetime.strptime(latest_date, date_format)\n\n        # \u8ba1\u7b97\u603b\u5c0f\u65f6\u6570\n        total_hours = (e_date - s_date).total_seconds() / 3600\n\n        # \u8ba1\u7b97\u65f6\u95f4\u6b65\u6570\n        return int(total_hours / hours_per_step) + 1\n\n    @property\n    def basins(self):\n        \"\"\"Return the basins of the dataset\"\"\"\n        return self.t_s_dict[\"sites_id\"]\n\n    @property\n    def times(self):\n        \"\"\"Return the times of all basins\n\n        TODO: Although we support get different time ranges for different basins,\n        we didn't implement the reading function for this case in _read_xyc method.\n        Hence, it's better to choose unified time range for all basins\n        \"\"\"\n        min_time_unit = self.data_cfgs[\"min_time_unit\"]\n        min_time_interval = self.data_cfgs[\"min_time_interval\"]\n        time_step = f\"{min_time_interval}{min_time_unit}\"\n        if isinstance(self.t_s_dict[\"t_final_range\"][0], tuple):\n            times_ = []\n            trange_type_num = len(self.t_s_dict[\"t_final_range\"])\n            if trange_type_num not in [self.ngrid, 1]:\n                raise ValueError(\n                    \"The number of time ranges should be equal to the number of basins \"\n                    \"if you choose different time ranges for different basins\"\n                )\n            detect_date_format(self.t_s_dict[\"t_final_range\"][0][0])\n            for start_date_str, end_date_str in self.t_s_dict[\"t_final_range\"]:\n                s_date = pd.to_datetime(start_date_str)\n                e_date = pd.to_datetime(end_date_str)\n                time_series = pd.date_range(start=s_date, end=e_date, freq=time_step)\n                times_.append(time_series)\n        else:\n            detect_date_format(self.t_s_dict[\"t_final_range\"][0])\n            trange_type_num = 1\n            s_date = pd.to_datetime(self.t_s_dict[\"t_final_range\"][0])\n            e_date = pd.to_datetime(self.t_s_dict[\"t_final_range\"][1])\n            times_ = pd.date_range(start=s_date, end=e_date, freq=time_step)\n        return times_\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, item: int):\n        \"\"\"Get one sample from the dataset with a unified return format.\n\n        Args:\n            item: The index of the sample to retrieve.\n\n        Returns:\n            A tuple of (input_data, output_data), where input_data is a tensor\n            of input features and output_data is a tensor of target values.\n        \"\"\"\n        basin, idx, actual_length = self.lookup_table[item]\n        warmup_length = self.warmup_length\n        x = self.x[basin, idx - warmup_length : idx + actual_length, :]\n        y = self.y[basin, idx : idx + actual_length, :]\n        if self.c is None or self.c.shape[-1] == 0:\n            return torch.from_numpy(x).float(), torch.from_numpy(y).float()\n        c = self.c[basin, :]\n        c = np.repeat(c, x.shape[0], axis=0).reshape(c.shape[0], -1).T\n        xc = np.concatenate((x, c), axis=1)\n        return torch.from_numpy(xc).float(), torch.from_numpy(y).float()\n\n    def _load_data(self):\n        origin_data = self._read_xyc()\n        # normalization\n        norm_data = self._normalize(origin_data)\n        # \u542f\u7528 NaN \u5904\u7406\u4ee5\u786e\u4fdd\u6570\u636e\u6e05\u6d01\n        origin_data_wonan, norm_data_wonan = self._kill_nan(origin_data, norm_data)\n        # origin_data_wonan, norm_data_wonan = origin_data, norm_data  # \u5907\u7528\uff1a\u8df3\u8fc7 NaN \u5904\u7406\n        self._trans2nparr(origin_data_wonan, norm_data_wonan)\n        self._create_lookup_table()\n\n    def _trans2nparr(self, origin_data, norm_data):\n        \"\"\"To make __getitem__ more efficient,\n        we transform x, y, c to numpy array with shape (nsample, nt, nvar)\n        \"\"\"\n        for key in origin_data.keys():\n            _origin = origin_data[key]\n            _norm = norm_data[key]\n            if _origin is None or _norm is None:\n                norm_arr = None\n                origin_arr = None\n            else:\n                norm_arr = _norm.to_numpy()\n                origin_arr = _origin.to_numpy()\n            if key == \"relevant_cols\":\n                self.x_origin = origin_arr\n                self.x = norm_arr\n            elif key == \"target_cols\":\n                self.y_origin = origin_arr\n                self.y = norm_arr\n            elif key == \"constant_cols\":\n                self.c_origin = origin_arr\n                self.c = norm_arr\n            elif key == \"forecast_cols\":\n                self.f_origin = origin_arr\n                self.f = norm_arr\n            elif key == \"global_cols\":\n                self.g_origin = origin_arr\n                self.g = norm_arr\n            elif key == \"station_cols\":\n                # GNN\u7279\u6709\u7684\u7ad9\u70b9\u6570\u636e\n                self.station_cols_origin = origin_arr\n                self.station_cols = norm_arr\n            else:\n                raise ValueError(\n                    f\"Unknown data type {key} in origin_data, \"\n                    \"it should be one of relevant_cols, target_cols, constant_cols, forecast_cols, global_cols, station_cols\"\n                )\n\n    def _normalize(\n        self,\n        origin_data,\n    ):\n        \"\"\"_summary_\n\n        Parameters\n        ----------\n        origin_data : dict\n            data with key as data type\n\n        Returns\n        -------\n        _type_\n            _description_\n        \"\"\"\n        scaler_hub = ScalerHub(\n            origin_data,\n            data_cfgs=self.data_cfgs,\n            is_tra_val_te=self.is_tra_val_te,\n            data_source=self.data_source,\n        )\n        self.target_scaler = scaler_hub.target_scaler\n        return scaler_hub.norm_data\n\n    def _selected_time_points_for_denorm(self):\n        \"\"\"get the time points for denormalization\n\n        Returns\n        -------\n            a list of time points\n        \"\"\"\n        return self.target_scaler.data_target.coords[\"time\"][self.warmup_length :]\n\n    def denormalize(self, norm_data, pace_idx=None):\n        \"\"\"Denormalize the norm_data\n\n        Parameters\n        ----------\n        norm_data : np.ndarray\n            batch-first data\n        pace_idx : int, optional\n            which pace to show, by default None\n            sometimes we may have multiple results for one time period and we flatten them\n            so we need a temp time to replace real one\n\n        Returns\n        -------\n        xr.Dataset\n            denormlized data\n        \"\"\"\n        target_scaler = self.target_scaler\n        target_data = target_scaler.data_target\n        # the units are dimensionless for pure DL models\n        units = {k: \"dimensionless\" for k in target_data.attrs[\"units\"].keys()}\n        # mainly to get information about the time points of norm_data\n        selected_time_points = self._selected_time_points_for_denorm()\n        selected_data = target_data.sel(time=selected_time_points)\n\n        # \u5904\u7406\u4e09\u7ef4\u6570\u636e (basin, time, variable)\n        if norm_data.ndim == 3:\n            coords = {\n                \"basin\": selected_data.coords[\"basin\"],\n                \"time\": selected_data.coords[\"time\"],\n                \"variable\": selected_data.coords[\"variable\"],\n            }\n            dims = [\"basin\", \"time\", \"variable\"]\n            # add\n            if isinstance(selected_time_points, xr.DataArray):\n                # \u83b7\u53d6 target_data \u7684\u65f6\u95f4\u8f74\n                time_coords = target_data.coords[\"time\"].values\n                # \u627e\u5230 selected_time_points \u5bf9\u5e94\u7684\u6574\u6570\u7d22\u5f15\n                selected_indices = np.where(np.isin(time_coords, selected_time_points))[\n                    0\n                ]\n            else:\n                # \u5982\u679c selected_time_points \u5df2\u7ecf\u662f\u6574\u6570\u7d22\u5f15\uff0c\u76f4\u63a5\u4f7f\u7528\n                selected_indices = selected_time_points\n\n            # \u786e\u4fdd\u7d22\u5f15\u4e0d\u8d8a\u754c\n            max_idx = norm_data.shape[1] - 1\n            selected_indices = np.clip(selected_indices, 0, max_idx)\n            if norm_data.shape[1] != len(selected_data.coords[\"time\"]):\n                norm_data_3d = norm_data[:, selected_indices, :]\n            else:\n                norm_data_3d = norm_data\n\n        # \u5904\u7406\u56db\u7ef4\u6570\u636e\n        elif norm_data.ndim == 4:\n            # Check if the data is organized by basins\n            if self.evaluation_cfgs[\"evaluator\"][\"recover_mode\"] == \"bybasins\":\n                # Shape: (basin_num, i_e_time_length, forecast_length, nf)\n                basin_num, i_e_time_length, forecast_length, nf = norm_data.shape\n\n                # If pace_idx is specified, select the specific forecast step\n                if (\n                    pace_idx is not None\n                    and pace_idx != np.nan\n                    and pace_idx &gt;= 0\n                    and pace_idx &lt; forecast_length\n                ):\n                    norm_data_3d = norm_data[:, :, pace_idx, :]\n                    # \u521b\u5efa\u65b0\u7684\u5750\u6807\n                    # \u4fee\u6539\u8fd9\u91cc\uff1a\u786e\u4fddbasin\u5750\u6807\u957f\u5ea6\u4e0e\u6570\u636e\u7ef4\u5ea6\u5339\u914d\n                    if basin_num == 1 and len(selected_data.coords[\"basin\"]) &gt; 1:\n                        # \u5f53\u53ea\u6709\u4e00\u4e2a\u6d41\u57df\u65f6\uff0c\u9009\u62e9\u7b2c\u4e00\u4e2a\u6d41\u57df\u7684\u5750\u6807\n                        basin_coord = selected_data.coords[\"basin\"].values[:1]\n                    else:\n                        basin_coord = selected_data.coords[\"basin\"].values[:basin_num]\n\n                    coords = {\n                        \"basin\": basin_coord,\n                        \"time\": selected_data.coords[\"time\"][:i_e_time_length],\n                        \"variable\": selected_data.coords[\"variable\"],\n                    }\n                else:\n                    # \u5982\u679c\u6ca1\u6709\u6307\u5b9apace_idx\uff0c\u5219\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u7ef4\u5ea6'horizon'\n                    norm_data_3d = norm_data.reshape(\n                        basin_num, i_e_time_length * forecast_length, nf\n                    )\n                    # \u521b\u5efa\u65b0\u7684\u65f6\u95f4\u5750\u6807\uff0c\u91cd\u590di_e_time_length\u6b21\n                    new_times = []\n                    for i in range(forecast_length):\n                        if i &lt; len(selected_data.coords[\"time\"]):\n                            new_times.extend(\n                                selected_data.coords[\"time\"][:i_e_time_length]\n                            )\n\n                    # \u786e\u4fdd\u65f6\u95f4\u5750\u6807\u957f\u5ea6\u4e0e\u6570\u636e\u5339\u914d\n                    if len(new_times) &gt; i_e_time_length * forecast_length:\n                        new_times = new_times[: i_e_time_length * forecast_length]\n                    elif len(new_times) &lt; i_e_time_length * forecast_length:\n                        # \u5982\u679c\u65f6\u95f4\u5750\u6807\u4e0d\u8db3\uff0c\u4f7f\u7528\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u70b9\u586b\u5145\n                        last_time = (\n                            new_times[-1]\n                            if new_times\n                            else selected_data.coords[\"time\"][0]\n                        )\n                        while len(new_times) &lt; i_e_time_length * forecast_length:\n                            new_times.append(last_time)\n\n                    # \u4fee\u6539\u8fd9\u91cc\uff1a\u786e\u4fddbasin\u5750\u6807\u957f\u5ea6\u4e0e\u6570\u636e\u7ef4\u5ea6\u5339\u914d\n                    if basin_num == 1 and len(selected_data.coords[\"basin\"]) &gt; 1:\n                        basin_coord = selected_data.coords[\"basin\"].values[:1]\n                    else:\n                        basin_coord = selected_data.coords[\"basin\"].values[:basin_num]\n\n                    coords = {\n                        \"basin\": basin_coord,\n                        \"time\": new_times,\n                        \"variable\": selected_data.coords[\"variable\"],\n                    }\n            else:  # byforecast\u6a21\u5f0f\n                # \u5f62\u72b6\u4e3a (forecast_length, basin_num, i_e_time_length, nf)\n                forecast_length, basin_num, i_e_time_length, nf = norm_data.shape\n\n                # \u5982\u679c\u6307\u5b9a\u4e86pace_idx\uff0c\u5219\u9009\u62e9\u7279\u5b9a\u7684\u9884\u6d4b\u6b65\u957f\n                if (\n                    pace_idx is not None\n                    and pace_idx != np.nan\n                    and pace_idx &gt;= 0\n                    and pace_idx &lt; forecast_length\n                ):\n                    norm_data_3d = norm_data[pace_idx]\n                    # \u4fee\u6539\u8fd9\u91cc\uff1a\u786e\u4fddbasin\u5750\u6807\u957f\u5ea6\u4e0e\u6570\u636e\u7ef4\u5ea6\u5339\u914d\n                    if basin_num == 1 and len(selected_data.coords[\"basin\"]) &gt; 1:\n                        basin_coord = selected_data.coords[\"basin\"].values[:1]\n                    else:\n                        basin_coord = selected_data.coords[\"basin\"].values[:basin_num]\n\n                    coords = {\n                        \"basin\": basin_coord,\n                        \"time\": selected_data.coords[\"time\"][:i_e_time_length],\n                        \"variable\": selected_data.coords[\"variable\"],\n                    }\n                else:\n                    # If pace_idx is not specified, create a new dimension 'horizon'\n                    # Reshape (forecast_length, basin_num, i_e_time_length, nf) -&gt; (basin_num, forecast_length * i_e_time_length, nf)\n                    norm_data_3d = np.transpose(norm_data, (1, 0, 2, 3)).reshape(\n                        basin_num, forecast_length * i_e_time_length, nf\n                    )\n\n                    # \u521b\u5efa\u65b0\u7684\u65f6\u95f4\u5750\u6807\n                    new_times = []\n                    for i in range(forecast_length):\n                        if i &lt; len(selected_data.coords[\"time\"]):\n                            new_times.extend(\n                                selected_data.coords[\"time\"][:i_e_time_length]\n                            )\n\n                    # \u786e\u4fdd\u65f6\u95f4\u5750\u6807\u957f\u5ea6\u4e0e\u6570\u636e\u5339\u914d\n                    if len(new_times) &gt; forecast_length * i_e_time_length:\n                        new_times = new_times[: forecast_length * i_e_time_length]\n                    elif len(new_times) &lt; forecast_length * i_e_time_length:\n                        # \u5982\u679c\u65f6\u95f4\u5750\u6807\u4e0d\u8db3\uff0c\u4f7f\u7528\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u70b9\u586b\u5145\n                        last_time = (\n                            new_times[-1]\n                            if new_times\n                            else selected_data.coords[\"time\"][0]\n                        )\n                        while len(new_times) &lt; forecast_length * i_e_time_length:\n                            new_times.append(last_time)\n\n                    # \u4fee\u6539\u8fd9\u91cc\uff1a\u786e\u4fddbasin\u5750\u6807\u957f\u5ea6\u4e0e\u6570\u636e\u7ef4\u5ea6\u5339\u914d\n                    if basin_num == 1 and len(selected_data.coords[\"basin\"]) &gt; 1:\n                        basin_coord = selected_data.coords[\"basin\"].values[:1]\n                    else:\n                        basin_coord = selected_data.coords[\"basin\"].values[:basin_num]\n\n                    coords = {\n                        \"basin\": basin_coord,\n                        \"time\": new_times,\n                        \"variable\": selected_data.coords[\"variable\"],\n                    }\n            dims = [\"basin\", \"time\", \"variable\"]\n        else:\n            coords = selected_data.coords\n            dims = selected_data.dims\n            norm_data_3d = norm_data\n\n        # create DataArray and inverse transform\n        denorm_xr_ds = target_scaler.inverse_transform(\n            xr.DataArray(\n                norm_data_3d,\n                dims=dims,\n                coords=coords,\n                attrs={\"units\": units},\n            )\n        )\n        return set_unit_to_var(denorm_xr_ds)\n\n    def _to_dataarray_with_unit(self, *args):\n        \"\"\"Convert xarray datasets to xarray data arrays and set units for each variable.\n\n        Parameters\n        ----------\n        *args : xr.Dataset\n            Any number of xarray dataset inputs.\n\n        Returns\n        -------\n        tuple\n            A tuple of converted data arrays, with the same number as the input parameters.\n        \"\"\"\n        results = []\n        for ds in args:\n            if ds is not None:\n                # First convert some string-type data to floating-point type\n                results.append(self._trans2da_and_setunits(ds))\n            else:\n                results.append(None)\n        return tuple(results)\n\n    def _check_ts_xrds_unit(self, data_forcing_ds, data_output_ds):\n        \"\"\"Check timeseries xarray dataset unit and convert if necessary\n\n        Parameters\n        ----------\n        data_forcing_ds : xr.Dataset\n            the forcing data\n        data_output_ds : xr.Dataset\n            outputs including streamflow data\n        \"\"\"\n\n        def standardize_unit(unit):\n            unit = unit.lower()  # convert to lower case\n            unit = re.sub(r\"day\", \"d\", unit)\n            unit = re.sub(r\"hour\", \"h\", unit)\n            return unit\n\n        streamflow_unit = data_output_ds[self.streamflow_name].attrs[\"units\"]\n        prcp_unit = data_forcing_ds[self.precipitation_name].attrs[\"units\"]\n\n        standardized_streamflow_unit = standardize_unit(streamflow_unit)\n        standardized_prcp_unit = standardize_unit(prcp_unit)\n        if standardized_streamflow_unit != standardized_prcp_unit:\n            streamflow_dataset = data_output_ds[[self.streamflow_name]]\n            converted_streamflow_dataset = streamflow_unit_conv(\n                streamflow_dataset,\n                self.data_source.read_area(self.t_s_dict[\"sites_id\"]),\n                target_unit=prcp_unit,\n                source_unit=streamflow_unit,\n            )\n            data_output_ds[self.streamflow_name] = converted_streamflow_dataset[\n                self.streamflow_name\n            ]\n        return data_forcing_ds, data_output_ds\n\n    def _read_xyc(self):\n        \"\"\"Read x, y, c data from data source\n\n        Returns\n        -------\n        dict\n            data with key as data type\n            the dim must be (basin, time, lead_step, variable) for 4-d xr array;\n            the dim must be (basin, time, variable) for 3-d xr array;\n            the dim must be (basin, variable) for 2-d xr array;\n        \"\"\"\n        # Check if we have multiple time periods (for multi-period training)\n        t_range = self.t_s_dict[\"t_final_range\"]\n\n        # Check if first element is a list/tuple (indicating multiple periods)\n        if isinstance(t_range[0], (list, tuple)):\n            # Validate multi-period format\n            self._validate_multi_period_format(t_range)\n\n            # Multiple periods case - can be any number of periods\n            all_data = None\n            for start_date, end_date in t_range:\n                period_data = self._read_xyc_period(start_date, end_date)\n                if all_data is None:\n                    all_data = period_data\n                else:\n                    # Concatenate along time dimension\n                    for key in period_data:\n                        # \u786e\u4fdd\u4e24\u4e2a\u6570\u636e\u96c6\u7684\u65f6\u95f4\u7ef4\u5ea6\u90fd\u662f\u5b57\u7b26\u4e32\u7c7b\u578b\n                        if all_data[key] is not None and period_data[key] is not None:\n                            if not isinstance(all_data[key].time.values[0], str):\n                                all_data[key][\"time\"] = all_data[key].time.astype(str)\n                            if not isinstance(period_data[key].time.values[0], str):\n                                period_data[key][\"time\"] = period_data[key].time.astype(\n                                    str\n                                )\n\n                            all_data[key] = xr.concat(\n                                [all_data[key], period_data[key]], dim=\"time\"\n                            )\n            return all_data\n        else:\n            # Single period case (existing behavior)\n            start_date = t_range[0]\n            end_date = t_range[1]\n            return self._read_xyc_period(start_date, end_date)\n\n    def _read_xyc_period(self, start_date, end_date):\n        \"\"\"Template method for reading x, y, c data for a specific time period\n\n        This method can be overridden by subclasses to customize how data is read\n        for each time period while keeping the multi-period handling logic in the parent class.\n\n        Parameters\n        ----------\n        start_date : str\n            start time\n        end_date : str\n            end time\n\n        Returns\n        -------\n        dict\n            Dictionary containing relevant_cols, target_cols, and constant_cols data\n        \"\"\"\n        # Default implementation: delegate to the original method\n        return self._read_xyc_specified_time(start_date, end_date)\n\n    def _validate_multi_period_format(self, t_range):\n        \"\"\"Validate format of multi-period time ranges\n\n        Parameters\n        ----------\n        t_range : list\n            List of time periods, where each period should be [start_date, end_date]\n\n        Raises\n        ------\n        ValueError\n            If any period doesn't have exactly 2 elements (start_date, end_date)\n        \"\"\"\n        for i, period in enumerate(t_range):\n            if not isinstance(period, (list, tuple)) or len(period) != 2:\n                raise ValueError(\n                    f\"Period {i} must be a list/tuple with exactly 2 elements (start_date, end_date), got: {period}\"\n                )\n\n    def _rm_timeunit_key(self, ds_):\n        \"\"\"this means the data source return a dict with key as time_unit\n            in this BaseDataset, we only support unified time range for all basins, so we chose the first key\n            TODO: maybe this could be refactored better\n\n        Parameters\n        ----------\n        ds_ : dict\n            the xarray data with time_unit as key\n\n        Returns\n        ----------\n        ds_ : xr.Dataset\n            the output data without time_unit\n        \"\"\"\n        if isinstance(ds_, dict):\n            ds_ = ds_[list(ds_.keys())[0]]\n        return ds_\n\n    def _read_xyc_specified_time(self, start_date, end_date):\n        \"\"\"Read x, y, c data from data source with specified time range\n        We set this function as sometimes we need adjust the time range for some specific dataset,\n        such as seq2seq dataset (it needs one more period for the end of the time range)\n\n        Parameters\n        ----------\n        start_date : str\n            start time\n        end_date : str\n            end time\n        \"\"\"\n        data_forcing_ds_ = self.data_source.read_ts_xrdataset(\n            self.t_s_dict[\"sites_id\"],\n            [start_date, end_date],\n            self.data_cfgs[\"relevant_cols\"],\n        )\n        # y\n        data_output_ds_ = self.data_source.read_ts_xrdataset(\n            self.t_s_dict[\"sites_id\"],\n            [start_date, end_date],\n            self.data_cfgs[\"target_cols\"],\n        )\n        print(data_output_ds_)\n        data_forcing_ds_ = self._rm_timeunit_key(data_forcing_ds_)\n        data_output_ds_ = self._rm_timeunit_key(data_output_ds_)\n        data_forcing_ds, data_output_ds = self._check_ts_xrds_unit(\n            data_forcing_ds_, data_output_ds_\n        )\n        # c\n        data_attr_ds = self.data_source.read_attr_xrdataset(\n            self.t_s_dict[\"sites_id\"],\n            self.data_cfgs[\"constant_cols\"],\n            all_number=True,\n        )\n        x_origin, y_origin, c_origin = self._to_dataarray_with_unit(\n            data_forcing_ds, data_output_ds, data_attr_ds\n        )\n        return {\n            \"relevant_cols\": x_origin.transpose(\"basin\", \"time\", \"variable\"),\n            \"target_cols\": y_origin.transpose(\"basin\", \"time\", \"variable\"),\n            \"constant_cols\": (\n                c_origin.transpose(\"basin\", \"variable\")\n                if c_origin is not None\n                else None\n            ),\n        }\n\n    def _trans2da_and_setunits(self, ds):\n        \"\"\"Set units for dataarray transfromed from dataset\"\"\"\n        result = ds.to_array(dim=\"variable\")\n        units_dict = {\n            var: ds[var].attrs[\"units\"]\n            for var in ds.variables\n            if \"units\" in ds[var].attrs\n        }\n        result.attrs[\"units\"] = units_dict\n        return result\n\n    def _kill_nan(self, origin_data, norm_data):\n        \"\"\"This function is used to remove NaN values in the original data and its normalized data.\n\n        Parameters\n        ----------\n        origin_data : dict\n            the original data\n        norm_data : dict\n            the normalized data\n\n        Returns\n        -------\n        dict, dict\n            the original data and normalized data after removing NaN values\n        \"\"\"\n        data_cfgs = self.data_cfgs\n        origins_wonan = {}\n        norms_wonan = {}\n        for key in origin_data.keys():\n            _origin = origin_data[key]\n            _norm = norm_data[key]\n            if _origin is None or _norm is None:\n                origins_wonan[key] = None\n                norms_wonan[key] = None\n                continue\n            kill_way = \"interpolate\"\n            if key == \"relevant_cols\":\n                rm_nan = data_cfgs[\"relevant_rm_nan\"]\n            elif key == \"target_cols\":\n                rm_nan = data_cfgs[\"target_rm_nan\"]\n            elif key == \"constant_cols\":\n                rm_nan = data_cfgs[\"constant_rm_nan\"]\n                kill_way = \"mean\"\n            elif key == \"forecast_cols\":\n                rm_nan = data_cfgs[\"forecast_rm_nan\"]\n                kill_way = \"lead_step\"\n            elif key == \"global_cols\":\n                rm_nan = data_cfgs[\"global_rm_nan\"]\n            elif key == \"station_cols\":\n                rm_nan = data_cfgs.get(\"station_rm_nan\")\n            else:\n                raise ValueError(\n                    f\"Unknown data type {key} in origin_data, \"\n                    \"it should be one of relevant_cols, target_cols, constant_cols, forecast_cols, global_cols and station_cols\"\n                )\n\n            if rm_nan:\n                norm = self._kill_1type_nan(\n                    _norm,\n                    kill_way,\n                    \"original data\",\n                    \"nan_filled data\",\n                )\n                origin = self._kill_1type_nan(\n                    _origin,\n                    kill_way,\n                    \"original data\",\n                    \"nan_filled data\",\n                )\n            else:\n                norm = _norm\n                origin = _origin\n            if key == \"target_cols\" or not rm_nan:\n                warn_if_nan(origin, nan_mode=\"all\", data_name=\"nan_filled target data\")\n                warn_if_nan(norm, nan_mode=\"all\", data_name=\"nan_filled target data\")\n            else:\n                warn_if_nan(origin, nan_mode=\"any\", data_name=\"nan_filled input data\")\n                warn_if_nan(norm, nan_mode=\"any\", data_name=\"nan_filled input data\")\n            origins_wonan[key] = origin\n            norms_wonan[key] = norm\n        return origins_wonan, norms_wonan\n\n    def _kill_1type_nan(self, the_data, fill_nan, data_name_before, data_name_after):\n        is_any_nan = warn_if_nan(the_data, data_name=data_name_before)\n        if not is_any_nan:\n            return the_data\n        # As input, we cannot have NaN values\n        the_filled_data = _fill_gaps_da(the_data, fill_nan=fill_nan)\n        warn_if_nan(the_filled_data, data_name=data_name_after)\n        return the_filled_data\n\n    def _create_lookup_table(self):\n        lookup = []\n        # list to collect basins ids of basins without a single training sample\n        basin_coordinates = len(self.t_s_dict[\"sites_id\"])\n        rho = self.rho\n        warmup_length = self.warmup_length\n        horizon = self.horizon\n        # NOTE: we set seq_len to rho + horizon instead of warmup_length + rho + horizon\n        seq_len = rho + horizon\n        max_time_length = self.nt\n        variable_length_cfgs = self.training_cfgs.get(\"variable_length_cfgs\", {})\n        use_variable_length = variable_length_cfgs.get(\"use_variable_length\", False)\n        variable_length_type = variable_length_cfgs.get(\n            \"variable_length_type\", \"dynamic\"\n        )  # only used for case when use_variable_length is True\n        fixed_lengths = variable_length_cfgs.get(\"fixed_lengths\", [365, 1095, 1825])\n        # Use fixed type variable length if enabled and type is fixed\n        is_fixed_length_train = use_variable_length and variable_length_type == \"fixed\"\n        for basin in tqdm(range(basin_coordinates), file=sys.stdout, disable=False):\n            if not self.train_mode:\n                # we don't need to ignore those with full nan in target vars for prediction without loss calculation\n                # all samples should be included so that we can recover results to specified basins easily\n                lookup.extend(\n                    (basin, f, seq_len)\n                    for f in range(warmup_length, max_time_length - rho - horizon + 1)\n                )\n            else:\n                # some dataloader load data with warmup period, so leave some periods for it\n                # [warmup_len] -&gt; time_start -&gt; [rho] -&gt; [horizon]\n                #                       window: \\-----------------/ meaning rho + horizon\n                nan_array = np.isnan(self.y[basin, :, :])\n                if is_fixed_length_train:\n                    for window in fixed_lengths:\n                        lookup.extend(\n                            (basin, f, window)\n                            for f in range(\n                                warmup_length,\n                                max_time_length - window + 1,\n                            )\n                            # if all nan in window, we skip this sample\n                            if not np.all(nan_array[f : f + window])\n                        )\n                else:\n                    lookup.extend(\n                        (basin, f, seq_len)\n                        for f in range(\n                            warmup_length, max_time_length - rho - horizon + 1\n                        )\n                        # if all nan in rho + horizon window, we skip this sample\n                        if not np.all(nan_array[f : f + rho + horizon])\n                    )\n        self.lookup_table = dict(enumerate(lookup))\n        self.num_samples = len(self.lookup_table)\n\n    def _create_multi_len_lookup_table(self):\n        \"\"\"\n        Create a lookup table for multi-length training\n        TODO: not fully tested\n        \"\"\"\n        lookup = []\n        # list to collect basins ids of basins without a single training sample\n        basin_coordinates = len(self.t_s_dict[\"sites_id\"])\n        rho = self.rho\n        warmup_length = self.warmup_length\n        horizon = self.horizon\n        seq_len = warmup_length + rho + horizon\n        max_time_length = self.nt\n        variable_length_cfgs = self.training_cfgs.get(\"variable_length_cfgs\", {})\n        use_variable_length = variable_length_cfgs.get(\"use_variable_length\", False)\n        variable_length_type = variable_length_cfgs.get(\n            \"variable_length_type\", \"dynamic\"\n        )\n        fixed_lengths = variable_length_cfgs.get(\"fixed_lengths\", [365, 1095, 1825])\n        # Use fixed type variable length if enabled and type is fixed\n        is_fixed_length_train = use_variable_length and variable_length_type == \"fixed\"\n\n        # \u521d\u59cb\u5316\u4e0d\u540c\u957f\u5ea6\u7684lookup\u8868\n        self.lookup_tables_by_length = {length: [] for length in fixed_lengths}\n\n        # New: Global lookup table to map a single index to (window_length, index_within_that_window_length_table)\n        self.global_lookup_table_indices = []\n\n        for basin in tqdm(range(basin_coordinates), file=sys.stdout, disable=False):\n            if not self.train_mode:\n                # For prediction, we still use the original rho for simplicity if multi_length_training is enabled\n                # or we can extend this logic to support multi-length prediction if needed.\n                # For now, let's assume prediction uses a fixed rho or is handled differently.\n                # If multi_length_training is active, we might need to decide which window_len to use for prediction.\n                # For now, let's stick to the original logic for train_mode=False\n                lookup.extend(\n                    (basin, f, seq_len)\n                    for f in range(warmup_length, max_time_length - rho - horizon + 1)\n                )\n            else:\n                # some dataloader load data with warmup period, so leave some periods for it\n                # [warmup_len] -&gt; time_start -&gt; [rho] -&gt; [horizon]\n                nan_array = np.isnan(self.y[basin, :, :])\n                if is_fixed_length_train:\n                    for window in fixed_lengths:\n                        for f in range(\n                            warmup_length, max_time_length - window - horizon + 1\n                        ):\n                            # \u68c0\u67e5\u76ee\u6807\u533a\u95f4\u5185\u662f\u5426\u5168\u4e3anan\n                            if not np.all(nan_array[f + window : f + window + horizon]):\n                                # \u8bb0\u5f55 (basin, \u8d77\u59cb\u4f4d\u7f6e) \u5230\u5bf9\u5e94\u7a97\u53e3\u957f\u5ea6\u7684 lookup table\n                                self.lookup_tables_by_length[window].append((basin, f))\n                                # \u8bb0\u5f55 (\u7a97\u53e3\u957f\u5ea6, \u5728\u8be5\u7a97\u53e3\u957f\u5ea6 lookup table \u4e2d\u7684\u7d22\u5f15) \u5230\u5168\u5c40\u7d22\u5f15\u8868\n                                self.global_lookup_table_indices.append(\n                                    (\n                                        window,\n                                        len(self.lookup_tables_by_length[window]) - 1,\n                                    )\n                                )\n                else:\n                    lookup.extend(\n                        (basin, f, seq_len)\n                        for f in range(\n                            warmup_length, max_time_length - rho - horizon + 1\n                        )\n                        if not np.all(nan_array[f + rho : f + rho + horizon])\n                    )\n\n        if is_fixed_length_train and self.train_mode:\n            # If fixed-length training is enabled and in train mode, use the global lookup table\n            self.lookup_table = dict(enumerate(self.global_lookup_table_indices))\n            self.num_samples = len(self.global_lookup_table_indices)\n        else:\n            # Otherwise, use the original lookup table (for fixed length training or prediction)\n            self.lookup_table = dict(enumerate(lookup))\n            self.num_samples = len(self.lookup_table)\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.BaseDataset.basins","title":"<code>basins</code>  <code>property</code> <code>readonly</code>","text":"<p>Return the basins of the dataset</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.BaseDataset.ngrid","title":"<code>ngrid</code>  <code>property</code> <code>readonly</code>","text":"<p>How many basins/grids in the dataset</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.BaseDataset.ngrid--returns","title":"Returns","text":"<p>int     number of basins/grids</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.BaseDataset.noutputvar","title":"<code>noutputvar</code>  <code>property</code> <code>readonly</code>","text":"<p>How many output variables in the dataset Used in evaluation.</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.BaseDataset.noutputvar--returns","title":"Returns","text":"<p>int     number of variables</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.BaseDataset.nt","title":"<code>nt</code>  <code>property</code> <code>readonly</code>","text":"<p>length of longest time series in all basins</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.BaseDataset.nt--returns","title":"Returns","text":"<p>int     number of longest time steps</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.BaseDataset.times","title":"<code>times</code>  <code>property</code> <code>readonly</code>","text":"<p>Return the times of all basins</p> <p>TODO: Although we support get different time ranges for different basins, we didn't implement the reading function for this case in _read_xyc method. Hence, it's better to choose unified time range for all basins</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.BaseDataset.__getitem__","title":"<code>__getitem__(self, item)</code>  <code>special</code>","text":"<p>Get one sample from the dataset with a unified return format.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>int</code> <p>The index of the sample to retrieve.</p> required <p>Returns:</p> Type Description <p>A tuple of (input_data, output_data), where input_data is a tensor of input features and output_data is a tensor of target values.</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>def __getitem__(self, item: int):\n    \"\"\"Get one sample from the dataset with a unified return format.\n\n    Args:\n        item: The index of the sample to retrieve.\n\n    Returns:\n        A tuple of (input_data, output_data), where input_data is a tensor\n        of input features and output_data is a tensor of target values.\n    \"\"\"\n    basin, idx, actual_length = self.lookup_table[item]\n    warmup_length = self.warmup_length\n    x = self.x[basin, idx - warmup_length : idx + actual_length, :]\n    y = self.y[basin, idx : idx + actual_length, :]\n    if self.c is None or self.c.shape[-1] == 0:\n        return torch.from_numpy(x).float(), torch.from_numpy(y).float()\n    c = self.c[basin, :]\n    c = np.repeat(c, x.shape[0], axis=0).reshape(c.shape[0], -1).T\n    xc = np.concatenate((x, c), axis=1)\n    return torch.from_numpy(xc).float(), torch.from_numpy(y).float()\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.BaseDataset.__init__","title":"<code>__init__(self, cfgs, is_tra_val_te)</code>  <code>special</code>","text":""},{"location":"api/datasets/#torchhydro.datasets.data_sets.BaseDataset.__init__--parameters","title":"Parameters","text":"<p>cfgs     configs, including data and training + evaluation settings     which will be used for organizing batch data is_tra_val_te     train, vaild or test</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>def __init__(self, cfgs: dict, is_tra_val_te: str):\n    \"\"\"\n    Parameters\n    ----------\n    cfgs\n        configs, including data and training + evaluation settings\n        which will be used for organizing batch data\n    is_tra_val_te\n        train, vaild or test\n    \"\"\"\n    super(BaseDataset, self).__init__()\n    self.data_cfgs = cfgs[\"data_cfgs\"]\n    self.training_cfgs = cfgs[\"training_cfgs\"]\n    self.evaluation_cfgs = cfgs[\"evaluation_cfgs\"]\n    self._pre_load_data(is_tra_val_te)\n    # load and preprocess data\n    self._load_data()\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.BaseDataset.denormalize","title":"<code>denormalize(self, norm_data, pace_idx=None)</code>","text":"<p>Denormalize the norm_data</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.BaseDataset.denormalize--parameters","title":"Parameters","text":"<p>norm_data : np.ndarray     batch-first data pace_idx : int, optional     which pace to show, by default None     sometimes we may have multiple results for one time period and we flatten them     so we need a temp time to replace real one</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.BaseDataset.denormalize--returns","title":"Returns","text":"<p>xr.Dataset     denormlized data</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>def denormalize(self, norm_data, pace_idx=None):\n    \"\"\"Denormalize the norm_data\n\n    Parameters\n    ----------\n    norm_data : np.ndarray\n        batch-first data\n    pace_idx : int, optional\n        which pace to show, by default None\n        sometimes we may have multiple results for one time period and we flatten them\n        so we need a temp time to replace real one\n\n    Returns\n    -------\n    xr.Dataset\n        denormlized data\n    \"\"\"\n    target_scaler = self.target_scaler\n    target_data = target_scaler.data_target\n    # the units are dimensionless for pure DL models\n    units = {k: \"dimensionless\" for k in target_data.attrs[\"units\"].keys()}\n    # mainly to get information about the time points of norm_data\n    selected_time_points = self._selected_time_points_for_denorm()\n    selected_data = target_data.sel(time=selected_time_points)\n\n    # \u5904\u7406\u4e09\u7ef4\u6570\u636e (basin, time, variable)\n    if norm_data.ndim == 3:\n        coords = {\n            \"basin\": selected_data.coords[\"basin\"],\n            \"time\": selected_data.coords[\"time\"],\n            \"variable\": selected_data.coords[\"variable\"],\n        }\n        dims = [\"basin\", \"time\", \"variable\"]\n        # add\n        if isinstance(selected_time_points, xr.DataArray):\n            # \u83b7\u53d6 target_data \u7684\u65f6\u95f4\u8f74\n            time_coords = target_data.coords[\"time\"].values\n            # \u627e\u5230 selected_time_points \u5bf9\u5e94\u7684\u6574\u6570\u7d22\u5f15\n            selected_indices = np.where(np.isin(time_coords, selected_time_points))[\n                0\n            ]\n        else:\n            # \u5982\u679c selected_time_points \u5df2\u7ecf\u662f\u6574\u6570\u7d22\u5f15\uff0c\u76f4\u63a5\u4f7f\u7528\n            selected_indices = selected_time_points\n\n        # \u786e\u4fdd\u7d22\u5f15\u4e0d\u8d8a\u754c\n        max_idx = norm_data.shape[1] - 1\n        selected_indices = np.clip(selected_indices, 0, max_idx)\n        if norm_data.shape[1] != len(selected_data.coords[\"time\"]):\n            norm_data_3d = norm_data[:, selected_indices, :]\n        else:\n            norm_data_3d = norm_data\n\n    # \u5904\u7406\u56db\u7ef4\u6570\u636e\n    elif norm_data.ndim == 4:\n        # Check if the data is organized by basins\n        if self.evaluation_cfgs[\"evaluator\"][\"recover_mode\"] == \"bybasins\":\n            # Shape: (basin_num, i_e_time_length, forecast_length, nf)\n            basin_num, i_e_time_length, forecast_length, nf = norm_data.shape\n\n            # If pace_idx is specified, select the specific forecast step\n            if (\n                pace_idx is not None\n                and pace_idx != np.nan\n                and pace_idx &gt;= 0\n                and pace_idx &lt; forecast_length\n            ):\n                norm_data_3d = norm_data[:, :, pace_idx, :]\n                # \u521b\u5efa\u65b0\u7684\u5750\u6807\n                # \u4fee\u6539\u8fd9\u91cc\uff1a\u786e\u4fddbasin\u5750\u6807\u957f\u5ea6\u4e0e\u6570\u636e\u7ef4\u5ea6\u5339\u914d\n                if basin_num == 1 and len(selected_data.coords[\"basin\"]) &gt; 1:\n                    # \u5f53\u53ea\u6709\u4e00\u4e2a\u6d41\u57df\u65f6\uff0c\u9009\u62e9\u7b2c\u4e00\u4e2a\u6d41\u57df\u7684\u5750\u6807\n                    basin_coord = selected_data.coords[\"basin\"].values[:1]\n                else:\n                    basin_coord = selected_data.coords[\"basin\"].values[:basin_num]\n\n                coords = {\n                    \"basin\": basin_coord,\n                    \"time\": selected_data.coords[\"time\"][:i_e_time_length],\n                    \"variable\": selected_data.coords[\"variable\"],\n                }\n            else:\n                # \u5982\u679c\u6ca1\u6709\u6307\u5b9apace_idx\uff0c\u5219\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u7ef4\u5ea6'horizon'\n                norm_data_3d = norm_data.reshape(\n                    basin_num, i_e_time_length * forecast_length, nf\n                )\n                # \u521b\u5efa\u65b0\u7684\u65f6\u95f4\u5750\u6807\uff0c\u91cd\u590di_e_time_length\u6b21\n                new_times = []\n                for i in range(forecast_length):\n                    if i &lt; len(selected_data.coords[\"time\"]):\n                        new_times.extend(\n                            selected_data.coords[\"time\"][:i_e_time_length]\n                        )\n\n                # \u786e\u4fdd\u65f6\u95f4\u5750\u6807\u957f\u5ea6\u4e0e\u6570\u636e\u5339\u914d\n                if len(new_times) &gt; i_e_time_length * forecast_length:\n                    new_times = new_times[: i_e_time_length * forecast_length]\n                elif len(new_times) &lt; i_e_time_length * forecast_length:\n                    # \u5982\u679c\u65f6\u95f4\u5750\u6807\u4e0d\u8db3\uff0c\u4f7f\u7528\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u70b9\u586b\u5145\n                    last_time = (\n                        new_times[-1]\n                        if new_times\n                        else selected_data.coords[\"time\"][0]\n                    )\n                    while len(new_times) &lt; i_e_time_length * forecast_length:\n                        new_times.append(last_time)\n\n                # \u4fee\u6539\u8fd9\u91cc\uff1a\u786e\u4fddbasin\u5750\u6807\u957f\u5ea6\u4e0e\u6570\u636e\u7ef4\u5ea6\u5339\u914d\n                if basin_num == 1 and len(selected_data.coords[\"basin\"]) &gt; 1:\n                    basin_coord = selected_data.coords[\"basin\"].values[:1]\n                else:\n                    basin_coord = selected_data.coords[\"basin\"].values[:basin_num]\n\n                coords = {\n                    \"basin\": basin_coord,\n                    \"time\": new_times,\n                    \"variable\": selected_data.coords[\"variable\"],\n                }\n        else:  # byforecast\u6a21\u5f0f\n            # \u5f62\u72b6\u4e3a (forecast_length, basin_num, i_e_time_length, nf)\n            forecast_length, basin_num, i_e_time_length, nf = norm_data.shape\n\n            # \u5982\u679c\u6307\u5b9a\u4e86pace_idx\uff0c\u5219\u9009\u62e9\u7279\u5b9a\u7684\u9884\u6d4b\u6b65\u957f\n            if (\n                pace_idx is not None\n                and pace_idx != np.nan\n                and pace_idx &gt;= 0\n                and pace_idx &lt; forecast_length\n            ):\n                norm_data_3d = norm_data[pace_idx]\n                # \u4fee\u6539\u8fd9\u91cc\uff1a\u786e\u4fddbasin\u5750\u6807\u957f\u5ea6\u4e0e\u6570\u636e\u7ef4\u5ea6\u5339\u914d\n                if basin_num == 1 and len(selected_data.coords[\"basin\"]) &gt; 1:\n                    basin_coord = selected_data.coords[\"basin\"].values[:1]\n                else:\n                    basin_coord = selected_data.coords[\"basin\"].values[:basin_num]\n\n                coords = {\n                    \"basin\": basin_coord,\n                    \"time\": selected_data.coords[\"time\"][:i_e_time_length],\n                    \"variable\": selected_data.coords[\"variable\"],\n                }\n            else:\n                # If pace_idx is not specified, create a new dimension 'horizon'\n                # Reshape (forecast_length, basin_num, i_e_time_length, nf) -&gt; (basin_num, forecast_length * i_e_time_length, nf)\n                norm_data_3d = np.transpose(norm_data, (1, 0, 2, 3)).reshape(\n                    basin_num, forecast_length * i_e_time_length, nf\n                )\n\n                # \u521b\u5efa\u65b0\u7684\u65f6\u95f4\u5750\u6807\n                new_times = []\n                for i in range(forecast_length):\n                    if i &lt; len(selected_data.coords[\"time\"]):\n                        new_times.extend(\n                            selected_data.coords[\"time\"][:i_e_time_length]\n                        )\n\n                # \u786e\u4fdd\u65f6\u95f4\u5750\u6807\u957f\u5ea6\u4e0e\u6570\u636e\u5339\u914d\n                if len(new_times) &gt; forecast_length * i_e_time_length:\n                    new_times = new_times[: forecast_length * i_e_time_length]\n                elif len(new_times) &lt; forecast_length * i_e_time_length:\n                    # \u5982\u679c\u65f6\u95f4\u5750\u6807\u4e0d\u8db3\uff0c\u4f7f\u7528\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u70b9\u586b\u5145\n                    last_time = (\n                        new_times[-1]\n                        if new_times\n                        else selected_data.coords[\"time\"][0]\n                    )\n                    while len(new_times) &lt; forecast_length * i_e_time_length:\n                        new_times.append(last_time)\n\n                # \u4fee\u6539\u8fd9\u91cc\uff1a\u786e\u4fddbasin\u5750\u6807\u957f\u5ea6\u4e0e\u6570\u636e\u7ef4\u5ea6\u5339\u914d\n                if basin_num == 1 and len(selected_data.coords[\"basin\"]) &gt; 1:\n                    basin_coord = selected_data.coords[\"basin\"].values[:1]\n                else:\n                    basin_coord = selected_data.coords[\"basin\"].values[:basin_num]\n\n                coords = {\n                    \"basin\": basin_coord,\n                    \"time\": new_times,\n                    \"variable\": selected_data.coords[\"variable\"],\n                }\n        dims = [\"basin\", \"time\", \"variable\"]\n    else:\n        coords = selected_data.coords\n        dims = selected_data.dims\n        norm_data_3d = norm_data\n\n    # create DataArray and inverse transform\n    denorm_xr_ds = target_scaler.inverse_transform(\n        xr.DataArray(\n            norm_data_3d,\n            dims=dims,\n            coords=coords,\n            attrs={\"units\": units},\n        )\n    )\n    return set_unit_to_var(denorm_xr_ds)\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.BasinSingleFlowDataset","title":"<code> BasinSingleFlowDataset            (BaseDataset)         </code>","text":"<p>one time length output for each grid in a batch</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>class BasinSingleFlowDataset(BaseDataset):\n    \"\"\"one time length output for each grid in a batch\"\"\"\n\n    def __init__(self, cfgs: dict, is_tra_val_te: str):\n        super(BasinSingleFlowDataset, self).__init__(cfgs, is_tra_val_te, **kwargs)\n\n    def __getitem__(self, index):\n        xc, ys = super(BasinSingleFlowDataset, self).__getitem__(index)\n        y = ys[-1, :]\n        return xc, y\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.DplDataset","title":"<code> DplDataset            (BaseDataset)         </code>","text":"<p>pytorch dataset for Differential parameter learning</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>class DplDataset(BaseDataset):\n    \"\"\"pytorch dataset for Differential parameter learning\"\"\"\n\n    def __init__(self, cfgs: dict, is_tra_val_te: str):\n        \"\"\"\n        Parameters\n        ----------\n        cfgs\n            all configs\n        is_tra_val_te\n            train, vaild or test\n        \"\"\"\n        super(DplDataset, self).__init__(cfgs, is_tra_val_te)\n        # we don't use y_un_norm as its name because in the main function we will use \"y\"\n        # For physical hydrological models, we need warmup, hence the target values should exclude data in warmup period\n        self.warmup_length = self.training_cfgs[\"warmup_length\"]\n        self.target_as_input = self.data_cfgs[\"target_as_input\"]\n        self.constant_only = self.data_cfgs[\"constant_only\"]\n        if self.target_as_input and (not self.train_mode):\n            # if the target is used as input and train_mode is False,\n            # we need to get the target data in training period to generate pbm params\n            self.train_dataset = DplDataset(cfgs, is_tra_val_te=\"train\")\n\n    def __getitem__(self, item):\n        \"\"\"\n        Get one mini-batch for dPL (differential parameter learning) model\n\n        TODO: not check target_as_input and constant_only cases yet\n\n        Parameters\n        ----------\n        item\n            index\n\n        Returns\n        -------\n        tuple\n            a mini-batch data;\n            x_train (not normalized forcing), z_train (normalized data for DL model), y_train (not normalized output)\n        \"\"\"\n        warmup = self.warmup_length\n        rho = self.rho\n        horizon = self.horizon\n        xc_norm, _ = super(DplDataset, self).__getitem__(item)\n        basin, time, _ = self.lookup_table[item]\n        if self.target_as_input:\n            # y_morn and xc_norm are concatenated and used for DL model\n            y_norm = torch.from_numpy(\n                self.y[basin, time - warmup : time + rho + horizon, :]\n            ).float()\n            # the order of xc_norm and y_norm matters, please be careful!\n            z_train = torch.cat((xc_norm, y_norm), -1)\n        elif self.constant_only:\n            # only use attributes data for DL model\n            z_train = torch.from_numpy(self.c[basin, :]).float()\n        else:\n            z_train = xc_norm.float()\n        x_train = self.x_origin[basin, time - warmup : time + rho + horizon, :]\n        y_train = self.y_origin[basin, time : time + rho + horizon, :]\n        return (\n            torch.from_numpy(x_train).float(),\n            z_train,\n        ), torch.from_numpy(y_train).float()\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.DplDataset.__getitem__","title":"<code>__getitem__(self, item)</code>  <code>special</code>","text":"<p>Get one mini-batch for dPL (differential parameter learning) model</p> <p>TODO: not check target_as_input and constant_only cases yet</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.DplDataset.__getitem__--parameters","title":"Parameters","text":"<p>item     index</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.DplDataset.__getitem__--returns","title":"Returns","text":"<p>tuple     a mini-batch data;     x_train (not normalized forcing), z_train (normalized data for DL model), y_train (not normalized output)</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>def __getitem__(self, item):\n    \"\"\"\n    Get one mini-batch for dPL (differential parameter learning) model\n\n    TODO: not check target_as_input and constant_only cases yet\n\n    Parameters\n    ----------\n    item\n        index\n\n    Returns\n    -------\n    tuple\n        a mini-batch data;\n        x_train (not normalized forcing), z_train (normalized data for DL model), y_train (not normalized output)\n    \"\"\"\n    warmup = self.warmup_length\n    rho = self.rho\n    horizon = self.horizon\n    xc_norm, _ = super(DplDataset, self).__getitem__(item)\n    basin, time, _ = self.lookup_table[item]\n    if self.target_as_input:\n        # y_morn and xc_norm are concatenated and used for DL model\n        y_norm = torch.from_numpy(\n            self.y[basin, time - warmup : time + rho + horizon, :]\n        ).float()\n        # the order of xc_norm and y_norm matters, please be careful!\n        z_train = torch.cat((xc_norm, y_norm), -1)\n    elif self.constant_only:\n        # only use attributes data for DL model\n        z_train = torch.from_numpy(self.c[basin, :]).float()\n    else:\n        z_train = xc_norm.float()\n    x_train = self.x_origin[basin, time - warmup : time + rho + horizon, :]\n    y_train = self.y_origin[basin, time : time + rho + horizon, :]\n    return (\n        torch.from_numpy(x_train).float(),\n        z_train,\n    ), torch.from_numpy(y_train).float()\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.DplDataset.__init__","title":"<code>__init__(self, cfgs, is_tra_val_te)</code>  <code>special</code>","text":""},{"location":"api/datasets/#torchhydro.datasets.data_sets.DplDataset.__init__--parameters","title":"Parameters","text":"<p>cfgs     all configs is_tra_val_te     train, vaild or test</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>def __init__(self, cfgs: dict, is_tra_val_te: str):\n    \"\"\"\n    Parameters\n    ----------\n    cfgs\n        all configs\n    is_tra_val_te\n        train, vaild or test\n    \"\"\"\n    super(DplDataset, self).__init__(cfgs, is_tra_val_te)\n    # we don't use y_un_norm as its name because in the main function we will use \"y\"\n    # For physical hydrological models, we need warmup, hence the target values should exclude data in warmup period\n    self.warmup_length = self.training_cfgs[\"warmup_length\"]\n    self.target_as_input = self.data_cfgs[\"target_as_input\"]\n    self.constant_only = self.data_cfgs[\"constant_only\"]\n    if self.target_as_input and (not self.train_mode):\n        # if the target is used as input and train_mode is False,\n        # we need to get the target data in training period to generate pbm params\n        self.train_dataset = DplDataset(cfgs, is_tra_val_te=\"train\")\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.FlexibleDataset","title":"<code> FlexibleDataset            (BaseDataset)         </code>","text":"<p>A dataset whose datasources are from multiple sources according to the configuration</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>class FlexibleDataset(BaseDataset):\n    \"\"\"A dataset whose datasources are from multiple sources according to the configuration\"\"\"\n\n    def __init__(self, cfgs: dict, is_tra_val_te: str):\n        super(FlexibleDataset, self).__init__(cfgs, is_tra_val_te)\n\n    @property\n    def data_source(self):\n        source_cfgs = self.data_cfgs[\"source_cfgs\"]\n        return {\n            name: data_sources_dict[name](path)\n            for name, path in zip(\n                source_cfgs[\"source_names\"], source_cfgs[\"source_paths\"]\n            )\n        }\n\n    def _read_xyc(self):\n        var_to_source_map = self.data_cfgs[\"var_to_source_map\"]\n        x_datasets, y_datasets, c_datasets = [], [], []\n        gage_ids = self.t_s_dict[\"sites_id\"]\n        t_range = self.t_s_dict[\"t_final_range\"]\n\n        for var_name in var_to_source_map:\n            source_name = var_to_source_map[var_name]\n            data_source_ = self.data_source[source_name]\n            if var_name in self.data_cfgs[\"relevant_cols\"]:\n                x_datasets.append(\n                    data_source_.read_ts_xrdataset(gage_ids, t_range, [var_name])\n                )\n            elif var_name in self.data_cfgs[\"target_cols\"]:\n                y_datasets.append(\n                    data_source_.read_ts_xrdataset(gage_ids, t_range, [var_name])\n                )\n            elif var_name in self.data_cfgs[\"constant_cols\"]:\n                c_datasets.append(\n                    data_source_.read_attr_xrdataset(gage_ids, [var_name])\n                )\n\n        # \u5408\u5e76\u6240\u6709x, y, c\u7c7b\u578b\u7684\u6570\u636e\u96c6\n        x = xr.merge(x_datasets) if x_datasets else xr.Dataset()\n        y = xr.merge(y_datasets) if y_datasets else xr.Dataset()\n        c = xr.merge(c_datasets) if c_datasets else xr.Dataset()\n        # Check if any flow variable exists in y dataset instead of hardcoding \"streamflow\"\n        flow_var_name = (\n            self.streamflow_name\n            if hasattr(self, \"streamflow_name\") and self.streamflow_name in y\n            else None\n        )\n        if flow_var_name is None:\n            # fallback: check if any target variable is in y\n            for target_var in self.data_cfgs[\"target_cols\"]:\n                if target_var in y:\n                    flow_var_name = target_var\n                    break\n        if flow_var_name and flow_var_name in y:\n            area = data_source_.camels.read_area(self.t_s_dict[\"sites_id\"])\n            y.update(streamflow_unit_conv(y[[flow_var_name]], area))\n        x_origin, y_origin, c_origin = self._to_dataarray_with_unit(x, y, c)\n        return x_origin, y_origin, c_origin\n\n    def _normalize(self):\n        var_to_source_map = self.data_cfgs[\"var_to_source_map\"]\n        for var_name in var_to_source_map:\n            source_name = var_to_source_map[var_name]\n            data_source_ = self.data_source[source_name]\n            break\n        # TODO: only support CAMELS for now\n        scaler_hub = ScalerHub(\n            self.y_origin,\n            self.x_origin,\n            self.c_origin,\n            data_cfgs=self.data_cfgs,\n            is_tra_val_te=self.is_tra_val_te,\n            data_source=data_source_.camels,\n        )\n        self.target_scaler = scaler_hub.target_scaler\n        return scaler_hub.x, scaler_hub.y, scaler_hub.c\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.FloodEventDataset","title":"<code> FloodEventDataset            (BaseDataset)         </code>","text":"<p>Dataset class for flood event detection and prediction tasks.</p> <p>This dataset is specifically designed to handle flood event data where flood_event column contains binary indicators (0 for normal, non-zero for flood). It automatically creates a flood_mask from the flood_event data for special loss computation purposes.</p> <p>The dataset reads data using SelfMadeHydroDataset from hydrodatasource, expecting CSV files with columns like: time, rain, inflow, flood_event.</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>class FloodEventDataset(BaseDataset):\n    \"\"\"Dataset class for flood event detection and prediction tasks.\n\n    This dataset is specifically designed to handle flood event data where\n    flood_event column contains binary indicators (0 for normal, non-zero for flood).\n    It automatically creates a flood_mask from the flood_event data for special\n    loss computation purposes.\n\n    The dataset reads data using SelfMadeHydroDataset from hydrodatasource,\n    expecting CSV files with columns like: time, rain, inflow, flood_event.\n    \"\"\"\n\n    def __init__(self, cfgs: dict, is_tra_val_te: str):\n        \"\"\"Initialize FloodEventDataset\n\n        Parameters\n        ----------\n        cfgs : dict\n            Configuration dictionary containing data_cfgs, training_cfgs, evaluation_cfgs\n        is_tra_val_te : str\n            One of 'train', 'valid', or 'test'\n        \"\"\"\n        # Find flood_event column index for later processing\n        target_cols = cfgs[\"data_cfgs\"][\"target_cols\"]\n        self.flood_event_idx = None\n        for i, col in enumerate(target_cols):\n            if \"flood_event\" in col.lower():\n                self.flood_event_idx = i\n                break\n\n        if self.flood_event_idx is None:\n            raise ValueError(\n                \"flood_event column not found in target_cols. Please ensure flood_event is included in the target columns.\"\n            )\n        super(FloodEventDataset, self).__init__(cfgs, is_tra_val_te)\n\n    @property\n    def noutputvar(self):\n        \"\"\"How many output variables in the dataset\n        Used in evaluation.\n        For flood datasets, the number of output variables is 2.\n        But we don't need flood_mask in evaluation.\n\n        Returns\n        -------\n        int\n            number of variables\n        \"\"\"\n        return len(self.data_cfgs[\"target_cols\"]) - 1\n\n    def _create_flood_mask(self, y):\n        \"\"\"Create flood mask from flood_event column\n\n        Parameters\n        ----------\n        y : np.ndarray\n            Target data with shape [seq_len, n_targets] containing flood_event column\n\n        Returns\n        -------\n        np.ndarray\n            Flood mask with shape [seq_len, 1] where 1 indicates flood event, 0 indicates normal\n        \"\"\"\n        if self.flood_event_idx &gt;= y.shape[1]:\n            raise ValueError(\n                f\"flood_event_idx {self.flood_event_idx} exceeds target dimensions {y.shape[1]}\"\n            )\n\n        # Extract flood_event column\n        flood_events = y[:, self.flood_event_idx]\n\n        # Create binary mask: 1 for flood (non-zero), 0 for normal (zero)\n        no_flood_data = min(flood_events)\n        flood_mask = (flood_events != no_flood_data).astype(np.float32)\n\n        # Reshape to maintain dimension consistency\n        flood_mask = flood_mask.reshape(-1, 1)\n\n        return flood_mask\n\n    def _create_lookup_table(self):\n        \"\"\"Create lookup table based on flood events with sliding window\n\n        This method creates samples where:\n        1. For each flood event sequence:\n           - In training: use sliding window to generate samples with fixed length\n           - In testing: use the entire flood event sequence as one sample with its actual length\n        2. Each sample covers the full sequence length without internal structure division\n        \"\"\"\n        lookup = []\n\n        # Calculate total sample sequence length for training/validation\n        sample_seqlen = self.warmup_length + self.rho + self.horizon\n\n        for basin_idx in range(self.ngrid):\n            # Get flood events for this basin\n            flood_events = self.y_origin[basin_idx, :, self.flood_event_idx]\n\n            # Find flood event sequences (consecutive non-zero values)\n            flood_sequences = self._find_flood_sequences(flood_events)\n\n            for seq_start, seq_end in flood_sequences:\n                if self.is_new_batch_way:\n                    # For test period, use the entire flood event sequence as one sample\n                    # But we need to ensure the sample includes enough context (sample_seqlen)\n                    flood_length = seq_end - seq_start + 1\n\n                    # Calculate the start index to include enough context before the flood\n                    # We want to include some data before the flood event starts\n                    context_before = min(sample_seqlen - flood_length, seq_start)\n                    context_before = max(context_before, 0)\n                    # The actual start index should be early enough to provide context\n                    actual_start = seq_start - context_before\n\n                    # The total length should be at least sample_seqlen or the actual flood sequence length\n                    total_length = max(sample_seqlen, flood_length + context_before)\n\n                    # Ensure we don't exceed the data bounds\n                    if actual_start + total_length &gt; self.nt:\n                        total_length = self.nt - actual_start\n\n                    lookup.append((basin_idx, actual_start, total_length))\n                else:\n                    # For training, use sliding window approach\n                    self._create_sliding_window_samples(\n                        basin_idx, seq_start, seq_end, sample_seqlen, lookup\n                    )\n\n        self.lookup_table = dict(enumerate(lookup))\n        self.num_samples = len(self.lookup_table)\n\n    def _find_flood_sequences(self, flood_events):\n        \"\"\"Find sequences of consecutive flood events\n\n        Parameters\n        ----------\n        flood_events : np.ndarray\n            1D array of flood event indicators\n\n        Returns\n        -------\n        list\n            List of tuples (start_idx, end_idx) for each flood sequence\n        \"\"\"\n        sequences = []\n        in_sequence = False\n        start_idx = None\n\n        for i, event in enumerate(flood_events):\n            if event &gt; 0 and not in_sequence:\n                # Start of a new flood sequence\n                in_sequence = True\n                start_idx = i\n            elif event == 0 and in_sequence:\n                # End of current flood sequence\n                in_sequence = False\n                sequences.append((start_idx, i - 1))\n            elif i == len(flood_events) - 1 and in_sequence:\n                # End of data while in sequence\n                sequences.append((start_idx, i))\n\n        return sequences\n\n    def _create_sliding_window_samples(\n        self, basin_idx, seq_start, seq_end, sample_seqlen, lookup\n    ):\n        \"\"\"Create samples for a flood sequence using sliding window approach with data validity check\n\n        Parameters\n        ----------\n        basin_idx : int\n            Index of the basin\n        seq_start : int\n            Start index of flood sequence\n        seq_end : int\n            End index of flood sequence\n        sample_seqlen : int\n            Maximum length of each sample (warmup_length + rho + horizon)\n        lookup : list\n            List to append new samples to (basin_idx, actual_start, actual_length)\n        \"\"\"\n        # Generate sliding window samples for this flood sequence\n        # Each window should include at least some flood event data\n\n        # Calculate the range where we can place the sliding window\n        # The window end should not exceed the flood sequence end\n        max_window_start = min(\n            seq_end - sample_seqlen + 1, self.nt - sample_seqlen\n        )  # Window end should not exceed seq_end or data bounds\n        min_window_start = max(\n            0, seq_start - sample_seqlen + 1\n        )  # Window must include at least the first flood event\n\n        # Ensure we have a valid range\n        if max_window_start &lt; min_window_start:\n            return  # Skip this flood sequence if no valid window can be created\n\n        # Generate samples with sliding window\n        for window_start in range(min_window_start, max_window_start + 1):\n            window_end = window_start + sample_seqlen - 1\n\n            # Check if the window is valid (doesn't exceed data bounds and flood sequence)\n            if window_end &lt; self.nt and window_end &lt;= seq_end:\n                # Check if this window includes at least some flood events\n                window_includes_flood = (window_start &lt;= seq_end) and (\n                    window_end &gt;= seq_start\n                )\n\n                if window_includes_flood:\n                    # Find the actual valid data range within this window closest to flood\n                    actual_start, actual_length = self._find_valid_data_range(\n                        basin_idx, window_start, window_end, seq_start, seq_end\n                    )\n\n                    # Only add sample if we have sufficient valid data\n                    if (\n                        actual_length &gt;= self.rho + self.horizon\n                    ):  # At least need rho + horizon\n                        lookup.append((basin_idx, actual_start, actual_length))\n\n    def _find_valid_data_range(\n        self, basin_idx, window_start, window_end, flood_start, flood_end\n    ):\n        \"\"\"Find the continuous valid data range closest to the flood sequence\n\n        Parameters\n        ----------\n        basin_idx : int\n            Basin index\n        window_start : int\n            Start of the window to check\n        window_end : int\n            End of the window to check\n        flood_start : int\n            Start index of the flood sequence\n        flood_end : int\n            End index of the flood sequence\n\n        Returns\n        -------\n        tuple\n            (actual_start, actual_length) of the valid data range closest to flood sequence\n        \"\"\"\n        # Get data for this basin and window\n        x_window = self.x[basin_idx, window_start : window_end + 1, :]\n\n        # Check for NaN values in both input and output\n        valid_mask = ~np.isnan(x_window).any(axis=1)  # Valid if no NaN in any feature\n\n        # Find the continuous valid sequence closest to the flood sequence\n        closest_start, closest_length = self._find_closest_valid_sequence(\n            valid_mask, window_start, flood_start, flood_end\n        )\n\n        if closest_length &lt;= 0:\n            return window_start, 0\n        return closest_start, closest_length\n\n    def _find_closest_valid_sequence(\n        self, valid_mask, window_start, flood_start, flood_end\n    ):\n        \"\"\"Find the continuous valid sequence closest to the flood sequence\n\n        Parameters\n        ----------\n        valid_mask : np.ndarray\n            Boolean array indicating valid positions within the window\n        window_start : int\n            Start index of the window in the original time series\n        flood_start : int\n            Start index of the flood sequence in the original time series\n        flood_end : int\n            End index of the flood sequence in the original time series\n\n        Returns\n        -------\n        tuple\n            (closest_start, closest_length) in original time series coordinates\n        \"\"\"\n        if not valid_mask.any():\n            return window_start, 0\n\n        # Find all continuous valid sequences within the window\n        sequences = []\n        current_start = None\n\n        for i, is_valid in enumerate(valid_mask):\n            if is_valid and current_start is None:\n                current_start = i\n            elif not is_valid and current_start is not None:\n                sequences.append((current_start, i - current_start))\n                current_start = None\n\n        # Handle case where sequence continues to the end\n        if current_start is not None:\n            sequences.append((current_start, len(valid_mask) - current_start))\n\n        if not sequences:\n            return window_start, 0\n\n        # If only one sequence, return it directly\n        if len(sequences) == 1:\n            seq_start_rel, seq_length = sequences[0]\n            seq_start_abs = window_start + seq_start_rel\n            return seq_start_abs, seq_length\n\n        # Find the sequence closest to the flood sequence\n        flood_center = (flood_start + flood_end) / 2\n        closest_sequence = None\n        min_distance = float(\"inf\")\n\n        for seq_start_rel, seq_length in sequences:\n            seq_start_abs = window_start + seq_start_rel\n            seq_end_abs = seq_start_abs + seq_length - 1\n            seq_center = (seq_start_abs + seq_end_abs) / 2\n\n            # Calculate distance from sequence center to flood center\n            distance = abs(seq_center - flood_center)\n\n            if distance &lt; min_distance:\n                min_distance = distance\n                closest_sequence = (seq_start_abs, seq_length)\n\n        return closest_sequence or (window_start, 0)\n\n    def __getitem__(self, item: int):\n        \"\"\"Get one sample from the dataset with flood mask\n\n        Returns samples with:\n        1. Variable length sequences (no padding)\n        2. Flood mask for weighted loss computation\n        \"\"\"\n        basin, start_idx, actual_length = self.lookup_table[item]\n        warmup_length = self.warmup_length\n        end_idx = start_idx + actual_length\n\n        # Get input and target data for the actual valid range\n        x = self.x[basin, start_idx:end_idx, :]\n        y = self.y[basin, start_idx + warmup_length : end_idx, :]\n\n        # Create flood mask from flood_event column\n        flood_mask = self._create_flood_mask(y)\n\n        # Replace the original flood_event column with the new flood_mask\n        y_with_flood_mask = y.copy()\n        y_with_flood_mask[:, self.flood_event_idx] = flood_mask.squeeze()\n\n        # Handle constant features if available\n        if self.c is None or self.c.shape[-1] == 0:\n            return (\n                torch.from_numpy(x).float(),\n                torch.from_numpy(y_with_flood_mask).float(),\n            )\n\n        # Add constant features to input\n        c = self.c[basin, :]\n        c = np.repeat(c, x.shape[0], axis=0).reshape(c.shape[0], -1).T\n        xc = np.concatenate((x, c), axis=1)\n\n        return torch.from_numpy(xc).float(), torch.from_numpy(y_with_flood_mask).float()\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.FloodEventDataset.noutputvar","title":"<code>noutputvar</code>  <code>property</code> <code>readonly</code>","text":"<p>How many output variables in the dataset Used in evaluation. For flood datasets, the number of output variables is 2. But we don't need flood_mask in evaluation.</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.FloodEventDataset.noutputvar--returns","title":"Returns","text":"<p>int     number of variables</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.FloodEventDataset.__getitem__","title":"<code>__getitem__(self, item)</code>  <code>special</code>","text":"<p>Get one sample from the dataset with flood mask</p> <p>Returns samples with: 1. Variable length sequences (no padding) 2. Flood mask for weighted loss computation</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>def __getitem__(self, item: int):\n    \"\"\"Get one sample from the dataset with flood mask\n\n    Returns samples with:\n    1. Variable length sequences (no padding)\n    2. Flood mask for weighted loss computation\n    \"\"\"\n    basin, start_idx, actual_length = self.lookup_table[item]\n    warmup_length = self.warmup_length\n    end_idx = start_idx + actual_length\n\n    # Get input and target data for the actual valid range\n    x = self.x[basin, start_idx:end_idx, :]\n    y = self.y[basin, start_idx + warmup_length : end_idx, :]\n\n    # Create flood mask from flood_event column\n    flood_mask = self._create_flood_mask(y)\n\n    # Replace the original flood_event column with the new flood_mask\n    y_with_flood_mask = y.copy()\n    y_with_flood_mask[:, self.flood_event_idx] = flood_mask.squeeze()\n\n    # Handle constant features if available\n    if self.c is None or self.c.shape[-1] == 0:\n        return (\n            torch.from_numpy(x).float(),\n            torch.from_numpy(y_with_flood_mask).float(),\n        )\n\n    # Add constant features to input\n    c = self.c[basin, :]\n    c = np.repeat(c, x.shape[0], axis=0).reshape(c.shape[0], -1).T\n    xc = np.concatenate((x, c), axis=1)\n\n    return torch.from_numpy(xc).float(), torch.from_numpy(y_with_flood_mask).float()\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.FloodEventDataset.__init__","title":"<code>__init__(self, cfgs, is_tra_val_te)</code>  <code>special</code>","text":"<p>Initialize FloodEventDataset</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.FloodEventDataset.__init__--parameters","title":"Parameters","text":"<p>cfgs : dict     Configuration dictionary containing data_cfgs, training_cfgs, evaluation_cfgs is_tra_val_te : str     One of 'train', 'valid', or 'test'</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>def __init__(self, cfgs: dict, is_tra_val_te: str):\n    \"\"\"Initialize FloodEventDataset\n\n    Parameters\n    ----------\n    cfgs : dict\n        Configuration dictionary containing data_cfgs, training_cfgs, evaluation_cfgs\n    is_tra_val_te : str\n        One of 'train', 'valid', or 'test'\n    \"\"\"\n    # Find flood_event column index for later processing\n    target_cols = cfgs[\"data_cfgs\"][\"target_cols\"]\n    self.flood_event_idx = None\n    for i, col in enumerate(target_cols):\n        if \"flood_event\" in col.lower():\n            self.flood_event_idx = i\n            break\n\n    if self.flood_event_idx is None:\n        raise ValueError(\n            \"flood_event column not found in target_cols. Please ensure flood_event is included in the target columns.\"\n        )\n    super(FloodEventDataset, self).__init__(cfgs, is_tra_val_te)\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.FloodEventDplDataset","title":"<code> FloodEventDplDataset            (FloodEventDataset)         </code>","text":"<p>Dataset class for flood event detection and prediction with differential parameter learning support.</p> <p>This dataset combines FloodEventDataset's flood event handling capabilities with DplDataset's data format for differential parameter learning (dPL) models. It handles flood event sequences and returns data in the format required for physical hydrological models with neural network components.</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>class FloodEventDplDataset(FloodEventDataset):\n    \"\"\"Dataset class for flood event detection and prediction with differential parameter learning support.\n\n    This dataset combines FloodEventDataset's flood event handling capabilities with\n    DplDataset's data format for differential parameter learning (dPL) models.\n    It handles flood event sequences and returns data in the format required for\n    physical hydrological models with neural network components.\n    \"\"\"\n\n    def __init__(self, cfgs: dict, is_tra_val_te: str):\n        \"\"\"Initialize FloodEventDplDataset\n\n        Parameters\n        ----------\n        cfgs : dict\n            Configuration dictionary containing data_cfgs, training_cfgs, evaluation_cfgs\n        is_tra_val_te : str\n            One of 'train', 'valid', or 'test'\n        \"\"\"\n        super(FloodEventDplDataset, self).__init__(cfgs, is_tra_val_te)\n\n        # Additional attributes for DPL functionality\n        self.target_as_input = self.data_cfgs[\"target_as_input\"]\n        self.constant_only = self.data_cfgs[\"constant_only\"]\n\n        if self.target_as_input and (not self.train_mode):\n            # if the target is used as input and train_mode is False,\n            # we need to get the target data in training period to generate pbm params\n            self.train_dataset = FloodEventDplDataset(cfgs, is_tra_val_te=\"train\")\n\n    def __getitem__(self, item: int):\n        \"\"\"Get one sample from the dataset in DPL format with flood mask\n\n        Returns data in the format required for differential parameter learning:\n        - x_train: not normalized forcing data\n        - z_train: normalized data for DL model (with flood mask)\n        - y_train: not normalized output data\n\n        Parameters\n        ----------\n        item : int\n            Index of the sample\n\n        Returns\n        -------\n        tuple\n            ((x_train, z_train), y_train) where:\n            - x_train: torch.Tensor, not normalized forcing data\n            - z_train: torch.Tensor, normalized data for DL model\n            - y_train: torch.Tensor, not normalized output data with flood mask\n        \"\"\"\n        basin, start_idx, actual_length = self.lookup_table[item]\n        end_idx = start_idx + actual_length\n        warmup_length = self.warmup_length\n        # Get normalized data first (using parent's logic for flood mask)\n        xc_norm, y_norm_with_mask = super(FloodEventDplDataset, self).__getitem__(item)\n\n        # Get original (not normalized) data\n        x_origin = self.x_origin[basin, start_idx:end_idx, :]\n        y_origin = self.y_origin[basin, start_idx + warmup_length : end_idx, :]\n\n        # Create flood mask for original y data\n        flood_mask_origin = self._create_flood_mask(y_origin)\n        y_origin_with_mask = y_origin.copy()\n        y_origin_with_mask[:, self.flood_event_idx] = flood_mask_origin.squeeze()\n\n        # Prepare z_train based on configuration\n        if self.target_as_input:\n            # y_norm and xc_norm are concatenated and used for DL model\n            # the order of xc_norm and y_norm matters, please be careful!\n            z_train = torch.cat((xc_norm, y_norm_with_mask), -1)\n        elif self.constant_only:\n            # only use attributes data for DL model\n            if self.c is None or self.c.shape[-1] == 0:\n                # If no constant features, use a zero tensor\n                z_train = torch.zeros((actual_length, 1)).float()\n            else:\n                c = self.c[basin, :]\n                # Repeat constants for the actual sequence length\n                c_repeated = (\n                    np.repeat(c, actual_length, axis=0).reshape(c.shape[0], -1).T\n                )\n                z_train = torch.from_numpy(c_repeated).float()\n        else:\n            # Use normalized input features with constants\n            z_train = xc_norm.float()\n\n        # Prepare x_train (original forcing data with constants if available)\n        if self.c is None or self.c.shape[-1] == 0:\n            x_train = torch.from_numpy(x_origin).float()\n        else:\n            c = self.c_origin[basin, :]\n            c_repeated = np.repeat(c, actual_length, axis=0).reshape(c.shape[0], -1).T\n            x_origin_with_c = np.concatenate((x_origin, c_repeated), axis=1)\n            x_train = torch.from_numpy(x_origin_with_c).float()\n\n        # y_train is the original output data with flood mask\n        y_train = torch.from_numpy(y_origin_with_mask).float()\n\n        return (x_train, z_train), y_train\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.FloodEventDplDataset.__getitem__","title":"<code>__getitem__(self, item)</code>  <code>special</code>","text":"<p>Get one sample from the dataset in DPL format with flood mask</p> <p>Returns data in the format required for differential parameter learning: - x_train: not normalized forcing data - z_train: normalized data for DL model (with flood mask) - y_train: not normalized output data</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.FloodEventDplDataset.__getitem__--parameters","title":"Parameters","text":"<p>item : int     Index of the sample</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.FloodEventDplDataset.__getitem__--returns","title":"Returns","text":"<p>tuple     ((x_train, z_train), y_train) where:     - x_train: torch.Tensor, not normalized forcing data     - z_train: torch.Tensor, normalized data for DL model     - y_train: torch.Tensor, not normalized output data with flood mask</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>def __getitem__(self, item: int):\n    \"\"\"Get one sample from the dataset in DPL format with flood mask\n\n    Returns data in the format required for differential parameter learning:\n    - x_train: not normalized forcing data\n    - z_train: normalized data for DL model (with flood mask)\n    - y_train: not normalized output data\n\n    Parameters\n    ----------\n    item : int\n        Index of the sample\n\n    Returns\n    -------\n    tuple\n        ((x_train, z_train), y_train) where:\n        - x_train: torch.Tensor, not normalized forcing data\n        - z_train: torch.Tensor, normalized data for DL model\n        - y_train: torch.Tensor, not normalized output data with flood mask\n    \"\"\"\n    basin, start_idx, actual_length = self.lookup_table[item]\n    end_idx = start_idx + actual_length\n    warmup_length = self.warmup_length\n    # Get normalized data first (using parent's logic for flood mask)\n    xc_norm, y_norm_with_mask = super(FloodEventDplDataset, self).__getitem__(item)\n\n    # Get original (not normalized) data\n    x_origin = self.x_origin[basin, start_idx:end_idx, :]\n    y_origin = self.y_origin[basin, start_idx + warmup_length : end_idx, :]\n\n    # Create flood mask for original y data\n    flood_mask_origin = self._create_flood_mask(y_origin)\n    y_origin_with_mask = y_origin.copy()\n    y_origin_with_mask[:, self.flood_event_idx] = flood_mask_origin.squeeze()\n\n    # Prepare z_train based on configuration\n    if self.target_as_input:\n        # y_norm and xc_norm are concatenated and used for DL model\n        # the order of xc_norm and y_norm matters, please be careful!\n        z_train = torch.cat((xc_norm, y_norm_with_mask), -1)\n    elif self.constant_only:\n        # only use attributes data for DL model\n        if self.c is None or self.c.shape[-1] == 0:\n            # If no constant features, use a zero tensor\n            z_train = torch.zeros((actual_length, 1)).float()\n        else:\n            c = self.c[basin, :]\n            # Repeat constants for the actual sequence length\n            c_repeated = (\n                np.repeat(c, actual_length, axis=0).reshape(c.shape[0], -1).T\n            )\n            z_train = torch.from_numpy(c_repeated).float()\n    else:\n        # Use normalized input features with constants\n        z_train = xc_norm.float()\n\n    # Prepare x_train (original forcing data with constants if available)\n    if self.c is None or self.c.shape[-1] == 0:\n        x_train = torch.from_numpy(x_origin).float()\n    else:\n        c = self.c_origin[basin, :]\n        c_repeated = np.repeat(c, actual_length, axis=0).reshape(c.shape[0], -1).T\n        x_origin_with_c = np.concatenate((x_origin, c_repeated), axis=1)\n        x_train = torch.from_numpy(x_origin_with_c).float()\n\n    # y_train is the original output data with flood mask\n    y_train = torch.from_numpy(y_origin_with_mask).float()\n\n    return (x_train, z_train), y_train\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.FloodEventDplDataset.__init__","title":"<code>__init__(self, cfgs, is_tra_val_te)</code>  <code>special</code>","text":"<p>Initialize FloodEventDplDataset</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.FloodEventDplDataset.__init__--parameters","title":"Parameters","text":"<p>cfgs : dict     Configuration dictionary containing data_cfgs, training_cfgs, evaluation_cfgs is_tra_val_te : str     One of 'train', 'valid', or 'test'</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>def __init__(self, cfgs: dict, is_tra_val_te: str):\n    \"\"\"Initialize FloodEventDplDataset\n\n    Parameters\n    ----------\n    cfgs : dict\n        Configuration dictionary containing data_cfgs, training_cfgs, evaluation_cfgs\n    is_tra_val_te : str\n        One of 'train', 'valid', or 'test'\n    \"\"\"\n    super(FloodEventDplDataset, self).__init__(cfgs, is_tra_val_te)\n\n    # Additional attributes for DPL functionality\n    self.target_as_input = self.data_cfgs[\"target_as_input\"]\n    self.constant_only = self.data_cfgs[\"constant_only\"]\n\n    if self.target_as_input and (not self.train_mode):\n        # if the target is used as input and train_mode is False,\n        # we need to get the target data in training period to generate pbm params\n        self.train_dataset = FloodEventDplDataset(cfgs, is_tra_val_te=\"train\")\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.GNNDataset","title":"<code> GNNDataset            (FloodEventDataset)         </code>","text":"<p>Optimized GNN Dataset for hydrological Graph Neural Network tasks.</p> <p>This dataset extends FloodEventDataset to support Graph Neural Networks by: 1. Integrating station data via StationHydroDataset 2. Processing adjacency matrices with flexible edge weight and attribute handling 3. Merging basin-level features (xc) with station-level features (sxc) per node 4. Returning GNN-ready format: (sxc, y, edge_index, edge_attr)</p> <p>Key Features: - Leverages BaseDataset's universal normalization and NaN handling for station data - Supports flexible edge weight selection (specify column or default to binary) - Always constructs edge_index and edge_attr for each basin - Merges basin and station features to create comprehensive node representations</p> <p>Configuration keys in data_cfgs.gnn_cfgs: - station_cols: List of station variable names to load - station_rm_nan: Whether to remove/interpolate NaN values (default: True) - station_scaler_type: Scaler type for station data normalization - use_adjacency: Whether to load adjacency matrices (default: True) - adjacency_src_col: Source node column name (default: \"ID\") - adjacency_dst_col: Destination node column name (default: \"NEXTDOWNID\") - adjacency_edge_attr_cols: Columns for edge attributes (default: [\"dist_hdn\", \"elev_diff\", \"strm_slope\"]) - adjacency_weight_col: Column to use as edge weights (default: None for binary weights) - return_edge_weight: Whether to return edge_weight instead of edge_attr (default: False)</p> <p>edge_attr : torch.Tensor     Edge attributes [num_edges, edge_attr_dim]</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>class GNNDataset(FloodEventDataset):\n    \"\"\"Optimized GNN Dataset for hydrological Graph Neural Network tasks.\n\n    This dataset extends FloodEventDataset to support Graph Neural Networks by:\n    1. Integrating station data via StationHydroDataset\n    2. Processing adjacency matrices with flexible edge weight and attribute handling\n    3. Merging basin-level features (xc) with station-level features (sxc) per node\n    4. Returning GNN-ready format: (sxc, y, edge_index, edge_attr)\n\n    Key Features:\n    - Leverages BaseDataset's universal normalization and NaN handling for station data\n    - Supports flexible edge weight selection (specify column or default to binary)\n    - Always constructs edge_index and edge_attr for each basin\n    - Merges basin and station features to create comprehensive node representations\n\n    Configuration keys in data_cfgs.gnn_cfgs:\n    - station_cols: List of station variable names to load\n    - station_rm_nan: Whether to remove/interpolate NaN values (default: True)\n    - station_scaler_type: Scaler type for station data normalization\n    - use_adjacency: Whether to load adjacency matrices (default: True)\n    - adjacency_src_col: Source node column name (default: \"ID\")\n    - adjacency_dst_col: Destination node column name (default: \"NEXTDOWNID\")\n    - adjacency_edge_attr_cols: Columns for edge attributes (default: [\"dist_hdn\", \"elev_diff\", \"strm_slope\"])\n    - adjacency_weight_col: Column to use as edge weights (default: None for binary weights)\n    - return_edge_weight: Whether to return edge_weight instead of edge_attr (default: False)\n\n    edge_attr : torch.Tensor\n        Edge attributes [num_edges, edge_attr_dim]\n    \"\"\"\n\n    def __init__(self, cfgs: dict, is_tra_val_te: str):\n        # Extract and extend configuration for station data\n        self._extend_data_cfgs_for_stations(cfgs)\n\n        # Store GNN-specific settings\n        self.gnn_cfgs = cfgs[\"data_cfgs\"].get(\"station_cfgs\", {})\n\n        # Initialize parent (this will call BaseDataset._load_data() automatically)\n        super(GNNDataset, self).__init__(cfgs, is_tra_val_te)\n\n        # Load adjacency data after main data processing\n        self.adjacency_data = self._load_adjacency_data()\n\n    def _extend_data_cfgs_for_stations(self, cfgs):\n        \"\"\"Extend data configuration to include station data as a standard data type\n\n        This allows BaseDataset to handle station data using its universal processing pipeline.\n        \"\"\"\n        data_cfgs = cfgs[\"data_cfgs\"]\n        gnn_cfgs = data_cfgs.get(\"station_cfgs\", {})\n\n        # Add station_cols to data configuration if specified\uff08\u8fd9\u4e2a\u4e0d\u89c1\u5f97\u975e\u5f97\u6709gnn_cfgs,\u6b63\u5e38\u5e94\u8be5\u662fdata_cfgs\u91cc\u9762\u7ee7\u7eed\u6269\u5145\u7684\uff09\n        if gnn_cfgs.get(\"station_cols\"):\n            data_cfgs[\"station_cols\"] = gnn_cfgs[\"station_cols\"]\n            # Add station data processing settings to leverage BaseDataset pipeline\n            data_cfgs[\"station_rm_nan\"] = gnn_cfgs.get(\"station_rm_nan\", True)\n\n    def _read_xyc(self):\n        \"\"\"Read X, Y, C data including station data using unified approach\n\n        This is the ONLY method we need to override from BaseDataset.\n        All other processing (normalization, NaN handling, array conversion)\n        is handled automatically by BaseDataset's pipeline.\n        \"\"\"\n        # Read standard basin data using parent's logic\n        data_dict = super(GNNDataset, self)._read_xyc()\n\n        # Add station data if configured\n        if self.data_cfgs.get(\"station_cols\"):\n            station_data = self._read_all_station_data()\n            data_dict[\"station_cols\"] = station_data\n\n        return data_dict\n\n    def _read_all_station_data(self):\n        \"\"\"Read station data for all basins using StationHydroDataset\n\n        Creates xr.DataArray with the same structure as other data types\n        so that BaseDataset can process it using the universal pipeline.\n        \"\"\"\n        if not hasattr(self.data_source, \"get_stations_by_basin\"):\n            LOGGER.warning(\n                \"Data source does not support station data, skipping station data reading\"\n            )\n            return None\n\n        # Convert basin IDs from \"songliao_21100150\" to \"21100150\" for StationHydroDataset\n        basin_ids_with_prefix = self.t_s_dict[\"sites_id\"]\n        basin_ids = self._convert_basin_to_station_ids(basin_ids_with_prefix)\n        t_range = self.t_s_dict[\"t_final_range\"]\n\n        # Collect station data for all basins\n        all_station_data = []\n\n        for basin_id in basin_ids:\n            basin_station_data = self._read_basin_station_data(basin_id, t_range)\n            all_station_data.append(basin_station_data)\n\n        # Combine into unified xr.DataArray structure\n        if all_station_data and any(data is not None for data in all_station_data):\n            combined_station_data = self._combine_station_data_arrays(\n                all_station_data, basin_ids\n            )\n            return combined_station_data\n        else:\n            return None\n\n    def _read_basin_station_data(self, basin_id, t_range):\n        \"\"\"Read station data for a single basin, supporting multi-period case\"\"\"\n        try:\n            # Get stations for this basin\n            station_ids = self.data_source.get_stations_by_basin(basin_id)\n\n            if not station_ids:\n                return None\n\n            # Handle multi-period case\n            if isinstance(t_range[0], (list, tuple)):\n                # Validate that each period has exactly 2 elements (start and end date)\n                for i, period in enumerate(t_range):\n                    if not isinstance(period, (list, tuple)) or len(period) != 2:\n                        raise ValueError(\n                            f\"Period {i} must be a list/tuple with exactly 2 elements (start_date, end_date), got: {period}\"\n                        )\n\n                # Multi-period case - read and concatenate data\n                all_station_data = None\n\n                for start_date, end_date in t_range:\n                    period_station_data = self.data_source.read_station_ts_xrdataset(\n                        station_id_lst=station_ids,\n                        t_range=[start_date, end_date],\n                        var_lst=self.data_cfgs[\"station_cols\"],\n                        time_units=self.gnn_cfgs.get(\"station_time_units\", [\"1D\"]),\n                    )\n\n                    if all_station_data is None:\n                        all_station_data = period_station_data\n                    else:\n                        all_station_data = xr.concat(\n                            [all_station_data, period_station_data], dim=\"time\"\n                        )\n\n                station_data = all_station_data\n            else:\n                # Single period case (existing behavior)\n                station_data = self.data_source.read_station_ts_xrdataset(\n                    station_id_lst=station_ids,\n                    t_range=t_range,\n                    var_lst=self.data_cfgs[\"station_cols\"],\n                    time_units=self.gnn_cfgs.get(\"station_time_units\", [\"1D\"]),\n                )\n\n            return self._process_station_xr_data(station_data)\n\n        except Exception as e:\n            LOGGER.warning(f\"Could not read station data for basin {basin_id}: {e}\")\n            return None\n\n    def _process_station_xr_data(self, station_data):\n        \"\"\"Process xarray station data into standard format\"\"\"\n        if not station_data:\n            return None\n\n        # Handle multiple time units\n        if isinstance(station_data, dict):\n            # Use first available time unit\n            time_unit = list(station_data.keys())[0]\n            station_ds = station_data[time_unit]\n        else:\n            station_ds = station_data\n\n        if not station_ds or not station_ds.sizes:\n            return None\n\n        # Convert to DataArray with standard format\n        if isinstance(station_ds, xr.Dataset):\n            station_da = station_ds.to_array(dim=\"variable\")\n            # Transpose to [time, station, variable]\n            station_da = station_da.transpose(\"time\", \"station\", \"variable\")\n        else:\n            station_da = station_ds\n\n        return station_da\n\n    def _combine_station_data_arrays(self, station_data_list, basin_ids):\n        \"\"\"Combine station data from all basins into a unified structure\n\n        Creates an xr.DataArray with dimensions [basin, time, station, variable]\n        similar to how other data types are structured in BaseDataset.\n        \"\"\"\n        # Find common time dimension and data structure\n        valid_data = [data for data in station_data_list if data is not None]\n        if not valid_data:\n            return None\n\n        # Use time dimension from first valid dataset\n        common_time = valid_data[0].coords[\"time\"]\n\n        # Find maximum number of stations and variables across all basins\n        max_stations = max(data.sizes.get(\"station\", 0) for data in valid_data)\n        max_variables = max(data.sizes.get(\"variable\", 0) for data in valid_data)\n\n        # Create unified data array\n        n_basins = len(basin_ids)\n        n_time = len(common_time)\n\n        # Initialize with NaN (BaseDataset will handle NaN processing)\n        unified_data = np.full(\n            (n_basins, n_time, max_stations, max_variables), np.nan, dtype=np.float32\n        )\n\n        # Fill with actual data\n        for i, (basin_id, station_data) in enumerate(zip(basin_ids, station_data_list)):\n            if station_data is not None:\n                # Align time dimension\n                try:\n                    aligned_data = station_data.reindex(\n                        time=common_time, method=\"nearest\"\n                    )\n                    data_array = aligned_data.values\n\n                    # Insert into unified array\n                    n_stations_basin = data_array.shape[1]\n                    n_vars_basin = data_array.shape[2]\n                    unified_data[i, :, :n_stations_basin, :n_vars_basin] = data_array\n                except Exception as e:\n                    LOGGER.warning(\n                        f\"Failed to align station data for basin {basin_id}: {e}\"\n                    )\n                    continue\n\n        # Create xr.DataArray with proper coordinates\n        station_coords = [f\"station_{j}\" for j in range(max_stations)]\n        variable_coords = self.data_cfgs[\"station_cols\"][:max_variables]\n\n        station_da = xr.DataArray(\n            unified_data,\n            dims=[\"basin\", \"time\", \"station\", \"variable\"],\n            coords={\n                \"basin\": basin_ids,\n                \"time\": common_time,\n                \"station\": station_coords,\n                \"variable\": variable_coords,\n            },\n        )\n\n        return station_da\n\n    def _load_adjacency_data(self):\n        \"\"\"Load and process adjacency data from .nc files\n\n        Returns\n        -------\n        dict\n            Dictionary containing edge_index, edge_attr for each basin\n        \"\"\"\n        if not self.gnn_cfgs.get(\"use_adjacency\", True):\n            return None\n\n        if not hasattr(self.data_source, \"read_adjacency_xrdataset\"):\n            LOGGER.warning(\"Data source does not support adjacency data\")\n            return None\n\n        adjacency_data = {}\n        # basin_ids = self.t_s_dict[\"sites_id\"]\n        # Convert basin IDs from \"songliao_21100150\" to \"21100150\" for StationHydroDataset\n        basin_ids_with_prefix = self.t_s_dict[\"sites_id\"]\n        basin_ids = self._convert_basin_to_station_ids(basin_ids_with_prefix)\n\n        for basin_id in basin_ids:\n            try:\n                # Read adjacency data from .nc file\n                adj_df = self.data_source.read_adjacency_xrdataset(basin_id)\n\n                if adj_df is None:\n                    LOGGER.warning(\n                        f\"No adjacency data for basin {basin_id}, using self-loops\"\n                    )\n                    adjacency_data[basin_id] = self._create_self_loop_adjacency(\n                        basin_id\n                    )\n                else:\n                    # Let _process_adjacency_dataframe handle the format checking and processing\n                    adjacency_data[basin_id] = self._process_adjacency_dataframe(\n                        adj_df, basin_id\n                    )\n\n            except Exception as e:\n                LOGGER.warning(\n                    f\"Failed to load adjacency data for basin {basin_id}: {e}\"\n                )\n                adjacency_data[basin_id] = self._create_self_loop_adjacency(basin_id)\n\n        return adjacency_data\n\n    def _process_adjacency_dataframe(self, adj_df, basin_id):\n        \"\"\"Process adjacency DataFrame into edge_index and edge_attr tensors\n\n        Standard GNN processing: extract edges and their attributes from DataFrame or xarray Dataset.\n\n        Parameters\n        ----------\n        adj_df : pd.DataFrame or xr.Dataset\n            Adjacency DataFrame/Dataset with columns like ID, NEXTDOWNID, dist_hdn, elev_diff, strm_slope\n        basin_id : str\n            Basin identifier\n\n        Returns\n        -------\n        dict\n            Dictionary containing edge_index, edge_attr, edge_weight, num_nodes\n        \"\"\"\n        import torch\n        import pandas as pd\n        import xarray as xr\n        import numpy as np\n\n        # Convert xarray Dataset to pandas DataFrame if needed\n        if isinstance(adj_df, xr.Dataset):\n            try:\n                # Convert xarray Dataset to pandas DataFrame\n                adj_df = adj_df.to_dataframe().reset_index()\n                # LOGGER.info(f\"Basin {basin_id}: Converted xarray Dataset to DataFrame with shape {adj_df.shape}\")\n                # LOGGER.info(f\"Basin {basin_id}: DataFrame columns = {list(adj_df.columns)}\")\n            except Exception as e:\n                LOGGER.error(\n                    f\"Basin {basin_id}: Failed to convert xarray Dataset to DataFrame: {e}\"\n                )\n                return self._create_self_loop_adjacency(basin_id)\n\n        # Configuration (simplified)\n        src_col = self.gnn_cfgs.get(\"adjacency_src_col\", \"ID\")\n        dst_col = self.gnn_cfgs.get(\"adjacency_dst_col\", \"NEXTDOWNID\")\n        edge_attr_cols = self.gnn_cfgs.get(\n            \"adjacency_edge_attr_cols\", [\"dist_hdn\", \"elev_diff\", \"strm_slope\"]\n        )\n        weight_col = self.gnn_cfgs.get(\"adjacency_weight_col\", None)  # \u65b0\u589e\uff1a\u6307\u5b9a\u6743\u91cd\u5217\n        # Check if required columns exist\n        if src_col not in adj_df.columns:\n            LOGGER.warning(\n                f\"Basin {basin_id}: Source column '{src_col}' not found in adjacency data. Available columns: {list(adj_df.columns)}\"\n            )\n            return self._create_self_loop_adjacency(basin_id)\n\n        if dst_col not in adj_df.columns:\n            LOGGER.warning(\n                f\"Basin {basin_id}: Destination column '{dst_col}' not found in adjacency data. Available columns: {list(adj_df.columns)}\"\n            )\n            return self._create_self_loop_adjacency(basin_id)\n\n        # Clean and convert numeric columns to proper dtypes in batch\n        # Handle string \"nan\" values that may come from NetCDF files\n        numeric_cols = [\n            col\n            for col in edge_attr_cols + ([weight_col] if weight_col else [])\n            if col in adj_df.columns\n        ]\n        if numeric_cols:\n            # Batch replace string \"nan\" with actual NaN and convert to numeric\n            adj_df[numeric_cols] = adj_df[numeric_cols].replace(\n                [\"nan\", \"NaN\", \"NAN\"], np.nan\n            )\n            adj_df[numeric_cols] = adj_df[numeric_cols].apply(\n                pd.to_numeric, errors=\"coerce\"\n            )\n            LOGGER.debug(\n                f\"Basin {basin_id}: Converted {len(numeric_cols)} numeric columns in batch\"\n            )\n\n        # Create comprehensive node mapping including all stations in the basin\n        # First get all nodes that appear in adjacency matrix (connected nodes)\n        connected_nodes = set(adj_df[src_col].dropna()) | set(adj_df[dst_col].dropna())\n\n        # Then get all stations in this basin (including isolated nodes)\n        try:\n            if hasattr(self.data_source, \"get_stations_by_basin\"):\n                all_basin_stations = self.data_source.get_stations_by_basin(basin_id)\n                if all_basin_stations:\n                    # Convert station IDs to strings to match adjacency data format\n                    all_basin_nodes = set(\n                        str(station_id) for station_id in all_basin_stations\n                    )\n                    # Combine connected nodes with all basin nodes\n                    all_nodes = connected_nodes | all_basin_nodes\n                    isolated_nodes = all_basin_nodes - connected_nodes\n                    if isolated_nodes:\n                        LOGGER.info(\n                            f\"Basin {basin_id}: Found {len(isolated_nodes)} isolated nodes: {isolated_nodes}\"\n                        )\n                else:\n                    all_nodes = connected_nodes\n            else:\n                # Fallback to only connected nodes if station data unavailable\n                all_nodes = connected_nodes\n        except Exception as e:\n            LOGGER.warning(\n                f\"Basin {basin_id}: Failed to get all basin stations: {e}, using connected nodes only\"\n            )\n            all_nodes = connected_nodes\n\n        if len(all_nodes) == 0:\n            LOGGER.warning(f\"Basin {basin_id}: No valid nodes found\")\n            return self._create_self_loop_adjacency(basin_id)\n\n        node_to_idx = {node: idx for idx, node in enumerate(sorted(all_nodes))}\n        LOGGER.info(\n            f\"Basin {basin_id}: Found {len(all_nodes)} total nodes ({len(connected_nodes)} connected, {len(all_nodes) - len(connected_nodes)} isolated)\"\n        )\n\n        # Extract edges and attributes using vectorized operations\n        # First process edges from adjacency matrix\n        valid_rows = adj_df.dropna(subset=[src_col, dst_col])\n        edges_from_adj = []\n        edge_attrs_from_adj = []\n        edge_weights_from_adj = []\n\n        if len(valid_rows) &gt; 0:\n            # Vectorized edge creation from adjacency matrix\n            src_nodes = valid_rows[src_col].map(node_to_idx).values\n            dst_nodes = valid_rows[dst_col].map(node_to_idx).values\n            edges_from_adj = np.column_stack([src_nodes, dst_nodes])\n\n            # Vectorized edge attributes extraction\n            edge_attrs_list = []\n            for col in edge_attr_cols:\n                if col in valid_rows.columns:\n                    attrs = valid_rows[col].fillna(0.0).values\n                else:\n                    attrs = np.zeros(len(valid_rows))\n                edge_attrs_list.append(attrs)\n            edge_attrs_from_adj = (\n                np.column_stack(edge_attrs_list)\n                if edge_attrs_list\n                else np.zeros((len(valid_rows), len(edge_attr_cols)))\n            )\n\n            # Vectorized edge weights extraction\n            if weight_col and weight_col in valid_rows.columns:\n                edge_weights_from_adj = valid_rows[weight_col].fillna(1.0).values\n            else:\n                edge_weights_from_adj = np.ones(len(valid_rows))\n\n        # Add self-loops for isolated nodes (nodes not in adjacency matrix)\n        isolated_nodes = all_nodes - connected_nodes\n        edges_from_isolated = []\n        edge_attrs_from_isolated = []\n        edge_weights_from_isolated = []\n\n        if isolated_nodes:\n            # Create self-loops for isolated nodes\n            isolated_indices = [node_to_idx[node] for node in isolated_nodes]\n            edges_from_isolated = np.column_stack([isolated_indices, isolated_indices])\n            edge_attrs_from_isolated = np.zeros(\n                (len(isolated_nodes), len(edge_attr_cols))\n            )\n            edge_weights_from_isolated = np.ones(len(isolated_nodes))\n\n        # Combine edges from adjacency matrix and self-loops for isolated nodes\n        if len(edges_from_adj) &gt; 0 and len(edges_from_isolated) &gt; 0:\n            all_edges = np.vstack([edges_from_adj, edges_from_isolated])\n            all_edge_attrs = np.vstack([edge_attrs_from_adj, edge_attrs_from_isolated])\n            all_edge_weights = np.concatenate(\n                [edge_weights_from_adj, edge_weights_from_isolated]\n            )\n        elif len(edges_from_adj) &gt; 0:\n            all_edges = edges_from_adj\n            all_edge_attrs = edge_attrs_from_adj\n            all_edge_weights = edge_weights_from_adj\n        elif len(edges_from_isolated) &gt; 0:\n            all_edges = edges_from_isolated\n            all_edge_attrs = edge_attrs_from_isolated\n            all_edge_weights = edge_weights_from_isolated\n        else:\n            # Fallback: create self-loops for all nodes\n            # LOGGER.warning(f\"Basin {basin_id}: No edges found, creating self-loops for all nodes\")\n            n_nodes = len(all_nodes)\n            node_indices = list(range(n_nodes))\n            all_edges = np.column_stack([node_indices, node_indices])\n            all_edge_attrs = np.zeros((n_nodes, len(edge_attr_cols)))\n            all_edge_weights = np.ones(n_nodes)\n\n        # Convert to tensors\n        edge_index = torch.tensor(all_edges.T, dtype=torch.long).contiguous()\n        edge_attr = (\n            torch.tensor(all_edge_attrs, dtype=torch.float)\n            if all_edge_attrs is not None\n            else None\n        )\n        edge_weight = torch.tensor(all_edge_weights, dtype=torch.float)\n\n        return {\n            \"edge_index\": edge_index,\n            \"edge_attr\": edge_attr,\n            \"edge_weight\": edge_weight,  # \u65b0\u589e\uff1a\u5355\u72ec\u7684\u8fb9\u6743\u91cd\u5f20\u91cf\n            \"num_nodes\": len(all_nodes),\n            \"node_to_idx\": node_to_idx,\n            \"weight_col\": weight_col,  # \u8bb0\u5f55\u4f7f\u7528\u7684\u6743\u91cd\u5217\n        }\n\n    def _create_self_loop_adjacency(self, basin_id):\n        \"\"\"Create self-loop adjacency as fallback\"\"\"\n        import torch\n\n        try:\n            # Try to get station count for this basin\n            if hasattr(self.data_source, \"get_stations_by_basin\"):\n                station_ids = self.data_source.get_stations_by_basin(basin_id)\n                n_nodes = len(station_ids) if station_ids else 1\n            else:\n                n_nodes = 1\n        except Exception:\n            n_nodes = 1\n\n        # Create self-loops: edge_index = [[0,1,2,...], [0,1,2,...]]\n        edge_index = torch.arange(n_nodes).repeat(2, 1)\n\n        # Create default edge attributes\n        edge_attr_cols = self.gnn_cfgs.get(\n            \"adjacency_edge_attr_cols\", [\"dist_hdn\", \"elev_diff\", \"strm_slope\"]\n        )\n        if edge_attr_cols:\n            edge_attr = torch.zeros((n_nodes, len(edge_attr_cols)), dtype=torch.float)\n        else:\n            edge_attr = None\n\n        # Create default edge weights (1.0 for self-loops)\n        edge_weight = torch.ones(n_nodes, dtype=torch.float)\n\n        return {\n            \"edge_index\": edge_index,\n            \"edge_attr\": edge_attr,\n            \"edge_weight\": edge_weight,  # \u65b0\u589e\uff1a\u8fb9\u6743\u91cd\n            \"num_nodes\": n_nodes,\n            \"node_to_idx\": {i: i for i in range(n_nodes)},\n            \"weight_col\": None,  # \u81ea\u73af\u60c5\u51b5\u4e0b\u6ca1\u6709\u6307\u5b9a\u6743\u91cd\u5217\n        }\n\n    # GNN-specific utility methods\n    def get_station_data(self, basin_idx):\n        \"\"\"Get station data for a specific basin\n\n        Since station data is now processed by BaseDataset pipeline,\n        it's available as self.station_cols (converted to numpy array).\n        \"\"\"\n        if hasattr(self, \"station_cols\") and self.station_cols is not None:\n            return self.station_cols[basin_idx]\n        return None\n\n    def get_adjacency_data(self, basin_idx):\n        \"\"\"Get adjacency data for a specific basin\n\n        Returns\n        -------\n        dict or None\n            Dictionary containing edge_index, edge_attr, edge_weight, etc. or None\n        \"\"\"\n        if self.adjacency_data is None:\n            return None\n\n        # Get the specific basin ID for this basin index\n        basin_id_with_prefix = self.t_s_dict[\"sites_id\"][basin_idx]\n        # Convert single basin ID to station ID (without prefix)\n        basin_id = self._convert_basin_to_station_ids([basin_id_with_prefix])[0]\n        return self.adjacency_data.get(basin_id)\n\n    def get_edge_weight(self, basin_idx):\n        \"\"\"Get edge weights for a specific basin\n\n        Parameters\n        ----------\n        basin_idx : int\n            Basin index\n\n        Returns\n        -------\n        torch.Tensor or None\n            Edge weights tensor [num_edges] or None\n        \"\"\"\n        adjacency_data = self.get_adjacency_data(basin_idx)\n        if adjacency_data is not None:\n            return adjacency_data.get(\"edge_weight\")\n        return None\n\n    def _convert_basin_to_station_ids(self, basin_ids):\n        \"\"\"Convert basin IDs (with prefix) to station IDs (without prefix) for StationHydroDataset\n\n        Parameters\n        ----------\n        basin_ids : list\n            List of basin IDs with prefix (e.g., [\"songliao_21100150\"])\n\n        Returns\n        -------\n        list\n            List of station IDs without prefix (e.g., [\"21100150\"])\n        \"\"\"\n        station_ids = []\n        for basin_id in basin_ids:\n            # Remove common prefixes\n            if \"_\" in basin_id:\n                # Extract the part after the last underscore\n                station_id = basin_id.split(\"_\")[-1]\n            else:\n                # If no underscore, use the original ID\n                station_id = basin_id\n            station_ids.append(station_id)\n        return station_ids\n\n    def _convert_station_to_basin_ids(self, station_ids, prefix=\"songliao\"):\n        \"\"\"Convert station IDs (without prefix) to basin IDs (with prefix) for consistency\n\n        Parameters\n        ----------\n        station_ids : list\n            List of station IDs without prefix (e.g., [\"21100150\"])\n        prefix : str\n            Prefix to add (default: \"songliao\")\n\n        Returns\n        -------\n        list\n            List of basin IDs with prefix (e.g., [\"songliao_21100150\"])\n        \"\"\"\n        basin_ids = []\n        for station_id in station_ids:\n            basin_id = f\"{prefix}_{station_id}\"\n            basin_ids.append(basin_id)\n        return basin_ids\n\n    def __getitem__(self, item: int):\n        \"\"\"Get one sample with GNN-specific data format.\n\n        This method merges basin-level features (xc) into each station node's\n        features (sxc), so each node's input includes both station and basin\n        attributes.\n\n        Args:\n            item: The index of the sample to retrieve.\n\n        Returns:\n            A tuple of (sxc, y, edge_index, edge_weight) where:\n            - sxc: Station features merged with basin features.\n                   Shape: [num_stations, seq_length, feature_dim]\n            - y: Target values for prediction.\n                 Shape: [forecast_length, output_dim]\n            - edge_index: Edge connectivity.\n                          Shape: [2, num_edges]\n            - edge_weight: Edge weights.\n                           Shape: [num_edges]\n        \"\"\"\n        import torch\n        import numpy as np\n\n        # Get basic sample from parent (includes flood mask if FloodEventDataset)\n        basic_sample = super(GNNDataset, self).__getitem__(item)\n\n        # Extract x, y from parent's output\n        if isinstance(basic_sample, tuple):\n            x, y = (\n                basic_sample  # x: [seq_length, x_feature_dim], y_full: [full_length, y_feature_dim]\n            )\n        elif isinstance(basic_sample, dict):\n            x = basic_sample.get(\"x\")\n            y = basic_sample.get(\"y\")\n        else:\n            raise ValueError(f\"Unexpected basic_sample format: {type(basic_sample)}\")\n\n        # Get sample metadata\n        basin, time_idx, actual_length = self.lookup_table[item]\n\n        # For GNN prediction, we only need the forecast part of y as target\n        # The structure should be: warmup + hindcast (rho) + forecast (horizon)\n        # We only predict the forecast (horizon) part\n\n        # Get station data for current basin and time window\n        station_data = self.get_station_data(basin)  # [time, station, variable]\n        adjacency_data = self.get_adjacency_data(basin)\n\n        # Extract station data for the time window (input sequence)\n        if station_data is not None:\n            # For station data, we need the input sequence (not just forecast part)\n            seq_end = time_idx + actual_length\n            sxc_raw = station_data[\n                time_idx:seq_end\n            ]  # [seq_length, num_stations, station_feature_dim]\n        else:\n            # If no station data, create dummy station data\n            LOGGER.warning(\n                f\"No station data for basin {basin}, using single dummy station\"\n            )\n            dummy_station_features = 1  # Number of dummy features\n            sxc_raw = np.zeros(\n                (actual_length, 1, dummy_station_features)\n            )  # [seq_length, 1, 1]\n\n        # Get basin-level features (xc) for merging\n        # x contains basin-level features, we need to replicate it for each station\n        if x is not None and x.ndim &gt;= 2:\n            xc = x  # [seq_length, basin_feature_dim]\n            basin_feature_dim = xc.shape[-1]\n            seq_length, num_stations, station_feature_dim = sxc_raw.shape\n\n            # Replicate basin features for each station and concatenate with station features\n            # xc expanded: [seq_length, 1, basin_feature_dim] -&gt; [seq_length, num_stations, basin_feature_dim]\n            xc_expanded = np.tile(xc[:, np.newaxis, :], (1, num_stations, 1))\n\n            # Concatenate station features with basin features\n            # sxc_temp: [seq_length, num_stations, station_feature_dim + basin_feature_dim]\n            sxc_temp = np.concatenate([sxc_raw, xc_expanded], axis=-1)\n\n            # Transpose to get desired shape: [num_stations, seq_length, feature_dim]\n            sxc = sxc_temp.transpose(1, 0, 2)\n        else:\n            # If no basin features, use only station features and transpose\n            # sxc: [num_stations, seq_length, station_feature_dim]\n            sxc = sxc_raw.transpose(1, 0, 2)\n\n        # Process adjacency data (GNN edge orientation handled here)\n        # Edge orientation logic: support 'upstream', 'downstream', 'bidirectional' (default: downstream)\n        edge_orientation = self.gnn_cfgs.get(\"edge_orientation\", \"downstream\")\n        if adjacency_data is not None:\n            edge_index = adjacency_data[\"edge_index\"]  # [2, num_edges]\n            edge_attr = adjacency_data[\n                \"edge_attr\"\n            ]  # [num_edges, edge_attr_dim] or None\n            edge_weight = adjacency_data.get(\"edge_weight\")  # [num_edges]\n            # If edge_weight is None, fill with ones (all edges weight=1)\n            if edge_weight is None:\n                num_edges = edge_index.shape[1]\n                edge_weight = torch.ones(num_edges, dtype=torch.float)\n\n            # Edge orientation handling\n            if edge_orientation == \"downstream\":\n                # Reverse all edges: swap source and target\n                edge_index = edge_index[[1, 0], :]\n            elif edge_orientation == \"bidirectional\":\n                # Add reversed edges to make bidirectional\n                edge_index_rev = edge_index[[1, 0], :]\n                edge_index = torch.cat([edge_index, edge_index_rev], dim=1)\n                if edge_attr is not None:\n                    edge_attr = torch.cat([edge_attr, edge_attr], dim=0)\n                if edge_weight is not None:\n                    edge_weight = torch.cat([edge_weight, edge_weight], dim=0)\n            # else: downstream (default), do nothing\n\n        else:\n            # Default: self-loops for each station\n            num_stations = sxc.shape[\n                0\n            ]  # Now sxc is [num_stations, seq_length, feature_dim]\n            edge_index = torch.arange(num_stations).repeat(2, 1)\n            edge_attr = None\n            edge_weight = torch.ones(num_stations, dtype=torch.float)  # \u9ed8\u8ba4\u6743\u91cd\u4e3a1\n\n        # Ensure edge_attr has proper shape\n        if edge_attr is None:\n            num_edges = edge_index.shape[1]\n            edge_attr_dim = len(\n                self.gnn_cfgs.get(\n                    \"adjacency_edge_attr_cols\", [\"dist_hdn\", \"elev_diff\", \"strm_slope\"]\n                )\n            )\n            edge_attr = torch.zeros((num_edges, edge_attr_dim), dtype=torch.float)\n\n        # Convert to tensors if needed\n        if not isinstance(sxc, torch.Tensor):\n            sxc = torch.tensor(sxc, dtype=torch.float)\n        if not isinstance(y, torch.Tensor):\n            y = torch.tensor(y, dtype=torch.float)\n\n        return sxc, y, edge_index, edge_weight  # edge_attr\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.GNNDataset.__getitem__","title":"<code>__getitem__(self, item)</code>  <code>special</code>","text":"<p>Get one sample with GNN-specific data format.</p> <p>This method merges basin-level features (xc) into each station node's features (sxc), so each node's input includes both station and basin attributes.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>int</code> <p>The index of the sample to retrieve.</p> required <p>Returns:</p> Type Description <code>A tuple of (sxc, y, edge_index, edge_weight) where</code> <ul> <li>sxc: Station features merged with basin features.        Shape: [num_stations, seq_length, feature_dim]</li> <li>y: Target values for prediction.      Shape: [forecast_length, output_dim]</li> <li>edge_index: Edge connectivity.               Shape: [2, num_edges]</li> <li>edge_weight: Edge weights.                Shape: [num_edges]</li> </ul> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>def __getitem__(self, item: int):\n    \"\"\"Get one sample with GNN-specific data format.\n\n    This method merges basin-level features (xc) into each station node's\n    features (sxc), so each node's input includes both station and basin\n    attributes.\n\n    Args:\n        item: The index of the sample to retrieve.\n\n    Returns:\n        A tuple of (sxc, y, edge_index, edge_weight) where:\n        - sxc: Station features merged with basin features.\n               Shape: [num_stations, seq_length, feature_dim]\n        - y: Target values for prediction.\n             Shape: [forecast_length, output_dim]\n        - edge_index: Edge connectivity.\n                      Shape: [2, num_edges]\n        - edge_weight: Edge weights.\n                       Shape: [num_edges]\n    \"\"\"\n    import torch\n    import numpy as np\n\n    # Get basic sample from parent (includes flood mask if FloodEventDataset)\n    basic_sample = super(GNNDataset, self).__getitem__(item)\n\n    # Extract x, y from parent's output\n    if isinstance(basic_sample, tuple):\n        x, y = (\n            basic_sample  # x: [seq_length, x_feature_dim], y_full: [full_length, y_feature_dim]\n        )\n    elif isinstance(basic_sample, dict):\n        x = basic_sample.get(\"x\")\n        y = basic_sample.get(\"y\")\n    else:\n        raise ValueError(f\"Unexpected basic_sample format: {type(basic_sample)}\")\n\n    # Get sample metadata\n    basin, time_idx, actual_length = self.lookup_table[item]\n\n    # For GNN prediction, we only need the forecast part of y as target\n    # The structure should be: warmup + hindcast (rho) + forecast (horizon)\n    # We only predict the forecast (horizon) part\n\n    # Get station data for current basin and time window\n    station_data = self.get_station_data(basin)  # [time, station, variable]\n    adjacency_data = self.get_adjacency_data(basin)\n\n    # Extract station data for the time window (input sequence)\n    if station_data is not None:\n        # For station data, we need the input sequence (not just forecast part)\n        seq_end = time_idx + actual_length\n        sxc_raw = station_data[\n            time_idx:seq_end\n        ]  # [seq_length, num_stations, station_feature_dim]\n    else:\n        # If no station data, create dummy station data\n        LOGGER.warning(\n            f\"No station data for basin {basin}, using single dummy station\"\n        )\n        dummy_station_features = 1  # Number of dummy features\n        sxc_raw = np.zeros(\n            (actual_length, 1, dummy_station_features)\n        )  # [seq_length, 1, 1]\n\n    # Get basin-level features (xc) for merging\n    # x contains basin-level features, we need to replicate it for each station\n    if x is not None and x.ndim &gt;= 2:\n        xc = x  # [seq_length, basin_feature_dim]\n        basin_feature_dim = xc.shape[-1]\n        seq_length, num_stations, station_feature_dim = sxc_raw.shape\n\n        # Replicate basin features for each station and concatenate with station features\n        # xc expanded: [seq_length, 1, basin_feature_dim] -&gt; [seq_length, num_stations, basin_feature_dim]\n        xc_expanded = np.tile(xc[:, np.newaxis, :], (1, num_stations, 1))\n\n        # Concatenate station features with basin features\n        # sxc_temp: [seq_length, num_stations, station_feature_dim + basin_feature_dim]\n        sxc_temp = np.concatenate([sxc_raw, xc_expanded], axis=-1)\n\n        # Transpose to get desired shape: [num_stations, seq_length, feature_dim]\n        sxc = sxc_temp.transpose(1, 0, 2)\n    else:\n        # If no basin features, use only station features and transpose\n        # sxc: [num_stations, seq_length, station_feature_dim]\n        sxc = sxc_raw.transpose(1, 0, 2)\n\n    # Process adjacency data (GNN edge orientation handled here)\n    # Edge orientation logic: support 'upstream', 'downstream', 'bidirectional' (default: downstream)\n    edge_orientation = self.gnn_cfgs.get(\"edge_orientation\", \"downstream\")\n    if adjacency_data is not None:\n        edge_index = adjacency_data[\"edge_index\"]  # [2, num_edges]\n        edge_attr = adjacency_data[\n            \"edge_attr\"\n        ]  # [num_edges, edge_attr_dim] or None\n        edge_weight = adjacency_data.get(\"edge_weight\")  # [num_edges]\n        # If edge_weight is None, fill with ones (all edges weight=1)\n        if edge_weight is None:\n            num_edges = edge_index.shape[1]\n            edge_weight = torch.ones(num_edges, dtype=torch.float)\n\n        # Edge orientation handling\n        if edge_orientation == \"downstream\":\n            # Reverse all edges: swap source and target\n            edge_index = edge_index[[1, 0], :]\n        elif edge_orientation == \"bidirectional\":\n            # Add reversed edges to make bidirectional\n            edge_index_rev = edge_index[[1, 0], :]\n            edge_index = torch.cat([edge_index, edge_index_rev], dim=1)\n            if edge_attr is not None:\n                edge_attr = torch.cat([edge_attr, edge_attr], dim=0)\n            if edge_weight is not None:\n                edge_weight = torch.cat([edge_weight, edge_weight], dim=0)\n        # else: downstream (default), do nothing\n\n    else:\n        # Default: self-loops for each station\n        num_stations = sxc.shape[\n            0\n        ]  # Now sxc is [num_stations, seq_length, feature_dim]\n        edge_index = torch.arange(num_stations).repeat(2, 1)\n        edge_attr = None\n        edge_weight = torch.ones(num_stations, dtype=torch.float)  # \u9ed8\u8ba4\u6743\u91cd\u4e3a1\n\n    # Ensure edge_attr has proper shape\n    if edge_attr is None:\n        num_edges = edge_index.shape[1]\n        edge_attr_dim = len(\n            self.gnn_cfgs.get(\n                \"adjacency_edge_attr_cols\", [\"dist_hdn\", \"elev_diff\", \"strm_slope\"]\n            )\n        )\n        edge_attr = torch.zeros((num_edges, edge_attr_dim), dtype=torch.float)\n\n    # Convert to tensors if needed\n    if not isinstance(sxc, torch.Tensor):\n        sxc = torch.tensor(sxc, dtype=torch.float)\n    if not isinstance(y, torch.Tensor):\n        y = torch.tensor(y, dtype=torch.float)\n\n    return sxc, y, edge_index, edge_weight  # edge_attr\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.GNNDataset.get_adjacency_data","title":"<code>get_adjacency_data(self, basin_idx)</code>","text":"<p>Get adjacency data for a specific basin</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.GNNDataset.get_adjacency_data--returns","title":"Returns","text":"<p>dict or None     Dictionary containing edge_index, edge_attr, edge_weight, etc. or None</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>def get_adjacency_data(self, basin_idx):\n    \"\"\"Get adjacency data for a specific basin\n\n    Returns\n    -------\n    dict or None\n        Dictionary containing edge_index, edge_attr, edge_weight, etc. or None\n    \"\"\"\n    if self.adjacency_data is None:\n        return None\n\n    # Get the specific basin ID for this basin index\n    basin_id_with_prefix = self.t_s_dict[\"sites_id\"][basin_idx]\n    # Convert single basin ID to station ID (without prefix)\n    basin_id = self._convert_basin_to_station_ids([basin_id_with_prefix])[0]\n    return self.adjacency_data.get(basin_id)\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.GNNDataset.get_edge_weight","title":"<code>get_edge_weight(self, basin_idx)</code>","text":"<p>Get edge weights for a specific basin</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.GNNDataset.get_edge_weight--parameters","title":"Parameters","text":"<p>basin_idx : int     Basin index</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.GNNDataset.get_edge_weight--returns","title":"Returns","text":"<p>torch.Tensor or None     Edge weights tensor [num_edges] or None</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>def get_edge_weight(self, basin_idx):\n    \"\"\"Get edge weights for a specific basin\n\n    Parameters\n    ----------\n    basin_idx : int\n        Basin index\n\n    Returns\n    -------\n    torch.Tensor or None\n        Edge weights tensor [num_edges] or None\n    \"\"\"\n    adjacency_data = self.get_adjacency_data(basin_idx)\n    if adjacency_data is not None:\n        return adjacency_data.get(\"edge_weight\")\n    return None\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.GNNDataset.get_station_data","title":"<code>get_station_data(self, basin_idx)</code>","text":"<p>Get station data for a specific basin</p> <p>Since station data is now processed by BaseDataset pipeline, it's available as self.station_cols (converted to numpy array).</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>def get_station_data(self, basin_idx):\n    \"\"\"Get station data for a specific basin\n\n    Since station data is now processed by BaseDataset pipeline,\n    it's available as self.station_cols (converted to numpy array).\n    \"\"\"\n    if hasattr(self, \"station_cols\") and self.station_cols is not None:\n        return self.station_cols[basin_idx]\n    return None\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.ObsForeDataset","title":"<code> ObsForeDataset            (BaseDataset)         </code>","text":"<p>\u5904\u7406\u89c2\u6d4b\u548c\u9884\u89c1\u671f\u6570\u636e\u7684\u6df7\u5408\u6570\u636e\u96c6</p> <p>\u8fd9\u4e2a\u7c7b\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u5177\u6709\u53cc\u7ef4\u5ea6\u9884\u89c1\u671f\u6570\u636e\u683c\u5f0f\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d lead_time \u548c time \u90fd\u662f\u72ec\u7acb\u7ef4\u5ea6\u3002 \u9002\u5408\u8868\u793a\u4e0d\u540c\u53d1\u5e03\u65f6\u95f4\u5bf9\u4e0d\u540c\u76ee\u6807\u65e5\u671f\u7684\u9884\u62a5\u3002</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>class ObsForeDataset(BaseDataset):\n    \"\"\"\u5904\u7406\u89c2\u6d4b\u548c\u9884\u89c1\u671f\u6570\u636e\u7684\u6df7\u5408\u6570\u636e\u96c6\n\n    \u8fd9\u4e2a\u7c7b\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u5177\u6709\u53cc\u7ef4\u5ea6\u9884\u89c1\u671f\u6570\u636e\u683c\u5f0f\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d lead_time \u548c time \u90fd\u662f\u72ec\u7acb\u7ef4\u5ea6\u3002\n    \u9002\u5408\u8868\u793a\u4e0d\u540c\u53d1\u5e03\u65f6\u95f4\u5bf9\u4e0d\u540c\u76ee\u6807\u65e5\u671f\u7684\u9884\u62a5\u3002\n    \"\"\"\n\n    def __init__(self, cfgs: dict, is_tra_val_te: str):\n        \"\"\"\u521d\u59cb\u5316\u89c2\u6d4b\u548c\u9884\u89c1\u671f\u6df7\u5408\u6570\u636e\u96c6\n\n        Parameters\n        ----------\n        cfgs : dict\n            all configs\n        is_tra_val_te : str\n            \u6307\u5b9a\u662f\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\u8fd8\u662f\u6d4b\u8bd5\u96c6\n        \"\"\"\n        # \u8c03\u7528\u7236\u7c7b\u521d\u59cb\u5316\u65b9\u6cd5\n        super(ObsForeDataset, self).__init__(cfgs, is_tra_val_te)\n        # for each batch, we fix length of hindcast and forecast length.\n        # data from different lead time with a number representing the lead time,\n        # for example, now is 2020-09-30, our min_time_interval is 1 day, hindcast length is 30 and forecast length is 1,\n        # lead_time = 3 means 2020-09-01 to 2020-09-30, and the forecast data is 2020-10-01 from 2020-09-28\n        # for forecast data, we have two different configurations:\n        # 1st, we can set a same lead time for all forecast time\n        # 2020-09-30now, 30hindcast, 2forecast, 3leadtime means 2020-09-01 to 2020-09-30 obs concatenate with 2020-10-01 forecast data from 2020-09-28 and 2020-10-02 forecast data from 2020-09-29\n        # 2nd, we can set a increasing lead time for each forecast time\n        # 2020-09-30now, 30hindcast, 2forecast, [1, 2]leadtime means 2020-09-01 to 2020-09-30 obs concatenate with 2020-10-01 to 2010-10-02 forecast data from 2020-09-30\n        self.lead_time_type = self.training_cfgs[\"lead_time_type\"]\n        if self.lead_time_type not in [\"fixed\", \"increasing\"]:\n            raise ValueError(\n                \"lead_time_type must be one of 'fixed' or 'increasing', \"\n                f\"but got {self.lead_time_type}\"\n            )\n        self.lead_time_start = self.training_cfgs[\"lead_time_start\"]\n        horizon = self.horizon\n        offset = np.zeros((horizon,), dtype=int)\n        if self.lead_time_type == \"fixed\":\n            offset = offset + self.lead_time_start\n        elif self.lead_time_type == \"increasing\":\n            offset = offset + np.arange(\n                self.lead_time_start, self.lead_time_start + horizon\n            )\n        self.horizon_offset = offset\n        feature_mapping = self.data_cfgs[\"feature_mapping\"]\n        #\n        xf_var_indices = {}\n        for obs_var, fore_var in feature_mapping.items():\n            # \u627e\u5230x\u4e2d\u9700\u8981\u88ab\u66ff\u6362\u7684\u53d8\u91cf\u7d22\u5f15\n            x_var_indice = [\n                i\n                for i, var in enumerate(self.data_cfgs[\"relevant_cols\"])\n                if var == obs_var\n            ][0]\n            # \u627e\u5230f\u4e2d\u5bf9\u5e94\u7684\u53d8\u91cf\u7d22\u5f15\n            f_var_indice = [\n                i\n                for i, var in enumerate(self.data_cfgs[\"forecast_cols\"])\n                if var == fore_var\n            ][0]\n            xf_var_indices[x_var_indice] = f_var_indice\n        self.xf_var_indices = xf_var_indices\n\n    def _read_xyc_specified_time(self, start_date, end_date, **kwargs):\n        \"\"\"read f data from data source with specified time range and add it to the whole dict\"\"\"\n        data_dict = super(ObsForeDataset, self)._read_xyc_specified_time(\n            start_date, end_date\n        )\n        lead_time = kwargs.get(\"lead_time\", None)\n        f_origin = self.data_source.read_ts_xrdataset(\n            self.t_s_dict[\"sites_id\"],\n            [start_date, end_date],\n            self.data_cfgs[\"forecast_cols\"],\n            forecast_mode=True,\n            lead_time=lead_time,\n        )\n        f_origin_ = self._rm_timeunit_key(f_origin)\n        f_data = self._trans2da_and_setunits(f_origin_)\n        data_dict[\"forecast_cols\"] = f_data.transpose(\n            \"basin\", \"time\", \"lead_step\", \"variable\"\n        )\n        return data_dict\n\n    def __getitem__(self, item: int):\n        \"\"\"Get a sample from the dataset\n\n        Parameters\n        ----------\n        item : int\n            index of sample\n\n        Returns\n        -------\n        tuple\n            A pair of (x, y) data, where x contains input features and lead time flags,\n            and y contains target values\n        \"\"\"\n        # train mode\n        basin, idx, _ = self.lookup_table[item]\n        warmup_length = self.warmup_length\n        # for x, we only chose data before horizon, but we may need forecast data for not all variables\n        # hence, to avoid nan values for some variables without forecast in horizon\n        # we still get data from the first time to the end of horizon\n        x = self.x[basin, idx - warmup_length : idx + self.rho + self.horizon, :]\n        # for y, we chose data after warmup_length\n        y = self.y[basin, idx : idx + self.rho + self.horizon, :]\n        # use offset to get forecast data\n        offset = self.horizon_offset\n        if self.lead_time_type == \"fixed\":\n            # Fixed lead_time mode - All forecast steps use the same lead_step\n            f = self.f[\n                basin, idx + self.rho : idx + self.rho + self.horizon, offset[0], :\n            ]\n        else:\n            # Increasing lead_time mode - Each forecast step uses a different lead_step\n            f = self.f[basin, idx + self.rho, offset, :]\n        xf = self._concat_xf(x, f)\n        if self.c is None or self.c.shape[-1] == 0:\n            xfc = xf\n        else:\n            c = self.c[basin, :]\n            c = np.repeat(c, xf.shape[0], axis=0).reshape(c.shape[0], -1).T\n            xfc = np.concatenate((xf, c), axis=1)\n\n        return torch.from_numpy(xfc).float(), torch.from_numpy(y).float()\n\n    def _concat_xf(self, x, f):\n        # Create a copy of x to avoid modifying the original data\n        x_combined = x.copy()\n\n        # Iterate through the variable mapping relationship\n        for x_idx, f_idx in self.xf_var_indices.items():\n            # Replace the variables in the forecast period of x with the forecast variables in f\n            # The forecast period of x starts from the rho position\n            x_combined[self.warmup_length + self.rho :, x_idx] = f[:, f_idx]\n\n        return x_combined\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.ObsForeDataset.__getitem__","title":"<code>__getitem__(self, item)</code>  <code>special</code>","text":"<p>Get a sample from the dataset</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.ObsForeDataset.__getitem__--parameters","title":"Parameters","text":"<p>item : int     index of sample</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.ObsForeDataset.__getitem__--returns","title":"Returns","text":"<p>tuple     A pair of (x, y) data, where x contains input features and lead time flags,     and y contains target values</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>def __getitem__(self, item: int):\n    \"\"\"Get a sample from the dataset\n\n    Parameters\n    ----------\n    item : int\n        index of sample\n\n    Returns\n    -------\n    tuple\n        A pair of (x, y) data, where x contains input features and lead time flags,\n        and y contains target values\n    \"\"\"\n    # train mode\n    basin, idx, _ = self.lookup_table[item]\n    warmup_length = self.warmup_length\n    # for x, we only chose data before horizon, but we may need forecast data for not all variables\n    # hence, to avoid nan values for some variables without forecast in horizon\n    # we still get data from the first time to the end of horizon\n    x = self.x[basin, idx - warmup_length : idx + self.rho + self.horizon, :]\n    # for y, we chose data after warmup_length\n    y = self.y[basin, idx : idx + self.rho + self.horizon, :]\n    # use offset to get forecast data\n    offset = self.horizon_offset\n    if self.lead_time_type == \"fixed\":\n        # Fixed lead_time mode - All forecast steps use the same lead_step\n        f = self.f[\n            basin, idx + self.rho : idx + self.rho + self.horizon, offset[0], :\n        ]\n    else:\n        # Increasing lead_time mode - Each forecast step uses a different lead_step\n        f = self.f[basin, idx + self.rho, offset, :]\n    xf = self._concat_xf(x, f)\n    if self.c is None or self.c.shape[-1] == 0:\n        xfc = xf\n    else:\n        c = self.c[basin, :]\n        c = np.repeat(c, xf.shape[0], axis=0).reshape(c.shape[0], -1).T\n        xfc = np.concatenate((xf, c), axis=1)\n\n    return torch.from_numpy(xfc).float(), torch.from_numpy(y).float()\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.ObsForeDataset.__init__","title":"<code>__init__(self, cfgs, is_tra_val_te)</code>  <code>special</code>","text":"<p>\u521d\u59cb\u5316\u89c2\u6d4b\u548c\u9884\u89c1\u671f\u6df7\u5408\u6570\u636e\u96c6</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.ObsForeDataset.__init__--parameters","title":"Parameters","text":"<p>cfgs : dict     all configs is_tra_val_te : str     \u6307\u5b9a\u662f\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\u8fd8\u662f\u6d4b\u8bd5\u96c6</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>def __init__(self, cfgs: dict, is_tra_val_te: str):\n    \"\"\"\u521d\u59cb\u5316\u89c2\u6d4b\u548c\u9884\u89c1\u671f\u6df7\u5408\u6570\u636e\u96c6\n\n    Parameters\n    ----------\n    cfgs : dict\n        all configs\n    is_tra_val_te : str\n        \u6307\u5b9a\u662f\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\u8fd8\u662f\u6d4b\u8bd5\u96c6\n    \"\"\"\n    # \u8c03\u7528\u7236\u7c7b\u521d\u59cb\u5316\u65b9\u6cd5\n    super(ObsForeDataset, self).__init__(cfgs, is_tra_val_te)\n    # for each batch, we fix length of hindcast and forecast length.\n    # data from different lead time with a number representing the lead time,\n    # for example, now is 2020-09-30, our min_time_interval is 1 day, hindcast length is 30 and forecast length is 1,\n    # lead_time = 3 means 2020-09-01 to 2020-09-30, and the forecast data is 2020-10-01 from 2020-09-28\n    # for forecast data, we have two different configurations:\n    # 1st, we can set a same lead time for all forecast time\n    # 2020-09-30now, 30hindcast, 2forecast, 3leadtime means 2020-09-01 to 2020-09-30 obs concatenate with 2020-10-01 forecast data from 2020-09-28 and 2020-10-02 forecast data from 2020-09-29\n    # 2nd, we can set a increasing lead time for each forecast time\n    # 2020-09-30now, 30hindcast, 2forecast, [1, 2]leadtime means 2020-09-01 to 2020-09-30 obs concatenate with 2020-10-01 to 2010-10-02 forecast data from 2020-09-30\n    self.lead_time_type = self.training_cfgs[\"lead_time_type\"]\n    if self.lead_time_type not in [\"fixed\", \"increasing\"]:\n        raise ValueError(\n            \"lead_time_type must be one of 'fixed' or 'increasing', \"\n            f\"but got {self.lead_time_type}\"\n        )\n    self.lead_time_start = self.training_cfgs[\"lead_time_start\"]\n    horizon = self.horizon\n    offset = np.zeros((horizon,), dtype=int)\n    if self.lead_time_type == \"fixed\":\n        offset = offset + self.lead_time_start\n    elif self.lead_time_type == \"increasing\":\n        offset = offset + np.arange(\n            self.lead_time_start, self.lead_time_start + horizon\n        )\n    self.horizon_offset = offset\n    feature_mapping = self.data_cfgs[\"feature_mapping\"]\n    #\n    xf_var_indices = {}\n    for obs_var, fore_var in feature_mapping.items():\n        # \u627e\u5230x\u4e2d\u9700\u8981\u88ab\u66ff\u6362\u7684\u53d8\u91cf\u7d22\u5f15\n        x_var_indice = [\n            i\n            for i, var in enumerate(self.data_cfgs[\"relevant_cols\"])\n            if var == obs_var\n        ][0]\n        # \u627e\u5230f\u4e2d\u5bf9\u5e94\u7684\u53d8\u91cf\u7d22\u5f15\n        f_var_indice = [\n            i\n            for i, var in enumerate(self.data_cfgs[\"forecast_cols\"])\n            if var == fore_var\n        ][0]\n        xf_var_indices[x_var_indice] = f_var_indice\n    self.xf_var_indices = xf_var_indices\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.detect_date_format","title":"<code>detect_date_format(date_str)</code>","text":"<p>\u68c0\u6d4b\u65e5\u671f\u683c\u5f0f\uff0c\u652f\u6301\u5355\u4e2a\u5b57\u7b26\u4e32\u6216\u5b57\u7b26\u4e32\u5217\u8868</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.detect_date_format--parameters","title":"Parameters","text":"<p>date_str : str or list     \u65e5\u671f\u5b57\u7b26\u4e32\u6216\u65e5\u671f\u5b57\u7b26\u4e32\u5217\u8868</p>"},{"location":"api/datasets/#torchhydro.datasets.data_sets.detect_date_format--returns","title":"Returns","text":"<p>str     \u68c0\u6d4b\u5230\u7684\u65e5\u671f\u683c\u5f0f</p> Source code in <code>torchhydro/datasets/data_sets.py</code> <pre><code>def detect_date_format(date_str):\n    \"\"\"\n    \u68c0\u6d4b\u65e5\u671f\u683c\u5f0f\uff0c\u652f\u6301\u5355\u4e2a\u5b57\u7b26\u4e32\u6216\u5b57\u7b26\u4e32\u5217\u8868\n\n    Parameters\n    ----------\n    date_str : str or list\n        \u65e5\u671f\u5b57\u7b26\u4e32\u6216\u65e5\u671f\u5b57\u7b26\u4e32\u5217\u8868\n\n    Returns\n    -------\n    str\n        \u68c0\u6d4b\u5230\u7684\u65e5\u671f\u683c\u5f0f\n    \"\"\"\n    # \u5982\u679c\u8f93\u5165\u662f\u5217\u8868\uff0c\u4f7f\u7528\u7b2c\u4e00\u4e2a\u5143\u7d20\n    if isinstance(date_str, (list, tuple)):\n        if not date_str:  # \u5982\u679c\u5217\u8868\u4e3a\u7a7a\n            raise ValueError(\"Empty date list\")\n        date_str = date_str[0]  # \u4f7f\u7528\u7b2c\u4e00\u4e2a\u65e5\u671f\u5b57\u7b26\u4e32\n\n    # \u786e\u4fdd\u8f93\u5165\u662f\u5b57\u7b26\u4e32\n    if not isinstance(date_str, str):\n        raise ValueError(\n            f\"Date must be string or list of strings, got {type(date_str)}\"\n        )\n\n    # \u5c1d\u8bd5\u4e0d\u540c\u7684\u65e5\u671f\u683c\u5f0f\n    for date_format in DATE_FORMATS:\n        try:\n            datetime.strptime(date_str, date_format)\n            return date_format\n        except ValueError:\n            continue\n\n    raise ValueError(f\"Unknown date format: {date_str}\")\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_sources","title":"<code>data_sources</code>","text":"<p>Author: Wenyu Ouyang Date: 2024-04-02 14:37:09 LastEditTime: 2025-11-08 09:58:42 LastEditors: Wenyu Ouyang Description: A module for different data sources FilePath:       orchhydro       orchhydro\\datasets\\data_sources.py Copyright (c) 2023-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/datasets/#torchhydro.datasets.data_utils","title":"<code>data_utils</code>","text":"<p>Author: Wenyu Ouyang Date: 2023-09-21 15:37:58 LastEditTime: 2025-07-13 15:46:09 LastEditors: Wenyu Ouyang Description: Some basic funtions for dealing with data FilePath:       orchhydro       orchhydro\\datasets\\data_utils.py Copyright (c) 2023-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.choose_basins_with_area","title":"<code>choose_basins_with_area(gages, usgs_ids, smallest_area, largest_area)</code>","text":"<p>choose basins with not too large or too small area</p>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.choose_basins_with_area--parameters","title":"Parameters","text":"<p>gages     Camels, CamelsSeries, Gages or GagesPro object !!! usgs_ids \"list\"     given sites' ids smallest_area     lower limit; unit is km2 largest_area     upper limit; unit is km2</p>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.choose_basins_with_area--returns","title":"Returns","text":"<p>list     sites_chosen: [] -- ids of chosen gages</p> Source code in <code>torchhydro/datasets/data_utils.py</code> <pre><code>def choose_basins_with_area(\n    gages,\n    usgs_ids: list,\n    smallest_area: float,\n    largest_area: float,\n) -&gt; list:\n    \"\"\"\n    choose basins with not too large or too small area\n\n    Parameters\n    ----------\n    gages\n        Camels, CamelsSeries, Gages or GagesPro object\n    usgs_ids: list\n        given sites' ids\n    smallest_area\n        lower limit; unit is km2\n    largest_area\n        upper limit; unit is km2\n\n    Returns\n    -------\n    list\n        sites_chosen: [] -- ids of chosen gages\n\n    \"\"\"\n    basins_areas = gages.read_basin_area(usgs_ids).flatten()\n    sites_index = np.arange(len(usgs_ids))\n    sites_chosen = np.ones(len(usgs_ids))\n    for i in range(sites_index.size):\n        # loop for every site\n        if basins_areas[i] &lt; smallest_area or basins_areas[i] &gt; largest_area:\n            sites_chosen[sites_index[i]] = 0\n        else:\n            sites_chosen[sites_index[i]] = 1\n    return [usgs_ids[i] for i in range(len(sites_chosen)) if sites_chosen[i] &gt; 0]\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.choose_sites_in_ecoregion","title":"<code>choose_sites_in_ecoregion(gages, site_ids, ecoregion)</code>","text":"<p>Choose sites in ecoregions</p>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.choose_sites_in_ecoregion--parameters","title":"Parameters","text":"<p>gages : Gages     Only gages dataset has ecoregion attribute site_ids : list     all ids of sites ecoregion : Union[list, tuple]     which ecoregions</p>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.choose_sites_in_ecoregion--returns","title":"Returns","text":"<p>list     chosen sites' ids</p>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.choose_sites_in_ecoregion--raises","title":"Raises","text":"<p>NotImplementedError     PLease choose 'ECO2_CODE' or 'ECO3_CODE' NotImplementedError     must be in EC02 code list NotImplementedError     must be in EC03 code list</p> Source code in <code>torchhydro/datasets/data_utils.py</code> <pre><code>def choose_sites_in_ecoregion(\n    gages, site_ids: list, ecoregion: Union[list, tuple]\n) -&gt; list:\n    \"\"\"\n    Choose sites in ecoregions\n\n    Parameters\n    ----------\n    gages : Gages\n        Only gages dataset has ecoregion attribute\n    site_ids : list\n        all ids of sites\n    ecoregion : Union[list, tuple]\n        which ecoregions\n\n    Returns\n    -------\n    list\n        chosen sites' ids\n\n    Raises\n    ------\n    NotImplementedError\n        PLease choose 'ECO2_CODE' or 'ECO3_CODE'\n    NotImplementedError\n        must be in EC02 code list\n    NotImplementedError\n        must be in EC03 code list\n    \"\"\"\n    if ecoregion[0] not in [\"ECO2_CODE\", \"ECO3_CODE\"]:\n        raise NotImplementedError(\"PLease choose 'ECO2_CODE' or 'ECO3_CODE'\")\n    if ecoregion[0] == \"ECO2_CODE\":\n        ec02_code_lst = [\n            5.2,\n            5.3,\n            6.2,\n            7.1,\n            8.1,\n            8.2,\n            8.3,\n            8.4,\n            8.5,\n            9.2,\n            9.3,\n            9.4,\n            9.5,\n            9.6,\n            10.1,\n            10.2,\n            10.4,\n            11.1,\n            12.1,\n            13.1,\n        ]\n        if ecoregion[1] not in ec02_code_lst:\n            raise NotImplementedError(\n                f\"No such EC02 code, please choose from {ec02_code_lst}\"\n            )\n        attr_name = \"ECO2_BAS_DOM\"\n    elif ecoregion[1] in np.arange(1, 85):\n        attr_name = \"ECO3_BAS_DOM\"\n    else:\n        raise NotImplementedError(\"No such EC03 code, please choose from 1 - 85\")\n    attr_lst = [attr_name]\n    data_attr = gages.read_constant_cols(site_ids, attr_lst)\n    eco_names = data_attr[:, 0]\n    return [site_ids[i] for i in range(eco_names.size) if eco_names[i] == ecoregion[1]]\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.dam_num_chosen","title":"<code>dam_num_chosen(gages, usgs_id, dam_num)</code>","text":"<p>choose basins of dams</p> Source code in <code>torchhydro/datasets/data_utils.py</code> <pre><code>def dam_num_chosen(gages, usgs_id, dam_num):\n    \"\"\"choose basins of dams\"\"\"\n    assert all(x &lt; y for x, y in zip(usgs_id, usgs_id[1:]))\n    attr_lst = [\"NDAMS_2009\"]\n    data_attr = gages.read_constant_cols(usgs_id, attr_lst)\n    return (\n        [\n            usgs_id[i]\n            for i in range(data_attr.size)\n            if dam_num[0] &lt;= data_attr[:, 0][i] &lt; dam_num[1]\n        ]\n        if type(dam_num) == list\n        else [\n            usgs_id[i] for i in range(data_attr.size) if data_attr[:, 0][i] == dam_num\n        ]\n    )\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.dor_reservoirs_chosen","title":"<code>dor_reservoirs_chosen(gages, usgs_id, dor_chosen)</code>","text":"<p>choose basins of small DOR(calculated by NOR_STORAGE/RUNAVE7100)</p> Source code in <code>torchhydro/datasets/data_utils.py</code> <pre><code>def dor_reservoirs_chosen(gages, usgs_id, dor_chosen) -&gt; list:\n    \"\"\"\n    choose basins of small DOR(calculated by NOR_STORAGE/RUNAVE7100)\n\n    \"\"\"\n\n    dors = get_dor_values(gages, usgs_id)\n    if type(dor_chosen) in [list, tuple]:\n        # right half-open range\n        chosen_id = [\n            usgs_id[i]\n            for i in range(dors.size)\n            if dor_chosen[0] &lt;= dors[i] &lt; dor_chosen[1]\n        ]\n    elif dor_chosen &lt; 0:\n        chosen_id = [usgs_id[i] for i in range(dors.size) if dors[i] &lt; -dor_chosen]\n    else:\n        chosen_id = [usgs_id[i] for i in range(dors.size) if dors[i] &gt;= dor_chosen]\n\n    assert all(x &lt; y for x, y in zip(chosen_id, chosen_id[1:]))\n    return chosen_id\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.set_unit_to_var","title":"<code>set_unit_to_var(ds)</code>","text":"<p>returned xa.Dataset need has units for each variable -- xr.DataArray or the dataset cannot be saved to netCDF file</p>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.set_unit_to_var--parameters","title":"Parameters","text":"<p>ds : xr.Dataset     the dataset with units as attributes</p>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.set_unit_to_var--returns","title":"Returns","text":"<p>ds : xr.Dataset     unit attrs are for each variable dataarray</p> Source code in <code>torchhydro/datasets/data_utils.py</code> <pre><code>def set_unit_to_var(ds):\n    \"\"\"returned xa.Dataset need has units for each variable -- xr.DataArray\n    or the dataset cannot be saved to netCDF file\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        the dataset with units as attributes\n\n    Returns\n    -------\n    ds : xr.Dataset\n        unit attrs are for each variable dataarray\n    \"\"\"\n    units_dict = ds.attrs[\"units\"]\n    for var_name, units in units_dict.items():\n        if var_name in ds:\n            ds[var_name].attrs[\"units\"] = units\n    if \"units\" in ds.attrs:\n        del ds.attrs[\"units\"]\n    return ds\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.unify_streamflow_unit","title":"<code>unify_streamflow_unit(ds, area=None, inverse=False)</code>","text":"<p>Unify the unit of xr_dataset to be mm/day in a basin or inverse</p>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.unify_streamflow_unit--parameters","title":"Parameters","text":"<p>!!! ds \"xarray dataset\"     description !!! area     area of each basin</p>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.unify_streamflow_unit--returns","title":"Returns","text":"<p>type description</p> Source code in <code>torchhydro/datasets/data_utils.py</code> <pre><code>def unify_streamflow_unit(ds: xr.Dataset, area=None, inverse=False):\n    \"\"\"Unify the unit of xr_dataset to be mm/day in a basin or inverse\n\n    Parameters\n    ----------\n    ds: xarray dataset\n        _description_\n    area:\n        area of each basin\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    # use pint to convert unit\n    if not inverse:\n        target_unit = \"mm/d\"\n        q = ds.pint.quantify()\n        a = area.pint.quantify()\n        r = q[list(q.keys())[0]] / a[list(a.keys())[0]]\n        result = r.pint.to(target_unit).to_dataset(name=list(q.keys())[0])\n    else:\n        target_unit = \"m^3/s\"\n        r = ds.pint.quantify()\n        a = area.pint.quantify()\n        q = r[list(r.keys())[0]] * a[list(a.keys())[0]]\n        # q = q.pint.quantify()\n        result = q.pint.to(target_unit).to_dataset(name=list(r.keys())[0])\n    # dequantify to get normal xr_dataset\n    return result.pint.dequantify()\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.warn_if_nan","title":"<code>warn_if_nan(dataarray, max_display=5, nan_mode='any', data_name='')</code>","text":"<p>Issue a warning if the dataarray contains any NaN values and display their locations.</p>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.warn_if_nan--parameters","title":"Parameters","text":"<p>!!! dataarray \"xr.DataArray\"     Input dataarray to check for NaN values. !!! max_display \"int\"     Maximum number of NaN locations to display in the warning. !!! nan_mode \"str\"     Mode of NaN checking:     'any' means if any NaNs exist return True, if all values are NaNs raise ValueError     'all' means if all values are NaNs return True !!! data_name \"str\"     Name of the dataarray to be displayed in the warning message.</p> Source code in <code>torchhydro/datasets/data_utils.py</code> <pre><code>def warn_if_nan(dataarray, max_display=5, nan_mode=\"any\", data_name=\"\"):\n    \"\"\"\n    Issue a warning if the dataarray contains any NaN values and display their locations.\n\n    Parameters\n    -----------\n    dataarray: xr.DataArray\n        Input dataarray to check for NaN values.\n    max_display: int\n        Maximum number of NaN locations to display in the warning.\n    nan_mode: str\n        Mode of NaN checking:\n        'any' means if any NaNs exist return True, if all values are NaNs raise ValueError\n        'all' means if all values are NaNs return True\n    data_name: str\n        Name of the dataarray to be displayed in the warning message.\n    \"\"\"\n    if dataarray is None:\n        raise ValueError(\"The dataarray is None!\")\n    if nan_mode not in [\"any\", \"all\"]:\n        raise ValueError(\"nan_mode must be 'any' or 'all'\")\n\n    if np.all(np.isnan(dataarray.values)):\n        if nan_mode == \"any\":\n            raise ValueError(\"The dataarray contains only NaN values!\")\n        else:\n            return True\n\n    nan_indices = np.argwhere(np.isnan(dataarray.values))\n    total_nans = len(nan_indices)\n\n    if total_nans &lt;= 0:\n        return False\n    message = f\"The {data_name} dataarray contains {total_nans} NaN values!\"\n\n    # Displaying only the first few NaN locations if there are too many\n    display_indices = nan_indices[:max_display].tolist()\n    message += (\n        f\" Here are the indices of the first {max_display} NaNs: {display_indices}...\"\n        if total_nans &gt; max_display\n        else f\" Here are the indices of the NaNs: {display_indices}\"\n    )\n    warnings.warn(message)\n\n    return True\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.wrap_t_s_dict","title":"<code>wrap_t_s_dict(data_cfgs, is_tra_val_te)</code>","text":"<p>Basins and periods</p>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.wrap_t_s_dict--parameters","title":"Parameters","text":"<p>data_cfgs     configs for reading from data source is_tra_val_te     train, valid or test</p>"},{"location":"api/datasets/#torchhydro.datasets.data_utils.wrap_t_s_dict--returns","title":"Returns","text":"<p>OrderedDict     OrderedDict(sites_id=basins_id, t_final_range=t_range_list)</p> Source code in <code>torchhydro/datasets/data_utils.py</code> <pre><code>def wrap_t_s_dict(data_cfgs: dict, is_tra_val_te: str) -&gt; OrderedDict:\n    \"\"\"\n    Basins and periods\n\n    Parameters\n    ----------\n    data_cfgs\n        configs for reading from data source\n    is_tra_val_te\n        train, valid or test\n\n    Returns\n    -------\n    OrderedDict\n        OrderedDict(sites_id=basins_id, t_final_range=t_range_list)\n    \"\"\"\n    basins_id = data_cfgs[\"object_ids\"]\n    if type(basins_id) is str and basins_id == \"ALL\":\n        raise ValueError(\"Please specify the basins_id in configs!\")\n    if any(x &gt;= y for x, y in zip(basins_id, basins_id[1:])):\n        # raise a warning if the basins_id is not sorted\n        warnings.warn(\"The basins_id is not sorted!\")\n    if f\"t_range_{is_tra_val_te}\" in data_cfgs:\n        t_range_list = data_cfgs[f\"t_range_{is_tra_val_te}\"]\n    else:\n        raise KeyError(f\"Error! The mode {is_tra_val_te} was not found. Please add it.\")\n    return OrderedDict(sites_id=basins_id, t_final_range=t_range_list)\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.sampler","title":"<code>sampler</code>","text":"<p>Author: Wenyu Ouyang Date: 2023-09-25 08:21:27 LastEditTime: 2025-07-13 15:47:53 LastEditors: Wenyu Ouyang Description: Some sampling class or functions FilePath:       orchhydro       orchhydro\\datasets\\sampler.py Copyright (c) 2023-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/datasets/#torchhydro.datasets.sampler.BasinBatchSampler","title":"<code> BasinBatchSampler            (Sampler)         </code>","text":"<p>A custom sampler for hydrological modeling that iterates over a dataset in a way tailored for batches of hydrological data. It ensures that each batch contains data from a single randomly selected 'basin' out of several basins, with batches constructed to respect the specified batch size and the unique characteristics of hydrological datasets. TODO: made by Xinzhuo Wu, maybe need to be tested more</p>"},{"location":"api/datasets/#torchhydro.datasets.sampler.BasinBatchSampler--parameters","title":"Parameters","text":"<p>dataset : BaseDataset     The dataset to sample from, expected to have a <code>data_cfgs</code> attribute. num_samples : Optional[int], default=None     The total number of samples to draw (optional). generator : Optional[torch.Generator]     A PyTorch Generator object for random number generation (optional).</p> <p>The sampler divides the dataset by the number of basins, then iterates through each basin's range in shuffled order, ensuring non-overlapping, basin-specific batches suitable for models that predict hydrological outcomes.</p> Source code in <code>torchhydro/datasets/sampler.py</code> <pre><code>class BasinBatchSampler(Sampler[int]):\n    \"\"\"\n    A custom sampler for hydrological modeling that iterates over a dataset in\n    a way tailored for batches of hydrological data. It ensures that each batch\n    contains data from a single randomly selected 'basin' out of several basins,\n    with batches constructed to respect the specified batch size and the unique\n    characteristics of hydrological datasets.\n    TODO: made by Xinzhuo Wu, maybe need to be tested more\n\n    Parameters\n    ----------\n    dataset : BaseDataset\n        The dataset to sample from, expected to have a `data_cfgs` attribute.\n    num_samples : Optional[int], default=None\n        The total number of samples to draw (optional).\n    generator : Optional[torch.Generator]\n        A PyTorch Generator object for random number generation (optional).\n\n    The sampler divides the dataset by the number of basins, then iterates through\n    each basin's range in shuffled order, ensuring non-overlapping, basin-specific\n    batches suitable for models that predict hydrological outcomes.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset,\n        num_samples: Optional[int] = None,\n        generator=None,\n    ) -&gt; None:\n        self.dataset = dataset\n        self._num_samples = num_samples\n        self.generator = generator\n\n        if not isinstance(self.num_samples, int) or self.num_samples &lt;= 0:\n            raise ValueError(\n                f\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\n            )\n\n    @property\n    def num_samples(self) -&gt; int:\n        return len(self.dataset)\n\n    def __iter__(self) -&gt; Iterator[int]:\n        n = self.dataset.training_cfgs[\"batch_size\"]\n        basin_number = len(self.dataset.data_cfgs[\"object_ids\"])\n        basin_range = len(self.dataset) // basin_number\n        if n &gt; basin_range:\n            raise ValueError(\n                f\"batch_size should equal or less than basin_range={basin_range} \"\n            )\n\n        if self.generator is None:\n            seed = int(torch.empty((), dtype=torch.int64).random_().item())\n            generator = torch.Generator()\n            generator.manual_seed(seed)\n        else:\n            generator = self.generator\n\n        # basin_list = torch.randperm(basin_number)\n        # for select_basin in basin_list:\n        #     x = torch.randperm(basin_range)\n        #     for i in range(0, basin_range, n):\n        #         yield from (x[i : i + n] + basin_range * select_basin.item()).tolist()\n        x = torch.randperm(self.num_samples)\n        for i in range(0, self.num_samples, n):\n            yield from (x[i : i + n]).tolist()\n\n    def __len__(self) -&gt; int:\n        return self.num_samples\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.sampler.KuaiSampler","title":"<code> KuaiSampler            (RandomSampler)         </code>","text":"Source code in <code>torchhydro/datasets/sampler.py</code> <pre><code>class KuaiSampler(RandomSampler):\n    def __init__(\n        self,\n        dataset,\n        batch_size,\n        warmup_length,\n        rho_horizon,\n        ngrid,\n        nt,\n    ):\n        \"\"\"a sampler from Kuai Fang's paper: https://doi.org/10.1002/2017GL075619\n           He used a random pick-up that we don't need to iterate all samples.\n           Then, we can train model more quickly\n\n        Parameters\n        ----------\n        dataset : torch.utils.data.Dataset\n            just a object of dataset class inherited from torch.utils.data.Dataset\n        batch_size : int\n            we need batch_size to calculate the number of samples in an epoch\n        warmup_length : int\n            warmup length, typically for physical hydrological models\n        rho_horizon : int\n            sequence length of a mini-batch, for encoder-decoder models, rho+horizon, for decoder-only models, horizon\n        ngrid : int\n            number of basins\n        nt : int\n            number of all periods\n        \"\"\"\n        while batch_size * rho_horizon &gt;= ngrid * nt:\n            # try to use a smaller batch_size to make the model runnable\n            batch_size = int(batch_size / 10)\n        batch_size = max(batch_size, 1)\n        # 99% chance that all periods' data are used in an epoch\n        n_iter_ep = int(\n            np.ceil(\n                np.log(0.01)\n                / np.log(1 - batch_size * rho_horizon / ngrid / (nt - warmup_length))\n            )\n        )\n        assert n_iter_ep &gt;= 1\n        # __len__ means the number of all samples, then, the number of loops in an epoch is __len__()/batch_size = n_iter_ep\n        # hence we return n_iter_ep * batch_size\n        num_samples = n_iter_ep * batch_size\n        super(KuaiSampler, self).__init__(dataset, num_samples=num_samples)\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.sampler.KuaiSampler.__init__","title":"<code>__init__(self, dataset, batch_size, warmup_length, rho_horizon, ngrid, nt)</code>  <code>special</code>","text":"<p>a sampler from Kuai Fang's paper: https://doi.org/10.1002/2017GL075619    He used a random pick-up that we don't need to iterate all samples.    Then, we can train model more quickly</p>"},{"location":"api/datasets/#torchhydro.datasets.sampler.KuaiSampler.__init__--parameters","title":"Parameters","text":"<p>dataset : torch.utils.data.Dataset     just a object of dataset class inherited from torch.utils.data.Dataset batch_size : int     we need batch_size to calculate the number of samples in an epoch warmup_length : int     warmup length, typically for physical hydrological models rho_horizon : int     sequence length of a mini-batch, for encoder-decoder models, rho+horizon, for decoder-only models, horizon ngrid : int     number of basins nt : int     number of all periods</p> Source code in <code>torchhydro/datasets/sampler.py</code> <pre><code>def __init__(\n    self,\n    dataset,\n    batch_size,\n    warmup_length,\n    rho_horizon,\n    ngrid,\n    nt,\n):\n    \"\"\"a sampler from Kuai Fang's paper: https://doi.org/10.1002/2017GL075619\n       He used a random pick-up that we don't need to iterate all samples.\n       Then, we can train model more quickly\n\n    Parameters\n    ----------\n    dataset : torch.utils.data.Dataset\n        just a object of dataset class inherited from torch.utils.data.Dataset\n    batch_size : int\n        we need batch_size to calculate the number of samples in an epoch\n    warmup_length : int\n        warmup length, typically for physical hydrological models\n    rho_horizon : int\n        sequence length of a mini-batch, for encoder-decoder models, rho+horizon, for decoder-only models, horizon\n    ngrid : int\n        number of basins\n    nt : int\n        number of all periods\n    \"\"\"\n    while batch_size * rho_horizon &gt;= ngrid * nt:\n        # try to use a smaller batch_size to make the model runnable\n        batch_size = int(batch_size / 10)\n    batch_size = max(batch_size, 1)\n    # 99% chance that all periods' data are used in an epoch\n    n_iter_ep = int(\n        np.ceil(\n            np.log(0.01)\n            / np.log(1 - batch_size * rho_horizon / ngrid / (nt - warmup_length))\n        )\n    )\n    assert n_iter_ep &gt;= 1\n    # __len__ means the number of all samples, then, the number of loops in an epoch is __len__()/batch_size = n_iter_ep\n    # hence we return n_iter_ep * batch_size\n    num_samples = n_iter_ep * batch_size\n    super(KuaiSampler, self).__init__(dataset, num_samples=num_samples)\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.sampler.fl_sample_basin","title":"<code>fl_sample_basin(dataset)</code>","text":"<p>Sample one basin data as a client from a dataset for federated learning</p>"},{"location":"api/datasets/#torchhydro.datasets.sampler.fl_sample_basin--parameters","title":"Parameters","text":"<p>dataset     dataset</p>"},{"location":"api/datasets/#torchhydro.datasets.sampler.fl_sample_basin--returns","title":"Returns","text":"<pre><code>dict of image index\n</code></pre> Source code in <code>torchhydro/datasets/sampler.py</code> <pre><code>def fl_sample_basin(dataset: BaseDataset):\n    \"\"\"\n    Sample one basin data as a client from a dataset for federated learning\n\n    Parameters\n    ----------\n    dataset\n        dataset\n\n    Returns\n    -------\n        dict of image index\n    \"\"\"\n    lookup_table = dataset.lookup_table\n    basins = dataset.basins\n    # one basin is one user\n    num_users = len(basins)\n    # set group for basins\n    basin_groups = defaultdict(list)\n    for idx, (basin, date) in lookup_table.items():\n        basin_groups[basin].append(idx)\n\n    # one user is one basin\n    user_basins = defaultdict(list)\n    for i, basin in enumerate(basins):\n        user_id = i % num_users\n        user_basins[user_id].append(basin)\n\n    # a lookup_table subset for each user\n    user_lookup_tables = {}\n    for user_id, basins in user_basins.items():\n        user_lookup_table = {}\n        for basin in basins:\n            for idx in basin_groups[basin]:\n                user_lookup_table[idx] = lookup_table[idx]\n        user_lookup_tables[user_id] = user_lookup_table\n\n    return user_lookup_tables\n</code></pre>"},{"location":"api/datasets/#torchhydro.datasets.sampler.fl_sample_region","title":"<code>fl_sample_region(dataset)</code>","text":"<p>Sample one region data as a client from a dataset for federated learning</p> <p>TODO: not finished</p> Source code in <code>torchhydro/datasets/sampler.py</code> <pre><code>def fl_sample_region(dataset: BaseDataset):\n    \"\"\"\n    Sample one region data as a client from a dataset for federated learning\n\n    TODO: not finished\n\n    \"\"\"\n    num_users = 10\n    num_shards, num_imgs = 200, 250\n    idx_shard = list(range(num_shards))\n    dict_users = {i: np.array([]) for i in range(num_users)}\n    idxs = np.arange(num_shards * num_imgs)\n    # labels = dataset.train_labels.numpy()\n    labels = np.array(dataset.train_labels)\n\n    # sort labels\n    idxs_labels = np.vstack((idxs, labels))\n    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n    idxs = idxs_labels[0, :]\n\n    # divide and assign\n    for i in range(num_users):\n        rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n        idx_shard = list(set(idx_shard) - rand_set)\n        for rand in rand_set:\n            dict_users[i] = np.concatenate(\n                (dict_users[i], idxs[rand * num_imgs : (rand + 1) * num_imgs]), axis=0\n            )\n    return dict_users\n</code></pre>"},{"location":"api/explainers/","title":"Explainers API","text":""},{"location":"api/explainers/#torchhydro.explainers.shap","title":"<code>shap</code>","text":"<p>Author: Wenyu Ouyang Date: 2023-10-19 21:34:29 LastEditTime: 2025-07-12 11:25:01 LastEditors: Wenyu Ouyang Description: SHAP methods for deep learning models FilePath: /torchhydro/torchhydro/explainers/shap.py Copyright (c) 2023-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/explainers/#torchhydro.explainers.shap.deep_explain_model_heatmap","title":"<code>deep_explain_model_heatmap(dl_model, test_dataset)</code>","text":"<p>Generate feature heatmap for prediction at a start time Parameters</p> <p>model     trained model test_dataset     test dataset Returns</p> <p>None</p> Source code in <code>torchhydro/explainers/shap.py</code> <pre><code>def deep_explain_model_heatmap(dl_model, test_dataset) -&gt; None:\n    \"\"\"Generate feature heatmap for prediction at a start time\n    Parameters\n    ----------\n    model\n        trained model\n    test_dataset\n        test dataset\n    Returns\n    -------\n    None\n    \"\"\"\n    dl_model.eval()\n    history = test_dataset.__getitem__(0)[0]\n    # background shape (L, N, M)\n    # L - batch size, N - history length, M - feature size\n    # for each element in each N x M batch in L,\n    # attribute to each prediction in forecast len\n    deep_explainer = shap.DeepExplainer(dl_model, history)\n    shap_values = shap_results(deep_explainer, history)\n    figs = plot_shap_value_heatmaps(shap_values)\n</code></pre>"},{"location":"api/explainers/#torchhydro.explainers.shap.deep_explain_model_summary_plot","title":"<code>deep_explain_model_summary_plot(dl_model, test_dataset)</code>","text":"<p>Generate feature summary plot for trained deep learning models</p>"},{"location":"api/explainers/#torchhydro.explainers.shap.deep_explain_model_summary_plot--parameters","title":"Parameters","text":"<p>model     trained model test_dataset     test dataset</p> Source code in <code>torchhydro/explainers/shap.py</code> <pre><code>def deep_explain_model_summary_plot(dl_model, test_dataset) -&gt; None:\n    \"\"\"Generate feature summary plot for trained deep learning models\n\n    Parameters\n    ----------\n    model\n        trained model\n    test_dataset\n        test dataset\n    \"\"\"\n    dl_model.eval()\n    history = test_dataset.__getitem__(0)[0].unsqueeze(0)\n    dl_model = dl_model.to(\"cpu\")\n    deep_explainer = shap.DeepExplainer(dl_model, history)\n    shap_values = shap_results(deep_explainer, history)\n    # summary plot shows overall feature ranking\n    # by average absolute shap values\n    fig = plot_summary_shap_values(shap_values, test_dataset.df.columns)\n    abs_mean_shap_values = shap_values.mean(axis=[\"preds\", \"batches\"])\n    multi_shap_values = abs_mean_shap_values.mean(axis=\"observations\")\n    # summary plot for multi-step outputs\n    # multi_shap_values = shap_values.apply_along_axis(np.mean, 'batches')\n    fig = plot_summary_shap_values_over_time_series(\n        shap_values, test_dataset.df.columns\n    )\n    history_numpy = torch.tensor(\n        history.cpu().numpy(), names=[\"batches\", \"observations\", \"features\"]\n    )\n\n    shap_values = shap_results(deep_explainer, history)\n    figs = plot_shap_values_from_history(shap_values, history_numpy)\n</code></pre>"},{"location":"api/explainers/#torchhydro.explainers.shap.jitter","title":"<code>jitter(values, jitter_strength=0.005)</code>","text":"<p>Add some jitter to the values.</p> Source code in <code>torchhydro/explainers/shap.py</code> <pre><code>def jitter(values, jitter_strength=0.005):\n    \"\"\"Add some jitter to the values.\"\"\"\n    return values + jitter_strength * np.random.randn(*values.shape)\n</code></pre>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis","title":"<code>uncertainty_analysis</code>","text":""},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.bin_aggregated_data","title":"<code>bin_aggregated_data(z_values, r_values, num_bins=10)</code>","text":"<p>Aggregate the z-values and r-values into bins, and calculate the average for each bin.</p>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.bin_aggregated_data--parameters","title":"Parameters","text":"<p>!!! z_values \"np.ndarray\"     Aggregated z-values (quantiles) across all basins and time steps !!! r_values \"np.ndarray\"     Aggregated r-values (ECDF) across all basins and time steps !!! num_bins \"int\"     Number of bins to divide the data into (default is 10)</p>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.bin_aggregated_data--returns","title":"Returns","text":"<p>!!! binned_z \"np.ndarray\"     Average z-values for each bin !!! binned_r \"np.ndarray\"     Average r-values for each bin</p> Source code in <code>torchhydro/explainers/uncertainty_analysis.py</code> <pre><code>def bin_aggregated_data(z_values, r_values, num_bins=10):\n    \"\"\"\n    Aggregate the z-values and r-values into bins, and calculate the average for each bin.\n\n    Parameters\n    ----------\n    z_values: np.ndarray\n        Aggregated z-values (quantiles) across all basins and time steps\n    r_values: np.ndarray\n        Aggregated r-values (ECDF) across all basins and time steps\n    num_bins: int\n        Number of bins to divide the data into (default is 10)\n\n    Returns\n    -------\n    binned_z: np.ndarray\n        Average z-values for each bin\n    binned_r: np.ndarray\n        Average r-values for each bin\n    \"\"\"\n    bins = np.linspace(0, 1, num_bins + 1)  # Create bin edges from 0 to 1\n    bin_indices = np.digitize(\n        z_values, bins\n    )  # Find out which bin each z-value belongs to\n\n    binned_z = np.zeros(num_bins)\n    binned_r = np.zeros(num_bins)\n\n    for i in range(1, num_bins + 1):\n        # Select the z and r values that fall into the current bin\n        in_bin = bin_indices == i\n\n        if np.sum(in_bin) &gt; 0:\n            # Calculate the average z and r values for this bin\n            binned_z[i - 1] = np.mean(z_values[in_bin])\n            binned_r[i - 1] = np.mean(r_values[in_bin])\n        else:\n            # If no values in this bin, assign NaN (can also handle this differently if needed)\n            binned_z[i - 1] = np.nan\n            binned_r[i - 1] = np.nan\n\n    return binned_z, binned_r\n</code></pre>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.calculate_empirical_cdf","title":"<code>calculate_empirical_cdf(predictions, obs_values)</code>","text":"<p>Calculate the empirical cumulative distribution function (CDF) of model predictions for each time step and compute the quantiles z_i of the observed values.</p>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.calculate_empirical_cdf--parameters","title":"Parameters","text":"<p>!!! predictions \"np.ndarray\"     2D array of predictions (shape = (MC_dropout_times, time_steps)) !!! obs_values \"np.ndarray\"     Time series of observed values (1D array, shape = (time_steps,))</p>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.calculate_empirical_cdf--returns","title":"Returns","text":"<p>!!! z_values \"np.ndarray\"     Quantiles of the observed values in the prediction distribution for each time step (1D array)</p> Source code in <code>torchhydro/explainers/uncertainty_analysis.py</code> <pre><code>def calculate_empirical_cdf(predictions, obs_values):\n    \"\"\"\n    Calculate the empirical cumulative distribution function (CDF) of model predictions for each time step\n    and compute the quantiles z_i of the observed values.\n\n    Parameters\n    ----------\n    predictions: np.ndarray\n        2D array of predictions (shape = (MC_dropout_times, time_steps))\n    obs_values: np.ndarray\n        Time series of observed values (1D array, shape = (time_steps,))\n\n    Returns\n    -------\n    z_values: np.ndarray\n        Quantiles of the observed values in the prediction distribution for each time step (1D array)\n    \"\"\"\n    time_steps = obs_values.shape[0]\n    mc_dropout_times = predictions.shape[0]\n\n    z_values = np.zeros(time_steps)\n\n    for t in range(time_steps):\n        # Sort the predictions for each time step\n        sorted_predictions = np.sort(predictions[:, t])\n\n        # Calculate the quantile of the observed value in the prediction distribution\n        z_values[t] = np.sum(sorted_predictions &lt;= obs_values[t]) / mc_dropout_times\n\n    return z_values\n</code></pre>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.calculate_observed_ecdf","title":"<code>calculate_observed_ecdf(obs_values)</code>","text":"<p>Calculate the empirical cumulative distribution function (ECDF) of observed values.</p>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.calculate_observed_ecdf--parameters","title":"Parameters","text":"<p>!!! obs_values \"np.ndarray\"     Time series of observed values (1D array, shape = (time_steps,))</p>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.calculate_observed_ecdf--returns","title":"Returns","text":"<p>!!! ecdf \"np.ndarray\"     Proportion of observed values in the cumulative distribution (1D array, same shape as obs_values)</p> Source code in <code>torchhydro/explainers/uncertainty_analysis.py</code> <pre><code>def calculate_observed_ecdf(obs_values):\n    \"\"\"\n    Calculate the empirical cumulative distribution function (ECDF) of observed values.\n\n    Parameters\n    ----------\n    obs_values: np.ndarray\n        Time series of observed values (1D array, shape = (time_steps,))\n\n    Returns\n    -------\n    ecdf: np.ndarray\n        Proportion of observed values in the cumulative distribution (1D array, same shape as obs_values)\n    \"\"\"\n    time_steps = obs_values.shape[0]\n\n    # Sort observed values\n    sorted_obs = np.sort(obs_values)\n\n    # Calculate ECDF: for each observed value, the proportion of values &lt;= that value\n    ecdf = np.zeros(time_steps)\n    for i in range(time_steps):\n        ecdf[i] = np.sum(sorted_obs &lt;= obs_values[i]) / time_steps\n\n    return ecdf\n</code></pre>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.plot_probability_plot","title":"<code>plot_probability_plot(z_values, r_values, basin_name='Basin', scatter=True)</code>","text":"<p>Plot the probability plot for a single basin.</p>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.plot_probability_plot--parameters","title":"Parameters","text":"<p>!!! z_values \"np.ndarray\"     Quantiles of the observed values for each time step (1D array) !!! r_values \"np.ndarray\"     Empirical cumulative distribution of observed values (ECDF, 1D array) !!! basin_name \"str\"     Name of the basin (for title of the plot)</p> Source code in <code>torchhydro/explainers/uncertainty_analysis.py</code> <pre><code>def plot_probability_plot(z_values, r_values, basin_name=\"Basin\", scatter=True):\n    \"\"\"\n    Plot the probability plot for a single basin.\n\n    Parameters\n    ----------\n    z_values: np.ndarray\n        Quantiles of the observed values for each time step (1D array)\n    r_values: np.ndarray\n        Empirical cumulative distribution of observed values (ECDF, 1D array)\n    basin_name: str\n        Name of the basin (for title of the plot)\n    \"\"\"\n    # Sort both z_values and r_values by z_values\n    sorted_indices = np.argsort(z_values)\n    z_sorted = z_values[sorted_indices]\n    r_sorted = r_values[sorted_indices]\n\n    # Plot the probability plot with markers only (no lines)\n    if scatter:\n        plt.scatter(\n            z_sorted, r_sorted, label=f\"Probability Plot ({basin_name})\", color=\"b\"\n        )\n    else:\n        plt.plot(\n            z_sorted, r_sorted, label=f\"Probability Plot ({basin_name})\", color=\"b\"\n        )\n    plt.plot([0, 1], [0, 1], \"k--\", label=\"1:1 Line\")\n\n    # Add labels and grid\n    plt.xlabel(\"Predicted CDF Quantiles (z_i)\")\n    plt.ylabel(\"Observed ECDF (R_i/n)\")\n    plt.title(f\"Probability Plot - {basin_name}\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n</code></pre>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.process_and_aggregate_basins","title":"<code>process_and_aggregate_basins(basins_data, num_bins=0)</code>","text":"<p>Process multiple basins: aggregate z-values and r-values across basins and time steps.</p>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.process_and_aggregate_basins--parameters","title":"Parameters","text":"<p>!!! basins_data \"list of dict\"     List of dictionaries, each containing 'predictions', 'obs_values', and 'name' for a basin. !!! num_bins \"int\"     Number of bins to divide the data into (default is 0, which does not bin the data)</p>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.process_and_aggregate_basins--returns","title":"Returns","text":"<p>!!! all_z_values \"np.ndarray\"     Aggregated z-values (quantiles) across all basins and time steps !!! all_r_values \"np.ndarray\"     Aggregated r-values (ECDF) across all basins and time steps</p> Source code in <code>torchhydro/explainers/uncertainty_analysis.py</code> <pre><code>def process_and_aggregate_basins(basins_data, num_bins=0):\n    \"\"\"\n    Process multiple basins: aggregate z-values and r-values across basins and time steps.\n\n    Parameters\n    ----------\n    basins_data: list of dict\n        List of dictionaries, each containing 'predictions', 'obs_values', and 'name' for a basin.\n    num_bins: int\n        Number of bins to divide the data into (default is 0, which does not bin the data)\n\n    Returns\n    -------\n    all_z_values: np.ndarray\n        Aggregated z-values (quantiles) across all basins and time steps\n    all_r_values: np.ndarray\n        Aggregated r-values (ECDF) across all basins and time steps\n    \"\"\"\n    all_z_values = []\n    all_r_values = []\n\n    for basin_data in basins_data:\n        predictions = basin_data[\"predictions\"]\n        obs_values = basin_data[\"obs_values\"]\n\n        # Calculate z-values and r-values for the current basin\n        z_values = calculate_empirical_cdf(predictions, obs_values)\n        r_values = calculate_observed_ecdf(obs_values)\n\n        # Append the results to the global list\n        all_z_values.extend(z_values)\n        all_r_values.extend(r_values)\n\n    # Convert lists to numpy arrays\n    all_z_values = np.array(all_z_values)\n    all_r_values = np.array(all_r_values)\n\n    # Optionally bin the data\n    if num_bins &gt; 0:\n        all_z_values, all_r_values = bin_aggregated_data(\n            all_z_values, all_r_values, num_bins\n        )\n    return all_z_values, all_r_values\n</code></pre>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.process_basin","title":"<code>process_basin(predictions, obs_values, basin_name='Basin')</code>","text":"<p>Process a single basin: calculate z-values, ECDF, and plot the probability plot.</p>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.process_basin--parameters","title":"Parameters","text":"<p>!!! predictions \"np.ndarray\"     2D array of predictions for the basin (shape = (MC_dropout_times, time_steps)) !!! obs_values \"np.ndarray\"     Time series of observed values for the basin (1D array, shape = (time_steps,)) !!! basin_name \"str\"     Name of the basin (for title of the plot)</p> Source code in <code>torchhydro/explainers/uncertainty_analysis.py</code> <pre><code>def process_basin(predictions, obs_values, basin_name=\"Basin\"):\n    \"\"\"\n    Process a single basin: calculate z-values, ECDF, and plot the probability plot.\n\n    Parameters\n    ----------\n    predictions: np.ndarray\n        2D array of predictions for the basin (shape = (MC_dropout_times, time_steps))\n    obs_values: np.ndarray\n        Time series of observed values for the basin (1D array, shape = (time_steps,))\n    basin_name: str\n        Name of the basin (for title of the plot)\n    \"\"\"\n    # Calculate z-values (quantiles of the observed values in prediction distribution)\n    z_values = calculate_empirical_cdf(predictions, obs_values)\n\n    # Calculate observed ECDF\n    r_values = calculate_observed_ecdf(obs_values)\n\n    # Plot the probability plot for the basin\n    plot_probability_plot(z_values, r_values, basin_name)\n</code></pre>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.process_multiple_basins","title":"<code>process_multiple_basins(basins_data)</code>","text":"<p>Process multiple basins: for each basin, process and plot its probability plot.</p>"},{"location":"api/explainers/#torchhydro.explainers.uncertainty_analysis.process_multiple_basins--parameters","title":"Parameters","text":"<p>!!! basins_data \"list of dict\"     List of dictionaries, each containing 'predictions', 'obs_values', and 'name' for a basin.</p> Source code in <code>torchhydro/explainers/uncertainty_analysis.py</code> <pre><code>def process_multiple_basins(basins_data):\n    \"\"\"\n    Process multiple basins: for each basin, process and plot its probability plot.\n\n    Parameters\n    ----------\n    basins_data: list of dict\n        List of dictionaries, each containing 'predictions', 'obs_values', and 'name' for a basin.\n    \"\"\"\n    for basin_data in basins_data:\n        predictions = basin_data[\"predictions\"]\n        obs_values = basin_data[\"obs_values\"]\n        basin_name = basin_data[\"name\"]\n        process_basin(predictions, obs_values, basin_name)\n</code></pre>"},{"location":"api/explainers/#torchhydro.explainers.weight_anlysis","title":"<code>weight_anlysis</code>","text":""},{"location":"api/explainers/#torchhydro.explainers.weight_anlysis.get_latest_event_file","title":"<code>get_latest_event_file(event_file_lst)</code>","text":"<p>Get the latest event file in the current directory.</p>"},{"location":"api/explainers/#torchhydro.explainers.weight_anlysis.get_latest_event_file--returns","title":"Returns","text":"<p>str     The latest event file.</p> Source code in <code>torchhydro/explainers/weight_anlysis.py</code> <pre><code>def get_latest_event_file(event_file_lst):\n    \"\"\"Get the latest event file in the current directory.\n\n    Returns\n    -------\n    str\n        The latest event file.\n    \"\"\"\n    event_files = [Path(f) for f in event_file_lst]\n    event_file_names_lst = [event_file.stem.split(\".\") for event_file in event_files]\n    ctimes = [\n        int(event_file_names[event_file_names.index(\"tfevents\") + 1])\n        for event_file_names in event_file_names_lst\n    ]\n    return event_files[ctimes.index(max(ctimes))]\n</code></pre>"},{"location":"api/explainers/#torchhydro.explainers.weight_anlysis.plot_layer_hist_for_basin_model_fold","title":"<code>plot_layer_hist_for_basin_model_fold(model_name, chosen_layer_values, layers, save_fig_dir, cmap_str='Oranges')</code>","text":"<p>summary</p>"},{"location":"api/explainers/#torchhydro.explainers.weight_anlysis.plot_layer_hist_for_basin_model_fold--parameters","title":"Parameters","text":"<p>model_name : type description chosen_layer_values : type description layers : type description bsize : type description cmap_str : str, optional     chose from here: https://matplotlib.org/stable/tutorials/colors/colormaps.html#sequential, by default \"Oranges\"</p> Source code in <code>torchhydro/explainers/weight_anlysis.py</code> <pre><code>def plot_layer_hist_for_basin_model_fold(\n    model_name,\n    chosen_layer_values,\n    layers,\n    save_fig_dir,\n    cmap_str=\"Oranges\",\n):\n    \"\"\"_summary_\n\n    Parameters\n    ----------\n    model_name : _type_\n        _description_\n    chosen_layer_values : _type_\n        _description_\n    layers : _type_\n        _description_\n    bsize : _type_\n        _description_\n    cmap_str : str, optional\n        chose from here: https://matplotlib.org/stable/tutorials/colors/colormaps.html#sequential, by default \"Oranges\"\n    \"\"\"\n    if not os.path.exists(save_fig_dir):\n        os.makedirs(save_fig_dir)\n    for layer in layers:\n        two_model_layers = chosen_layer_values[layer]\n        try:\n            df_lstm_show = two_model_layers[model_name]\n        except KeyError:\n            # if the model does not have this layer, skip\n            continue\n        lstm_x_lst = []\n        lstm_y_lst = []\n        lstm_dash_lines = []\n        color_str = \"\"\n        lw_lst = []\n        alpha_lst = []\n        cmap = cm.get_cmap(cmap_str)\n        rgb_lst = []\n        norm_color = colors.Normalize(vmin=0, vmax=df_lstm_show.shape[1])\n        for i in df_lstm_show:\n            lstm_x_lst.append(df_lstm_show.index.values)\n            lstm_y_lst.append(df_lstm_show[i].values)\n            lstm_dash_lines.append(True)\n            color_str = color_str + \"r\"\n            rgba = cmap(norm_color(i))\n            rgb_lst.append(rgba)\n            alpha_lst.append(0.5)\n            lw_lst.append(0.5)\n        # the first and last line should be solid, have dark color and wide width\n        rgb_lst[0] = rgba\n        lstm_dash_lines[-1] = False\n        alpha_lst[-1] = 1\n        alpha_lst[0] = 1\n        lw_lst[-1] = 1\n        lw_lst[0] = 1\n        hplt.plot_ts(\n            lstm_x_lst,\n            lstm_y_lst,\n            dash_lines=lstm_dash_lines,\n            fig_size=(8, 4),\n            xlabel=\"weight_bias_value\",\n            ylabel=\"hist_num\",\n            # c_lst=color_str,\n            c_lst=rgb_lst,\n            linewidth=lw_lst,\n            alpha=alpha_lst,\n            leg_lst=None,\n        )\n        plt.savefig(\n            os.path.join(\n                save_fig_dir,\n                f\"{model_name}_{layer}_hist.png\",\n            ),\n            dpi=600,\n            bbox_inches=\"tight\",\n        )\n</code></pre>"},{"location":"api/explainers/#torchhydro.explainers.weight_anlysis.plot_param_hist_model","title":"<code>plot_param_hist_model(model_name, the_exp_dir, batchsize, chosen_layer_for_hist, start_epoch=0, end_epoch=100, epoch_interval=10, save_fig_dir=None)</code>","text":"<p>plot paramter histogram for each basin</p>"},{"location":"api/explainers/#torchhydro.explainers.weight_anlysis.plot_param_hist_model--parameters","title":"Parameters","text":"<p>!!! model_name \"str\"     name of a DL model the_exp_dir : str     saving directory for one experiment batchsize : int     batch size chosen_layer_for_hist : type description</p>"},{"location":"api/explainers/#torchhydro.explainers.weight_anlysis.plot_param_hist_model--returns","title":"Returns","text":"<p>type description</p> Source code in <code>torchhydro/explainers/weight_anlysis.py</code> <pre><code>def plot_param_hist_model(\n    model_name,\n    the_exp_dir,\n    batchsize,\n    chosen_layer_for_hist,\n    start_epoch=0,\n    end_epoch=100,\n    epoch_interval=10,\n    save_fig_dir=None,\n):\n    \"\"\"plot paramter histogram for each basin\n\n    Parameters\n    ----------\n    model_name: str\n        name of a DL model\n    the_exp_dir : str\n        saving directory for one experiment\n    batchsize : int\n        batch size\n    chosen_layer_for_hist : _type_\n        _description_\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    chosen_layer_values = {layer: {} for layer in chosen_layer_for_hist}\n    chosen_layer_values_consine = {layer: {} for layer in chosen_layer_for_hist}\n    _, df_histgram = read_tb_log(the_exp_dir, batchsize)\n    hist_cols = df_histgram.columns.values\n    model_layers = read_layer_name_from_tb_hist(hist_cols)\n    chosen_layers = chosen_layer_in_layers(model_layers, chosen_layer_for_hist)\n    k = 0\n    for layer in chosen_layer_for_hist:\n        if layer not in chosen_layers[k]:\n            continue\n        df_epochs_hist = epochs_hist_for_chosen_layer(\n            epoch_interval, chosen_layers[k], df_histgram\n        )\n        chosen_layer_values[layer][model_name] = df_epochs_hist\n        end_epoch_idx = int((end_epoch / epoch_interval - 1) * epoch_interval)\n        chosen_layer_values_consine[layer][model_name] = 1 - cosine(\n            df_epochs_hist[start_epoch], df_epochs_hist[end_epoch_idx]\n        )\n        k = k + 1\n    if save_fig_dir is None:\n        save_fig_dir = the_exp_dir\n    plot_layer_hist_for_basin_model_fold(\n        model_name,\n        chosen_layer_values,\n        chosen_layer_for_hist,\n        save_fig_dir=save_fig_dir,\n        cmap_str=\"Reds\",\n    )\n    return chosen_layer_values, chosen_layer_values_consine\n</code></pre>"},{"location":"api/explainers/#torchhydro.explainers.weight_anlysis.read_tb_log","title":"<code>read_tb_log(the_exp_dir, shown_batchsize, where_save=None)</code>","text":"<p>Copy a recent log file to the current directory and read the log file.</p>"},{"location":"api/explainers/#torchhydro.explainers.weight_anlysis.read_tb_log--parameters","title":"Parameters","text":"<p>the_exp_dir : str     saving directory for one experiment shown_batchsize : int     batch size for the experiment where_save : str, optional     A directory for saving plots, by default None,     which means same directory as the_exp_dir.     The reason we use a new dir different from the_exp_dir is that     ... (I forgot why... haha)</p>"},{"location":"api/explainers/#torchhydro.explainers.weight_anlysis.read_tb_log--returns","title":"Returns","text":"<p>type description</p>"},{"location":"api/explainers/#torchhydro.explainers.weight_anlysis.read_tb_log--raises","title":"Raises","text":"<p>FileNotFoundError     description</p> Source code in <code>torchhydro/explainers/weight_anlysis.py</code> <pre><code>def read_tb_log(the_exp_dir, shown_batchsize, where_save=None):\n    \"\"\"Copy a recent log file to the current directory and read the log file.\n\n    Parameters\n    ----------\n    the_exp_dir : str\n        saving directory for one experiment\n    shown_batchsize : int\n        batch size for the experiment\n    where_save : str, optional\n        A directory for saving plots, by default None,\n        which means same directory as the_exp_dir.\n        The reason we use a new dir different from the_exp_dir is that\n        ... (I forgot why... haha)\n\n    Returns\n    -------\n    _type_\n        _description_\n\n    Raises\n    ------\n    FileNotFoundError\n        _description_\n    \"\"\"\n    log_dir = os.path.join(\n        the_exp_dir,\n        f\"opt_Adadelta_lr_1.0_bsize_{str(shown_batchsize)}\",\n    )\n    if not os.path.exists(log_dir):\n        raise FileNotFoundError(f\"Log dir {log_dir} not found!\")\n    if where_save is None:\n        where_save = the_exp_dir\n        plot_save_dir = os.path.join(\n            where_save,\n            f\"opt_Adadelta_lr_1.0_bsize_{str(shown_batchsize)}\",\n        )\n    else:\n        plot_save_dir = os.path.join(\n            where_save,\n            f\"opt_Adadelta_lr_1.0_bsize_{str(shown_batchsize)}\",\n        )\n        copy_latest_tblog_file(log_dir, plot_save_dir)\n    scalar_file = os.path.join(plot_save_dir, \"scalars.csv\")\n    if not os.path.exists(scalar_file):\n        reader = SummaryReader(plot_save_dir)\n        df_scalar = reader.scalars\n        df_scalar.to_csv(scalar_file, index=False)\n    else:\n        df_scalar = pd.read_csv(scalar_file)\n\n    # reader = SummaryReader(result_dir)\n    histgram_file = os.path.join(plot_save_dir, \"histograms.pkl\")\n    if not os.path.exists(histgram_file):\n        reader = SummaryReader(plot_save_dir, pivot=True)\n        df_histgram = reader.histograms\n        # https://www.statology.org/pandas-save-dataframe/\n        df_histgram.to_pickle(histgram_file)\n    else:\n        df_histgram = pd.read_pickle(histgram_file)\n    return df_scalar, df_histgram\n</code></pre>"},{"location":"api/models/","title":"Models API","text":"<p>Author: Wenyu Ouyang Date: 2023-07-11 17:39:09 LastEditTime: 2023-07-11 20:40:37 LastEditors: Wenyu Ouyang Description: FilePath: \\HydroTL\\hydrotl\\models__init__.py Copyright (c) 2023-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/models/#torchhydro.models.ann","title":"<code>ann</code>","text":"<p>Author: Wenyu Ouyang Date: 2021-12-17 18:02:27 LastEditTime: 2025-06-25 14:07:58 LastEditors: Wenyu Ouyang Description: ANN model FilePath:       orchhydro       orchhydro\\models\u0007nn.py Copyright (c) 2021-2022 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/models/#torchhydro.models.ann.Mlp","title":"<code> Mlp            (SimpleAnn)         </code>","text":"Source code in <code>torchhydro/models/ann.py</code> <pre><code>class Mlp(SimpleAnn):\n    def __init__(\n        self,\n        nx: int,\n        ny: int,\n        hidden_size: Union[int, tuple, list] = None,\n        dr: Union[float, tuple, list] = 0.0,\n        activation: str = \"relu\",\n    ):\n        \"\"\"\n        MLP model inherited from SimpleAnn, using activation + dropout after each layer.\n        The final layer also goes through activation+dropout if there's a corresponding\n        dropout layer in dropout_list.\n        \"\"\"\n        if type(dr) is float:\n            if type(hidden_size) in [tuple, list]:\n                dr = [dr] * (len(hidden_size) + 1)\n            elif hidden_size is not None and hidden_size &gt; 0:\n                dr = [dr] * 2\n        super(Mlp, self).__init__(\n            nx=nx,\n            ny=ny,\n            hidden_size=hidden_size,\n            dr=dr,\n            activation=activation,\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass with activation followed by dropout for each layer in linear_list.\n        The number of dropout layers must match or be exactly one less than\n        the number of linear layers.\n        \"\"\"\n        # Raise an error if the number of linear layers and dropout layers do not match\n        # and do not satisfy \"number of linear layers = number of dropout layers + 1\"\n        if len(self.linear_list) != len(self.dropout_list) and (\n            len(self.linear_list) - 1\n        ) != len(self.dropout_list):\n            raise ValueError(\n                \"Mlp: linear_list and dropout_list sizes do not match. \"\n                \"They either have the same length or linear_list has exactly one more.\"\n            )\n\n        out = x\n        for i, layer in enumerate(self.linear_list):\n            out = layer(out)\n            out = self.activation(out)\n            if i &lt; len(self.dropout_list):\n                out = self.dropout_list[i](out)\n\n        return out\n</code></pre>"},{"location":"api/models/#torchhydro.models.ann.Mlp.__init__","title":"<code>__init__(self, nx, ny, hidden_size=None, dr=0.0, activation='relu')</code>  <code>special</code>","text":"<p>MLP model inherited from SimpleAnn, using activation + dropout after each layer. The final layer also goes through activation+dropout if there's a corresponding dropout layer in dropout_list.</p> Source code in <code>torchhydro/models/ann.py</code> <pre><code>def __init__(\n    self,\n    nx: int,\n    ny: int,\n    hidden_size: Union[int, tuple, list] = None,\n    dr: Union[float, tuple, list] = 0.0,\n    activation: str = \"relu\",\n):\n    \"\"\"\n    MLP model inherited from SimpleAnn, using activation + dropout after each layer.\n    The final layer also goes through activation+dropout if there's a corresponding\n    dropout layer in dropout_list.\n    \"\"\"\n    if type(dr) is float:\n        if type(hidden_size) in [tuple, list]:\n            dr = [dr] * (len(hidden_size) + 1)\n        elif hidden_size is not None and hidden_size &gt; 0:\n            dr = [dr] * 2\n    super(Mlp, self).__init__(\n        nx=nx,\n        ny=ny,\n        hidden_size=hidden_size,\n        dr=dr,\n        activation=activation,\n    )\n</code></pre>"},{"location":"api/models/#torchhydro.models.ann.Mlp.forward","title":"<code>forward(self, x)</code>","text":"<p>Forward pass with activation followed by dropout for each layer in linear_list. The number of dropout layers must match or be exactly one less than the number of linear layers.</p> Source code in <code>torchhydro/models/ann.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass with activation followed by dropout for each layer in linear_list.\n    The number of dropout layers must match or be exactly one less than\n    the number of linear layers.\n    \"\"\"\n    # Raise an error if the number of linear layers and dropout layers do not match\n    # and do not satisfy \"number of linear layers = number of dropout layers + 1\"\n    if len(self.linear_list) != len(self.dropout_list) and (\n        len(self.linear_list) - 1\n    ) != len(self.dropout_list):\n        raise ValueError(\n            \"Mlp: linear_list and dropout_list sizes do not match. \"\n            \"They either have the same length or linear_list has exactly one more.\"\n        )\n\n    out = x\n    for i, layer in enumerate(self.linear_list):\n        out = layer(out)\n        out = self.activation(out)\n        if i &lt; len(self.dropout_list):\n            out = self.dropout_list[i](out)\n\n    return out\n</code></pre>"},{"location":"api/models/#torchhydro.models.ann.SimpleAnn","title":"<code> SimpleAnn            (Module)         </code>","text":"Source code in <code>torchhydro/models/ann.py</code> <pre><code>class SimpleAnn(nn.Module):\n    def __init__(\n        self,\n        nx: int,\n        ny: int,\n        hidden_size: Union[int, tuple, list] = None,\n        dr: Union[float, tuple, list] = 0.0,\n        activation: str = \"relu\",\n    ):\n        \"\"\"\n        A simple multi-layer NN model with final linear layer\n\n        Parameters\n        ----------\n        nx\n            number of input neurons\n        ny\n            number of output neurons\n        hidden_size\n            a list/tuple which contains number of neurons in each hidden layer;\n            if int, only one hidden layer except for hidden_size=0\n        dr\n            dropout rate of layers, default is 0.0 which means no dropout;\n            here we set number of dropout layers to (number of nn layers - 1)\n        activation\n            activation function for hidden layers, default is \"relu\"\n        \"\"\"\n        super(SimpleAnn, self).__init__()\n        linear_list = nn.ModuleList()\n        dropout_list = nn.ModuleList()\n        if (\n            hidden_size is None\n            or (type(hidden_size) is int and hidden_size == 0)\n            or (type(hidden_size) in [tuple, list] and len(hidden_size) &lt; 1)\n        ):\n            linear_list.add_module(\"linear1\", nn.Linear(nx, ny))\n            if type(dr) in [tuple, list]:\n                dr = dr[0]\n            if dr &gt; 0.0:\n                dropout_list.append(nn.Dropout(dr))\n        elif type(hidden_size) is int:\n            linear_list.add_module(\"linear1\", nn.Linear(nx, hidden_size))\n            linear_list.add_module(\"linear2\", nn.Linear(hidden_size, ny))\n            if type(dr) in [tuple, list]:\n                # dropout layer do not have additional weights, so we do not name them here\n                dropout_list.append(nn.Dropout(dr[0]))\n                if len(dr) &gt; 1:\n                    dropout_list.append(nn.Dropout(dr[1]))\n            else:\n                # dr must be a float\n                dropout_list.append(nn.Dropout(dr))\n        else:\n            linear_list.add_module(\"linear1\", nn.Linear(nx, hidden_size[0]))\n            if type(dr) is float:\n                dr = [dr] * len(hidden_size)\n            elif len(dr) &gt; len(hidden_size) + 1:\n                raise ArithmeticError(\n                    \"At most, we set dropout layer for each nn layer, please check the number of dropout layers\"\n                )\n            # dropout_list.add_module(\"dropout1\", torch.nn.Dropout(dr[0]))\n            dropout_list.append(nn.Dropout(dr[0]))\n            for i in range(len(hidden_size) - 1):\n                linear_list.add_module(\n                    \"linear%d\" % (i + 1 + 1),\n                    nn.Linear(hidden_size[i], hidden_size[i + 1]),\n                )\n                dropout_list.append(\n                    nn.Dropout(dr[i + 1]),\n                )\n            linear_list.add_module(\n                \"linear%d\" % (len(hidden_size) + 1),\n                nn.Linear(hidden_size[-1], ny),\n            )\n            if len(dr) == len(linear_list):\n                # if final linear also need a dr\n                dropout_list.append(nn.Dropout(dr[-1]))\n        self.linear_list = linear_list\n        self.dropout_list = dropout_list\n        self.activation = self._get_activation(activation)\n\n    def forward(self, x):\n        for i, model in enumerate(self.linear_list):\n            if i == 0:\n                if len(self.linear_list) == 1:\n                    # final layer must be a linear layer\n                    return (\n                        model(x)\n                        if len(self.dropout_list) &lt; len(self.linear_list)\n                        else self.dropout_list[i](model(x))\n                    )\n                else:\n                    out = self.activation(self.dropout_list[i](model(x)))\n            elif i == len(self.linear_list) - 1:\n                # in final layer, no relu again\n                return (\n                    model(out)\n                    if len(self.dropout_list) &lt; len(self.linear_list)\n                    else self.dropout_list[i](model(out))\n                )\n            else:\n                out = self.activation(self.dropout_list[i](model(out)))\n\n    def _get_activation(self, name: str) -&gt; nn.Module:\n        \"\"\"a function to get activation function by name, reference from:\n        https://github.com/neuralhydrology/neuralhydrology/blob/master/neuralhydrology/modelzoo/fc.py\n\n        Parameters\n        ----------\n        name : str\n            _description_\n\n        Returns\n        -------\n        nn.Module\n            _description_\n\n        Raises\n        ------\n        NotImplementedError\n            _description_\n        \"\"\"\n        if name.lower() == \"tanh\":\n            activation = nn.Tanh()\n        elif name.lower() == \"sigmoid\":\n            activation = nn.Sigmoid()\n        elif name.lower() == \"relu\":\n            activation = nn.ReLU()\n        elif name.lower() == \"linear\":\n            activation = nn.Identity()\n        else:\n            raise NotImplementedError(\n                f\"{name} currently not supported as activation in this class\"\n            )\n        return activation\n</code></pre>"},{"location":"api/models/#torchhydro.models.ann.SimpleAnn.__init__","title":"<code>__init__(self, nx, ny, hidden_size=None, dr=0.0, activation='relu')</code>  <code>special</code>","text":"<p>A simple multi-layer NN model with final linear layer</p>"},{"location":"api/models/#torchhydro.models.ann.SimpleAnn.__init__--parameters","title":"Parameters","text":"<p>nx     number of input neurons ny     number of output neurons hidden_size     a list/tuple which contains number of neurons in each hidden layer;     if int, only one hidden layer except for hidden_size=0 dr     dropout rate of layers, default is 0.0 which means no dropout;     here we set number of dropout layers to (number of nn layers - 1) activation     activation function for hidden layers, default is \"relu\"</p> Source code in <code>torchhydro/models/ann.py</code> <pre><code>def __init__(\n    self,\n    nx: int,\n    ny: int,\n    hidden_size: Union[int, tuple, list] = None,\n    dr: Union[float, tuple, list] = 0.0,\n    activation: str = \"relu\",\n):\n    \"\"\"\n    A simple multi-layer NN model with final linear layer\n\n    Parameters\n    ----------\n    nx\n        number of input neurons\n    ny\n        number of output neurons\n    hidden_size\n        a list/tuple which contains number of neurons in each hidden layer;\n        if int, only one hidden layer except for hidden_size=0\n    dr\n        dropout rate of layers, default is 0.0 which means no dropout;\n        here we set number of dropout layers to (number of nn layers - 1)\n    activation\n        activation function for hidden layers, default is \"relu\"\n    \"\"\"\n    super(SimpleAnn, self).__init__()\n    linear_list = nn.ModuleList()\n    dropout_list = nn.ModuleList()\n    if (\n        hidden_size is None\n        or (type(hidden_size) is int and hidden_size == 0)\n        or (type(hidden_size) in [tuple, list] and len(hidden_size) &lt; 1)\n    ):\n        linear_list.add_module(\"linear1\", nn.Linear(nx, ny))\n        if type(dr) in [tuple, list]:\n            dr = dr[0]\n        if dr &gt; 0.0:\n            dropout_list.append(nn.Dropout(dr))\n    elif type(hidden_size) is int:\n        linear_list.add_module(\"linear1\", nn.Linear(nx, hidden_size))\n        linear_list.add_module(\"linear2\", nn.Linear(hidden_size, ny))\n        if type(dr) in [tuple, list]:\n            # dropout layer do not have additional weights, so we do not name them here\n            dropout_list.append(nn.Dropout(dr[0]))\n            if len(dr) &gt; 1:\n                dropout_list.append(nn.Dropout(dr[1]))\n        else:\n            # dr must be a float\n            dropout_list.append(nn.Dropout(dr))\n    else:\n        linear_list.add_module(\"linear1\", nn.Linear(nx, hidden_size[0]))\n        if type(dr) is float:\n            dr = [dr] * len(hidden_size)\n        elif len(dr) &gt; len(hidden_size) + 1:\n            raise ArithmeticError(\n                \"At most, we set dropout layer for each nn layer, please check the number of dropout layers\"\n            )\n        # dropout_list.add_module(\"dropout1\", torch.nn.Dropout(dr[0]))\n        dropout_list.append(nn.Dropout(dr[0]))\n        for i in range(len(hidden_size) - 1):\n            linear_list.add_module(\n                \"linear%d\" % (i + 1 + 1),\n                nn.Linear(hidden_size[i], hidden_size[i + 1]),\n            )\n            dropout_list.append(\n                nn.Dropout(dr[i + 1]),\n            )\n        linear_list.add_module(\n            \"linear%d\" % (len(hidden_size) + 1),\n            nn.Linear(hidden_size[-1], ny),\n        )\n        if len(dr) == len(linear_list):\n            # if final linear also need a dr\n            dropout_list.append(nn.Dropout(dr[-1]))\n    self.linear_list = linear_list\n    self.dropout_list = dropout_list\n    self.activation = self._get_activation(activation)\n</code></pre>"},{"location":"api/models/#torchhydro.models.ann.SimpleAnn.forward","title":"<code>forward(self, x)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/ann.py</code> <pre><code>def forward(self, x):\n    for i, model in enumerate(self.linear_list):\n        if i == 0:\n            if len(self.linear_list) == 1:\n                # final layer must be a linear layer\n                return (\n                    model(x)\n                    if len(self.dropout_list) &lt; len(self.linear_list)\n                    else self.dropout_list[i](model(x))\n                )\n            else:\n                out = self.activation(self.dropout_list[i](model(x)))\n        elif i == len(self.linear_list) - 1:\n            # in final layer, no relu again\n            return (\n                model(out)\n                if len(self.dropout_list) &lt; len(self.linear_list)\n                else self.dropout_list[i](model(out))\n            )\n        else:\n            out = self.activation(self.dropout_list[i](model(out)))\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits","title":"<code>crits</code>","text":"<p>Author: Wenyu Ouyang Date: 2021-12-31 11:08:29 LastEditTime: 2025-07-13 16:36:07 LastEditors: Wenyu Ouyang Description: Loss functions FilePath:       orchhydro       orchhydro\\models\\crits.py Copyright (c) 2021-2022 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/models/#torchhydro.models.crits.DynamicTaskPrior","title":"<code> DynamicTaskPrior            (Module)         </code>","text":"<p>Dynamic Task Prioritization</p> <p>This method is proposed in https://openaccess.thecvf.com/content_ECCV_2018/html/Michelle_Guo_Focus_on_the_ECCV_2018_paper.html In contrast to UW and other curriculum learning methods, where easy tasks are prioritized above difficult tasks, It shows the importance of prioritizing difficult tasks first. It automatically prioritize more difficult tasks by adaptively adjusting the mixing weight of each task's loss. Here we choose correlation as KPI. As KPI must be in [0,1], we set (corr+1)/2 as KPI</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>class DynamicTaskPrior(torch.nn.Module):\n    r\"\"\"Dynamic Task Prioritization\n\n    This method is proposed in https://openaccess.thecvf.com/content_ECCV_2018/html/Michelle_Guo_Focus_on_the_ECCV_2018_paper.html\n    In contrast to UW and other curriculum learning methods, where easy tasks are prioritized above difficult tasks,\n    It shows the importance of prioritizing difficult tasks first.\n    It automatically prioritize more difficult tasks by adaptively adjusting the mixing weight of each task's loss.\n    Here we choose correlation as KPI. As KPI must be in [0,1], we set (corr+1)/2 as KPI\n    \"\"\"\n\n    def __init__(\n        self,\n        loss_funcs: Union[torch.nn.Module, list],\n        data_gap: list = None,\n        device: list = None,\n        limit_part: list = None,\n        gamma=2,\n        alpha=0.5,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        loss_funcs\n        data_gap\n        device\n        limit_part\n        gamma\n            the example-level focusing parameter\n        alpha\n            default is 1, which means we only use the newest KPI value\n        \"\"\"\n        if data_gap is None:\n            data_gap = [0, 2]\n        if device is None:\n            device = [0]\n        super(DynamicTaskPrior, self).__init__()\n        self.loss_funcs = loss_funcs\n        self.data_gap = data_gap\n        self.device = get_the_device(device)\n        self.limit_part = limit_part\n        self.gamma = gamma\n        self.alpha = alpha\n\n    def forward(self, output, target, kpi_last=None):\n        \"\"\"\n        Parameters\n        ----------\n        output\n            model's prediction\n        target\n            observation\n\n        kpi_last\n            the KPI value of last iteration; each element for an output\n            It use a moving average KPI as the weighting coefficient: KPI_i = alpha * KPI_i + (1-alpha) * KPI_{i-1}\n\n        Returns\n        -------\n        torch.Tensor\n            multi-task loss by Dynamic Task Prioritization method\n        \"\"\"\n        n_out = target.shape[-1]\n        loss = 0\n        kpis = torch.zeros(n_out).to(self.device)\n        for k in range(n_out):\n            if self.limit_part is not None and k in self.limit_part:\n                continue\n            p0 = output[:, :, k]\n            t0 = target[:, :, k]\n            mask = t0 == t0\n            p = p0[mask]\n            t = t0[mask]\n            if self.data_gap[k] &gt; 0:\n                p, t = deal_gap_data(p0, t0, self.data_gap[k], self.device)\n            if type(self.loss_funcs) is list:\n                temp = self.loss_funcs[k](p, t)\n            else:\n                temp = self.loss_funcs(p, t)\n            # kpi must be in [0, 1], as corr's range is [-1, 1], just trans corr to  (corr+1)/2\n            kpi = (torch.corrcoef(torch.stack([p, t], 1).T)[0, 1] + 1) / 2\n            if self.alpha &lt; 1:\n                assert kpi_last is not None\n                kpi = kpi * self.alpha + kpi_last[k] * (1 - self.alpha)\n                # if we exclude kpi from the backward, it trans to a normal multi-task model\n                # kpi = kpi.detach().clone() * self.alpha + kpi_last[k] * (1 - self.alpha)\n            kpis[k] = kpi\n            # focal loss\n            fl = -((1 - kpi) ** self.gamma) * torch.log(kpi)\n            loss += torch.sum(fl * temp, -1)\n        # if kpi has grad_fn, backward will repeat. It won't work\n        return loss, kpis.detach().clone()\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.DynamicTaskPrior.__init__","title":"<code>__init__(self, loss_funcs, data_gap=None, device=None, limit_part=None, gamma=2, alpha=0.5)</code>  <code>special</code>","text":""},{"location":"api/models/#torchhydro.models.crits.DynamicTaskPrior.__init__--parameters","title":"Parameters","text":"<p>loss_funcs data_gap device limit_part gamma     the example-level focusing parameter alpha     default is 1, which means we only use the newest KPI value</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def __init__(\n    self,\n    loss_funcs: Union[torch.nn.Module, list],\n    data_gap: list = None,\n    device: list = None,\n    limit_part: list = None,\n    gamma=2,\n    alpha=0.5,\n):\n    \"\"\"\n\n    Parameters\n    ----------\n    loss_funcs\n    data_gap\n    device\n    limit_part\n    gamma\n        the example-level focusing parameter\n    alpha\n        default is 1, which means we only use the newest KPI value\n    \"\"\"\n    if data_gap is None:\n        data_gap = [0, 2]\n    if device is None:\n        device = [0]\n    super(DynamicTaskPrior, self).__init__()\n    self.loss_funcs = loss_funcs\n    self.data_gap = data_gap\n    self.device = get_the_device(device)\n    self.limit_part = limit_part\n    self.gamma = gamma\n    self.alpha = alpha\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.DynamicTaskPrior.forward","title":"<code>forward(self, output, target, kpi_last=None)</code>","text":""},{"location":"api/models/#torchhydro.models.crits.DynamicTaskPrior.forward--parameters","title":"Parameters","text":"<p>output     model's prediction target     observation</p> <p>kpi_last     the KPI value of last iteration; each element for an output     It use a moving average KPI as the weighting coefficient: KPI_i = alpha * KPI_i + (1-alpha) * KPI_{i-1}</p>"},{"location":"api/models/#torchhydro.models.crits.DynamicTaskPrior.forward--returns","title":"Returns","text":"<p>torch.Tensor     multi-task loss by Dynamic Task Prioritization method</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(self, output, target, kpi_last=None):\n    \"\"\"\n    Parameters\n    ----------\n    output\n        model's prediction\n    target\n        observation\n\n    kpi_last\n        the KPI value of last iteration; each element for an output\n        It use a moving average KPI as the weighting coefficient: KPI_i = alpha * KPI_i + (1-alpha) * KPI_{i-1}\n\n    Returns\n    -------\n    torch.Tensor\n        multi-task loss by Dynamic Task Prioritization method\n    \"\"\"\n    n_out = target.shape[-1]\n    loss = 0\n    kpis = torch.zeros(n_out).to(self.device)\n    for k in range(n_out):\n        if self.limit_part is not None and k in self.limit_part:\n            continue\n        p0 = output[:, :, k]\n        t0 = target[:, :, k]\n        mask = t0 == t0\n        p = p0[mask]\n        t = t0[mask]\n        if self.data_gap[k] &gt; 0:\n            p, t = deal_gap_data(p0, t0, self.data_gap[k], self.device)\n        if type(self.loss_funcs) is list:\n            temp = self.loss_funcs[k](p, t)\n        else:\n            temp = self.loss_funcs(p, t)\n        # kpi must be in [0, 1], as corr's range is [-1, 1], just trans corr to  (corr+1)/2\n        kpi = (torch.corrcoef(torch.stack([p, t], 1).T)[0, 1] + 1) / 2\n        if self.alpha &lt; 1:\n            assert kpi_last is not None\n            kpi = kpi * self.alpha + kpi_last[k] * (1 - self.alpha)\n            # if we exclude kpi from the backward, it trans to a normal multi-task model\n            # kpi = kpi.detach().clone() * self.alpha + kpi_last[k] * (1 - self.alpha)\n        kpis[k] = kpi\n        # focal loss\n        fl = -((1 - kpi) ** self.gamma) * torch.log(kpi)\n        loss += torch.sum(fl * temp, -1)\n    # if kpi has grad_fn, backward will repeat. It won't work\n    return loss, kpis.detach().clone()\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.FloodBaseLoss","title":"<code> FloodBaseLoss            (Module, ABC)         </code>","text":"<p>Abstract base class for flood-related loss functions.</p> <p>All flood-related loss functions should inherit from this class. The labels tensor is expected to have the flood mask as the last column.</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>class FloodBaseLoss(torch.nn.Module, ABC):\n    \"\"\"\n    Abstract base class for flood-related loss functions.\n\n    All flood-related loss functions should inherit from this class.\n    The labels tensor is expected to have the flood mask as the last column.\n    \"\"\"\n\n    def __init__(self):\n        super(FloodBaseLoss, self).__init__()\n\n    @abstractmethod\n    def compute_flood_loss(\n        self, predictions: torch.Tensor, targets: torch.Tensor, flood_mask: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the flood-aware loss.\n\n        Parameters\n        ----------\n        predictions : torch.Tensor\n            Model predictions [batch_size, seq_len, output_features]\n        targets : torch.Tensor\n            Target values [batch_size, seq_len, output_features]\n        flood_mask : torch.Tensor\n            Flood mask [batch_size, seq_len] (1 for flood, 0 for normal)\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss value\n        \"\"\"\n        pass\n\n    def forward(\n        self, predictions: torch.Tensor, targets: torch.Tensor, flood_mask: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass that calls the abstract compute_flood_loss method.\n\n        Parameters\n        ----------\n        predictions : torch.Tensor\n            Model predictions [batch_size, seq_len, output_features]\n        targets : torch.Tensor\n            Target values [batch_size, seq_len, output_features]\n        flood_mask : torch.Tensor\n            Flood mask [batch_size, seq_len] (1 for flood, 0 for normal)\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss value\n        \"\"\"\n        return self.compute_flood_loss(predictions, targets, flood_mask)\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.FloodBaseLoss.compute_flood_loss","title":"<code>compute_flood_loss(self, predictions, targets, flood_mask)</code>","text":"<p>Compute the flood-aware loss.</p>"},{"location":"api/models/#torchhydro.models.crits.FloodBaseLoss.compute_flood_loss--parameters","title":"Parameters","text":"<p>predictions : torch.Tensor     Model predictions [batch_size, seq_len, output_features] targets : torch.Tensor     Target values [batch_size, seq_len, output_features] flood_mask : torch.Tensor     Flood mask [batch_size, seq_len] (1 for flood, 0 for normal)</p>"},{"location":"api/models/#torchhydro.models.crits.FloodBaseLoss.compute_flood_loss--returns","title":"Returns","text":"<p>torch.Tensor     Computed loss value</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>@abstractmethod\ndef compute_flood_loss(\n    self, predictions: torch.Tensor, targets: torch.Tensor, flood_mask: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the flood-aware loss.\n\n    Parameters\n    ----------\n    predictions : torch.Tensor\n        Model predictions [batch_size, seq_len, output_features]\n    targets : torch.Tensor\n        Target values [batch_size, seq_len, output_features]\n    flood_mask : torch.Tensor\n        Flood mask [batch_size, seq_len] (1 for flood, 0 for normal)\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss value\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.FloodBaseLoss.forward","title":"<code>forward(self, predictions, targets, flood_mask)</code>","text":"<p>Forward pass that calls the abstract compute_flood_loss method.</p>"},{"location":"api/models/#torchhydro.models.crits.FloodBaseLoss.forward--parameters","title":"Parameters","text":"<p>predictions : torch.Tensor     Model predictions [batch_size, seq_len, output_features] targets : torch.Tensor     Target values [batch_size, seq_len, output_features] flood_mask : torch.Tensor     Flood mask [batch_size, seq_len] (1 for flood, 0 for normal)</p>"},{"location":"api/models/#torchhydro.models.crits.FloodBaseLoss.forward--returns","title":"Returns","text":"<p>torch.Tensor     Computed loss value</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(\n    self, predictions: torch.Tensor, targets: torch.Tensor, flood_mask: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass that calls the abstract compute_flood_loss method.\n\n    Parameters\n    ----------\n    predictions : torch.Tensor\n        Model predictions [batch_size, seq_len, output_features]\n    targets : torch.Tensor\n        Target values [batch_size, seq_len, output_features]\n    flood_mask : torch.Tensor\n        Flood mask [batch_size, seq_len] (1 for flood, 0 for normal)\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss value\n    \"\"\"\n    return self.compute_flood_loss(predictions, targets, flood_mask)\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.FloodLoss","title":"<code> FloodLoss            (FloodBaseLoss)         </code>","text":"Source code in <code>torchhydro/models/crits.py</code> <pre><code>class FloodLoss(FloodBaseLoss):\n    def __init__(\n        self,\n        loss_func: Union[torch.nn.Module, str] = \"MSELoss\",\n        flood_weight: float = 2.0,\n        non_flood_weight: float = 1.0,\n        flood_strategy: str = \"weight\",\n        flood_focus_factor: float = 2.0,\n        device: list = None,\n        **kwargs,\n    ):\n        \"\"\"\n        General flood-aware loss function with configurable base loss and strategy.\n\n        Parameters\n        ----------\n        loss_func : Union[torch.nn.Module, str]\n            Base loss function to use. Can be a PyTorch loss function or string name.\n            Supported strings: \"MSELoss\", \"MAELoss\", \"RMSELoss\", \"L1Loss\"\n        flood_weight : float\n            Weight multiplier for flood events when using \"weight\" strategy, default is 2.0\n        non_flood_weight : float\n            Weight multiplier for non-flood events when using \"weight\" strategy, default is 1.0\n        flood_strategy : str\n            Strategy for handling flood events:\n            - \"weight\": Apply higher weights to flood events\n            - \"focal\": Use focal loss approach based on flood event frequency\n        flood_focus_factor : float\n            Factor for focal loss when using \"focal\" strategy, default is 2.0\n        device : list\n            Device configuration, default is None (auto-detect)\n        \"\"\"\n        super(FloodLoss, self).__init__()\n        self.flood_weight = flood_weight\n        self.non_flood_weight = non_flood_weight\n        self.flood_strategy = flood_strategy\n        self.flood_focus_factor = flood_focus_factor\n        self.device = get_the_device(device if device is not None else [0])\n\n        # Initialize base loss function\n        if isinstance(loss_func, str):\n            loss_dict = {\n                # NOTE: reduction=\"none\" is important, otherwise the loss will be reduced to a scalar\n                \"MSELoss\": torch.nn.MSELoss(reduction=\"none\"),\n                \"MAELoss\": torch.nn.L1Loss(reduction=\"none\"),\n                \"L1Loss\": torch.nn.L1Loss(reduction=\"none\"),\n                \"RMSELoss\": RMSELoss(),\n                \"HybridLoss\": HybridLoss(\n                    kwargs.get(\"mae_weight\", 0.5), reduction=\"none\"\n                ),\n            }\n            if loss_func in loss_dict:\n                self.base_loss_func = loss_dict[loss_func]\n            else:\n                raise ValueError(f\"Unsupported loss function string: {loss_func}\")\n        else:\n            self.base_loss_func = loss_func\n\n    def compute_flood_loss(\n        self, predictions: torch.Tensor, targets: torch.Tensor, flood_mask: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute flood-aware loss using the specified strategy.\n\n        Parameters\n        ----------\n        predictions : torch.Tensor\n            Model predictions [batch_size, seq_len, output_features]\n        targets : torch.Tensor\n            Target values [batch_size, seq_len, output_features]\n        flood_mask : torch.Tensor\n            Flood mask [batch_size, seq_len, 1] (1 for flood, 0 for normal)\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss value\n        \"\"\"\n        # Ensure flood_mask has correct shape\n        if flood_mask.dim() == 3 and flood_mask.shape[-1] == 1:\n            flood_mask = flood_mask.squeeze(-1)  # Remove last dimension if it's 1\n\n        if self.flood_strategy == \"weight\":\n            return self._compute_weighted_loss(predictions, targets, flood_mask)\n        elif self.flood_strategy == \"focal\":\n            return self._compute_focal_loss(predictions, targets, flood_mask)\n        else:\n            raise ValueError(f\"Unsupported flood strategy: {self.flood_strategy}\")\n\n    def _compute_weighted_loss(\n        self, predictions: torch.Tensor, targets: torch.Tensor, flood_mask: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute loss with higher weights for flood events.\"\"\"\n        # Compute base loss\n        if isinstance(self.base_loss_func, RMSELoss):\n            # Special handling for RMSELoss which doesn't have reduction=\"none\"\n            base_loss = torch.pow(predictions - targets, 2)\n        else:\n            mask = ~torch.isnan(targets)\n            predictions = predictions[mask]\n            targets = targets[mask]\n            flood_mask = flood_mask[\n                mask.squeeze(-1)\n            ]  # Ensure flood_mask matches predictions/targets\n            base_loss = self.base_loss_func(predictions, targets)\n        # return base_loss\n\n        # Apply flood weights\n        weights = torch.full_like(\n            flood_mask, self.non_flood_weight, dtype=predictions.dtype\n        )\n        weights[flood_mask &gt;= 1] = self.flood_weight\n\n        # Apply weights to loss\n        if base_loss.dim() == 3:  # [batch, seq, features]\n            weighted_loss = base_loss * weights.unsqueeze(-1)\n        else:  # [batch, seq]\n            weighted_loss = base_loss * weights\n        valid_mask = ~torch.isnan(weighted_loss)\n        weighted_loss = weighted_loss[valid_mask]\n\n        if isinstance(self.base_loss_func, RMSELoss):\n            return torch.sqrt(weighted_loss.mean())\n        else:\n            return torch.mean(weighted_loss)\n\n    def _compute_focal_loss(\n        self, predictions: torch.Tensor, targets: torch.Tensor, flood_mask: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute focal loss that emphasizes flood events.\"\"\"\n        # Compute base loss\n        if isinstance(self.base_loss_func, RMSELoss):\n            base_loss = torch.pow(predictions - targets, 2)\n        else:\n            base_loss = self.base_loss_func(predictions, targets)\n\n        # Calculate flood ratio for focal weighting\n        flood_ratio = flood_mask.float().mean(dim=1, keepdim=True)  # [batch_size, 1]\n\n        # Focal weight: higher weight when flood events are rare\n        focal_weight = (1 - flood_ratio) ** self.flood_focus_factor\n\n        # Separate flood and normal events\n        flood_loss = base_loss * flood_mask.unsqueeze(-1).float()\n        normal_loss = base_loss * (1 - flood_mask.unsqueeze(-1).float())\n\n        # Apply focal weight to flood events\n        weighted_flood_loss = flood_loss * focal_weight.unsqueeze(-1)\n\n        # Combine losses\n        total_loss = weighted_flood_loss + normal_loss\n\n        if isinstance(self.base_loss_func, RMSELoss):\n            return torch.sqrt(total_loss.mean())\n        else:\n            return total_loss.mean()\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.FloodLoss.__init__","title":"<code>__init__(self, loss_func='MSELoss', flood_weight=2.0, non_flood_weight=1.0, flood_strategy='weight', flood_focus_factor=2.0, device=None, **kwargs)</code>  <code>special</code>","text":"<p>General flood-aware loss function with configurable base loss and strategy.</p>"},{"location":"api/models/#torchhydro.models.crits.FloodLoss.__init__--parameters","title":"Parameters","text":"<p>loss_func : Union[torch.nn.Module, str]     Base loss function to use. Can be a PyTorch loss function or string name.     Supported strings: \"MSELoss\", \"MAELoss\", \"RMSELoss\", \"L1Loss\" flood_weight : float     Weight multiplier for flood events when using \"weight\" strategy, default is 2.0 non_flood_weight : float     Weight multiplier for non-flood events when using \"weight\" strategy, default is 1.0 flood_strategy : str     Strategy for handling flood events:     - \"weight\": Apply higher weights to flood events     - \"focal\": Use focal loss approach based on flood event frequency flood_focus_factor : float     Factor for focal loss when using \"focal\" strategy, default is 2.0 device : list     Device configuration, default is None (auto-detect)</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def __init__(\n    self,\n    loss_func: Union[torch.nn.Module, str] = \"MSELoss\",\n    flood_weight: float = 2.0,\n    non_flood_weight: float = 1.0,\n    flood_strategy: str = \"weight\",\n    flood_focus_factor: float = 2.0,\n    device: list = None,\n    **kwargs,\n):\n    \"\"\"\n    General flood-aware loss function with configurable base loss and strategy.\n\n    Parameters\n    ----------\n    loss_func : Union[torch.nn.Module, str]\n        Base loss function to use. Can be a PyTorch loss function or string name.\n        Supported strings: \"MSELoss\", \"MAELoss\", \"RMSELoss\", \"L1Loss\"\n    flood_weight : float\n        Weight multiplier for flood events when using \"weight\" strategy, default is 2.0\n    non_flood_weight : float\n        Weight multiplier for non-flood events when using \"weight\" strategy, default is 1.0\n    flood_strategy : str\n        Strategy for handling flood events:\n        - \"weight\": Apply higher weights to flood events\n        - \"focal\": Use focal loss approach based on flood event frequency\n    flood_focus_factor : float\n        Factor for focal loss when using \"focal\" strategy, default is 2.0\n    device : list\n        Device configuration, default is None (auto-detect)\n    \"\"\"\n    super(FloodLoss, self).__init__()\n    self.flood_weight = flood_weight\n    self.non_flood_weight = non_flood_weight\n    self.flood_strategy = flood_strategy\n    self.flood_focus_factor = flood_focus_factor\n    self.device = get_the_device(device if device is not None else [0])\n\n    # Initialize base loss function\n    if isinstance(loss_func, str):\n        loss_dict = {\n            # NOTE: reduction=\"none\" is important, otherwise the loss will be reduced to a scalar\n            \"MSELoss\": torch.nn.MSELoss(reduction=\"none\"),\n            \"MAELoss\": torch.nn.L1Loss(reduction=\"none\"),\n            \"L1Loss\": torch.nn.L1Loss(reduction=\"none\"),\n            \"RMSELoss\": RMSELoss(),\n            \"HybridLoss\": HybridLoss(\n                kwargs.get(\"mae_weight\", 0.5), reduction=\"none\"\n            ),\n        }\n        if loss_func in loss_dict:\n            self.base_loss_func = loss_dict[loss_func]\n        else:\n            raise ValueError(f\"Unsupported loss function string: {loss_func}\")\n    else:\n        self.base_loss_func = loss_func\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.FloodLoss.compute_flood_loss","title":"<code>compute_flood_loss(self, predictions, targets, flood_mask)</code>","text":"<p>Compute flood-aware loss using the specified strategy.</p>"},{"location":"api/models/#torchhydro.models.crits.FloodLoss.compute_flood_loss--parameters","title":"Parameters","text":"<p>predictions : torch.Tensor     Model predictions [batch_size, seq_len, output_features] targets : torch.Tensor     Target values [batch_size, seq_len, output_features] flood_mask : torch.Tensor     Flood mask [batch_size, seq_len, 1] (1 for flood, 0 for normal)</p>"},{"location":"api/models/#torchhydro.models.crits.FloodLoss.compute_flood_loss--returns","title":"Returns","text":"<p>torch.Tensor     Computed loss value</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def compute_flood_loss(\n    self, predictions: torch.Tensor, targets: torch.Tensor, flood_mask: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute flood-aware loss using the specified strategy.\n\n    Parameters\n    ----------\n    predictions : torch.Tensor\n        Model predictions [batch_size, seq_len, output_features]\n    targets : torch.Tensor\n        Target values [batch_size, seq_len, output_features]\n    flood_mask : torch.Tensor\n        Flood mask [batch_size, seq_len, 1] (1 for flood, 0 for normal)\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss value\n    \"\"\"\n    # Ensure flood_mask has correct shape\n    if flood_mask.dim() == 3 and flood_mask.shape[-1] == 1:\n        flood_mask = flood_mask.squeeze(-1)  # Remove last dimension if it's 1\n\n    if self.flood_strategy == \"weight\":\n        return self._compute_weighted_loss(predictions, targets, flood_mask)\n    elif self.flood_strategy == \"focal\":\n        return self._compute_focal_loss(predictions, targets, flood_mask)\n    else:\n        raise ValueError(f\"Unsupported flood strategy: {self.flood_strategy}\")\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.GaussianLoss","title":"<code> GaussianLoss            (Module)         </code>","text":"Source code in <code>torchhydro/models/crits.py</code> <pre><code>class GaussianLoss(torch.nn.Module):\n    def __init__(self, mu=0, sigma=0):\n        \"\"\"Compute the negative log likelihood of Gaussian Distribution\n        From https://arxiv.org/abs/1907.00235\n        \"\"\"\n        super(GaussianLoss, self).__init__()\n        self.mu = mu\n        self.sigma = sigma\n\n    def forward(self, x: torch.Tensor):\n        loss = -tdist.Normal(self.mu, self.sigma).log_prob(x)\n        return torch.sum(loss) / (loss.size(0) * loss.size(1))\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.GaussianLoss.__init__","title":"<code>__init__(self, mu=0, sigma=0)</code>  <code>special</code>","text":"<p>Compute the negative log likelihood of Gaussian Distribution From https://arxiv.org/abs/1907.00235</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def __init__(self, mu=0, sigma=0):\n    \"\"\"Compute the negative log likelihood of Gaussian Distribution\n    From https://arxiv.org/abs/1907.00235\n    \"\"\"\n    super(GaussianLoss, self).__init__()\n    self.mu = mu\n    self.sigma = sigma\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.GaussianLoss.forward","title":"<code>forward(self, x)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(self, x: torch.Tensor):\n    loss = -tdist.Normal(self.mu, self.sigma).log_prob(x)\n    return torch.sum(loss) / (loss.size(0) * loss.size(1))\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.HybridFloodloss","title":"<code> HybridFloodloss            (FloodBaseLoss)         </code>","text":"Source code in <code>torchhydro/models/crits.py</code> <pre><code>class HybridFloodloss(FloodBaseLoss):\n    def __init__(self, mae_weight=0.5):\n        \"\"\"\n        Hybrid Flood Loss: PES loss + mae_weight \u00d7 MAE with flood weighting\n\n        Combines PES loss (MSE \u00d7 sigmoid(MSE)) with Mean Absolute Error,\n        applying flood weighting to the loss.\n\n        The difference from FloodLoss is that this class filter flood events first then calculate loss,\n        because Hybrid does sigmoid on MSE, when the non-flood-weight is 0, which means we do not want to\n        calculate loss on non-flood events, so we need to filter them out first.\n\n        Parameters\n        ----------\n        mae_weight : float\n            Weight for the MAE component, default is 0.5\n        flood_weight : float\n            Weight multiplier for flood events, default is 2.0\n        \"\"\"\n        super(HybridFloodloss, self).__init__()\n        self.mae_weight = mae_weight\n\n    def compute_flood_loss(\n        self, predictions: torch.Tensor, targets: torch.Tensor, flood_mask: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute flood-aware loss using the specified strategy.\n\n        Parameters\n        ----------\n        predictions : torch.Tensor\n            Model predictions [batch_size, seq_len, output_features]\n        targets : torch.Tensor\n            Target values [batch_size, seq_len, output_features]\n        flood_mask : torch.Tensor\n            Flood mask [batch_size, seq_len, 1] (1 for flood, 0 for normal)\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss value\n        \"\"\"\n        boolean_mask = flood_mask.to(torch.bool)\n        predictions = predictions[boolean_mask]\n        targets = targets[boolean_mask]\n\n        base_loss_func = HybridLoss(self.mae_weight)\n        return base_loss_func(predictions, targets)\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.HybridFloodloss.__init__","title":"<code>__init__(self, mae_weight=0.5)</code>  <code>special</code>","text":"<p>Hybrid Flood Loss: PES loss + mae_weight \u00d7 MAE with flood weighting</p> <p>Combines PES loss (MSE \u00d7 sigmoid(MSE)) with Mean Absolute Error, applying flood weighting to the loss.</p> <p>The difference from FloodLoss is that this class filter flood events first then calculate loss, because Hybrid does sigmoid on MSE, when the non-flood-weight is 0, which means we do not want to calculate loss on non-flood events, so we need to filter them out first.</p>"},{"location":"api/models/#torchhydro.models.crits.HybridFloodloss.__init__--parameters","title":"Parameters","text":"<p>mae_weight : float     Weight for the MAE component, default is 0.5 flood_weight : float     Weight multiplier for flood events, default is 2.0</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def __init__(self, mae_weight=0.5):\n    \"\"\"\n    Hybrid Flood Loss: PES loss + mae_weight \u00d7 MAE with flood weighting\n\n    Combines PES loss (MSE \u00d7 sigmoid(MSE)) with Mean Absolute Error,\n    applying flood weighting to the loss.\n\n    The difference from FloodLoss is that this class filter flood events first then calculate loss,\n    because Hybrid does sigmoid on MSE, when the non-flood-weight is 0, which means we do not want to\n    calculate loss on non-flood events, so we need to filter them out first.\n\n    Parameters\n    ----------\n    mae_weight : float\n        Weight for the MAE component, default is 0.5\n    flood_weight : float\n        Weight multiplier for flood events, default is 2.0\n    \"\"\"\n    super(HybridFloodloss, self).__init__()\n    self.mae_weight = mae_weight\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.HybridFloodloss.compute_flood_loss","title":"<code>compute_flood_loss(self, predictions, targets, flood_mask)</code>","text":"<p>Compute flood-aware loss using the specified strategy.</p>"},{"location":"api/models/#torchhydro.models.crits.HybridFloodloss.compute_flood_loss--parameters","title":"Parameters","text":"<p>predictions : torch.Tensor     Model predictions [batch_size, seq_len, output_features] targets : torch.Tensor     Target values [batch_size, seq_len, output_features] flood_mask : torch.Tensor     Flood mask [batch_size, seq_len, 1] (1 for flood, 0 for normal)</p>"},{"location":"api/models/#torchhydro.models.crits.HybridFloodloss.compute_flood_loss--returns","title":"Returns","text":"<p>torch.Tensor     Computed loss value</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def compute_flood_loss(\n    self, predictions: torch.Tensor, targets: torch.Tensor, flood_mask: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute flood-aware loss using the specified strategy.\n\n    Parameters\n    ----------\n    predictions : torch.Tensor\n        Model predictions [batch_size, seq_len, output_features]\n    targets : torch.Tensor\n        Target values [batch_size, seq_len, output_features]\n    flood_mask : torch.Tensor\n        Flood mask [batch_size, seq_len, 1] (1 for flood, 0 for normal)\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss value\n    \"\"\"\n    boolean_mask = flood_mask.to(torch.bool)\n    predictions = predictions[boolean_mask]\n    targets = targets[boolean_mask]\n\n    base_loss_func = HybridLoss(self.mae_weight)\n    return base_loss_func(predictions, targets)\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.HybridLoss","title":"<code> HybridLoss            (Module)         </code>","text":"Source code in <code>torchhydro/models/crits.py</code> <pre><code>class HybridLoss(torch.nn.Module):\n    def __init__(self, mae_weight: float = 0.5, reduction: str = \"mean\"):\n        \"\"\"\n        Hybrid Loss: PES loss + mae_weight \u00d7 MAE\n\n        Combines PES loss (MSE \u00d7 sigmoid(MSE)) with Mean Absolute Error.\n\n        Parameters\n        ----------\n        mae_weight : float\n            Weight for the MAE component, default is 0.5\n        reduction : str\n            Reduction method for the loss, default is \"mean\". Can be \"mean\" or \"none\".\n            If \"none\", returns the loss without reduction.\n        \"\"\"\n        super(HybridLoss, self).__init__()\n        self.pes_loss = PESLoss()\n        self.mae = MAELoss(reduction=reduction)\n        self.mae_weight = mae_weight\n        self.reduction = reduction\n\n    def forward(self, output: torch.Tensor, target: torch.Tensor):\n        pes = self.pes_loss(output, target)\n        mae = self.mae(output, target)\n        if self.reduction == \"none\":\n            return pes + self.mae_weight * mae\n        elif self.reduction == \"mean\":\n            loss = pes + self.mae_weight * mae\n            valid_mask = ~torch.isnan(loss)\n            return torch.mean(loss[valid_mask])\n        else:\n            raise ValueError(\n                f\"Unsupported reduction method: {self.reduction}. Use 'mean' or 'none'.\"\n            )\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.HybridLoss.__init__","title":"<code>__init__(self, mae_weight=0.5, reduction='mean')</code>  <code>special</code>","text":"<p>Hybrid Loss: PES loss + mae_weight \u00d7 MAE</p> <p>Combines PES loss (MSE \u00d7 sigmoid(MSE)) with Mean Absolute Error.</p>"},{"location":"api/models/#torchhydro.models.crits.HybridLoss.__init__--parameters","title":"Parameters","text":"<p>mae_weight : float     Weight for the MAE component, default is 0.5 reduction : str     Reduction method for the loss, default is \"mean\". Can be \"mean\" or \"none\".     If \"none\", returns the loss without reduction.</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def __init__(self, mae_weight: float = 0.5, reduction: str = \"mean\"):\n    \"\"\"\n    Hybrid Loss: PES loss + mae_weight \u00d7 MAE\n\n    Combines PES loss (MSE \u00d7 sigmoid(MSE)) with Mean Absolute Error.\n\n    Parameters\n    ----------\n    mae_weight : float\n        Weight for the MAE component, default is 0.5\n    reduction : str\n        Reduction method for the loss, default is \"mean\". Can be \"mean\" or \"none\".\n        If \"none\", returns the loss without reduction.\n    \"\"\"\n    super(HybridLoss, self).__init__()\n    self.pes_loss = PESLoss()\n    self.mae = MAELoss(reduction=reduction)\n    self.mae_weight = mae_weight\n    self.reduction = reduction\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.HybridLoss.forward","title":"<code>forward(self, output, target)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(self, output: torch.Tensor, target: torch.Tensor):\n    pes = self.pes_loss(output, target)\n    mae = self.mae(output, target)\n    if self.reduction == \"none\":\n        return pes + self.mae_weight * mae\n    elif self.reduction == \"mean\":\n        loss = pes + self.mae_weight * mae\n        valid_mask = ~torch.isnan(loss)\n        return torch.mean(loss[valid_mask])\n    else:\n        raise ValueError(\n            f\"Unsupported reduction method: {self.reduction}. Use 'mean' or 'none'.\"\n        )\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.MAELoss","title":"<code> MAELoss            (Module)         </code>","text":"Source code in <code>torchhydro/models/crits.py</code> <pre><code>class MAELoss(torch.nn.Module):\n    def __init__(self, reduction: str = \"mean\"):\n        super().__init__()\n        self.reduction = reduction\n\n    def forward(self, output: torch.Tensor, target: torch.Tensor):\n        # Create a mask to filter out NaN values\n        mask = ~torch.isnan(target)\n\n        # Apply the mask to both target and output\n        target = target[mask]\n        output = output[mask]\n\n        # Calculate MAE for the non-NaN values\n        if self.reduction == \"mean\":  # Return mean MAe\n            return torch.mean(torch.abs(target - output))\n        elif self.reduction == \"none\":\n            return torch.abs(target - output)\n        else:\n            raise ValueError(\n                \"Reduction must be 'mean' or 'none', got {}\".format(self.reduction)\n            )\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.MAELoss.forward","title":"<code>forward(self, output, target)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(self, output: torch.Tensor, target: torch.Tensor):\n    # Create a mask to filter out NaN values\n    mask = ~torch.isnan(target)\n\n    # Apply the mask to both target and output\n    target = target[mask]\n    output = output[mask]\n\n    # Calculate MAE for the non-NaN values\n    if self.reduction == \"mean\":  # Return mean MAe\n        return torch.mean(torch.abs(target - output))\n    elif self.reduction == \"none\":\n        return torch.abs(target - output)\n    else:\n        raise ValueError(\n            \"Reduction must be 'mean' or 'none', got {}\".format(self.reduction)\n        )\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.MAPELoss","title":"<code> MAPELoss            (Module)         </code>","text":"<p>Returns MAPE using: target -&gt; True y output -&gt; Predtion by model</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>class MAPELoss(torch.nn.Module):\n    \"\"\"\n    Returns MAPE using:\n    target -&gt; True y\n    output -&gt; Predtion by model\n    \"\"\"\n\n    def __init__(self, variance_penalty=0.0):\n        super().__init__()\n        self.variance_penalty = variance_penalty\n\n    def forward(self, output: torch.Tensor, target: torch.Tensor):\n        if len(output) &gt; 1:\n            return torch.mean(\n                torch.abs(torch.sub(target, output) / target)\n            ) + self.variance_penalty * torch.std(torch.sub(target, output))\n        else:\n            return torch.mean(torch.abs(torch.sub(target, output) / target))\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.MAPELoss.forward","title":"<code>forward(self, output, target)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(self, output: torch.Tensor, target: torch.Tensor):\n    if len(output) &gt; 1:\n        return torch.mean(\n            torch.abs(torch.sub(target, output) / target)\n        ) + self.variance_penalty * torch.std(torch.sub(target, output))\n    else:\n        return torch.mean(torch.abs(torch.sub(target, output) / target))\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.MASELoss","title":"<code> MASELoss            (Module)         </code>","text":"Source code in <code>torchhydro/models/crits.py</code> <pre><code>class MASELoss(torch.nn.Module):\n    def __init__(self, baseline_method):\n        \"\"\"\n        This implements the MASE loss function (e.g. MAE_MODEL/MAE_NAIEVE)\n        \"\"\"\n        super(MASELoss, self).__init__()\n        self.method_dict = {\n            \"mean\": lambda x, y: torch.mean(x, 1).unsqueeze(1).repeat(1, y[1], 1)\n        }\n        self.baseline_method = self.method_dict[baseline_method]\n\n    def forward(\n        self, target: torch.Tensor, output: torch.Tensor, train_data: torch.Tensor, m=1\n    ) -&gt; torch.Tensor:\n        # Ugh why can't all tensors have batch size... Fixes for modern\n        if len(train_data.shape) &lt; 3:\n            train_data = train_data.unsqueeze(0)\n        if m == 1 and len(target.shape) == 1:\n            output = output.unsqueeze(0)\n            output = output.unsqueeze(2)\n            target = target.unsqueeze(0)\n            target = target.unsqueeze(2)\n        if len(target.shape) == 2:\n            output = output.unsqueeze(0)\n            target = target.unsqueeze(0)\n        result_baseline = self.baseline_method(train_data, output.shape)\n        MAE = torch.nn.L1Loss()\n        mae2 = MAE(output, target)\n        mase4 = MAE(result_baseline, target)\n        # Prevent divison by zero/loss exploding\n        if mase4 &lt; 0.001:\n            mase4 = 0.001\n        return mae2 / mase4\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.MASELoss.__init__","title":"<code>__init__(self, baseline_method)</code>  <code>special</code>","text":"<p>This implements the MASE loss function (e.g. MAE_MODEL/MAE_NAIEVE)</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def __init__(self, baseline_method):\n    \"\"\"\n    This implements the MASE loss function (e.g. MAE_MODEL/MAE_NAIEVE)\n    \"\"\"\n    super(MASELoss, self).__init__()\n    self.method_dict = {\n        \"mean\": lambda x, y: torch.mean(x, 1).unsqueeze(1).repeat(1, y[1], 1)\n    }\n    self.baseline_method = self.method_dict[baseline_method]\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.MASELoss.forward","title":"<code>forward(self, target, output, train_data, m=1)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(\n    self, target: torch.Tensor, output: torch.Tensor, train_data: torch.Tensor, m=1\n) -&gt; torch.Tensor:\n    # Ugh why can't all tensors have batch size... Fixes for modern\n    if len(train_data.shape) &lt; 3:\n        train_data = train_data.unsqueeze(0)\n    if m == 1 and len(target.shape) == 1:\n        output = output.unsqueeze(0)\n        output = output.unsqueeze(2)\n        target = target.unsqueeze(0)\n        target = target.unsqueeze(2)\n    if len(target.shape) == 2:\n        output = output.unsqueeze(0)\n        target = target.unsqueeze(0)\n    result_baseline = self.baseline_method(train_data, output.shape)\n    MAE = torch.nn.L1Loss()\n    mae2 = MAE(output, target)\n    mase4 = MAE(result_baseline, target)\n    # Prevent divison by zero/loss exploding\n    if mase4 &lt; 0.001:\n        mase4 = 0.001\n    return mae2 / mase4\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.MultiOutLoss","title":"<code> MultiOutLoss            (Module)         </code>","text":"Source code in <code>torchhydro/models/crits.py</code> <pre><code>class MultiOutLoss(torch.nn.Module):\n    def __init__(\n        self,\n        loss_funcs: Union[torch.nn.Module, list],\n        data_gap: list = None,\n        device: list = None,\n        limit_part: list = None,\n        item_weight: list = None,\n    ):\n        \"\"\"\n        Loss function for multiple output\n\n        Parameters\n        ----------\n        loss_funcs\n            The loss functions for each output\n        data_gap\n            It belongs to the feature dim.\n            If 1, then the corresponding value is uniformly-spaced with NaN values filling the gap;\n            in addition, the first non-nan value means the aggregated value of the following interval,\n            for example, in [5, nan, nan, nan], 5 means all four data's sum, although the next 3 values are nan\n            hence the calculation is a little different;\n            if 2, the first non-nan value means the average value of the following interval,\n            for example, in [5, nan, nan, nan], 5 means all four data's mean value;\n            default is [0, 2]\n        device\n            the number of device: -1 -&gt; \"cpu\" or \"cuda:x\" (x is 0, 1 or ...)\n        limit_part\n            when transfer learning, we may ignore some part;\n            the default is None, which means no ignorance;\n            other choices are list, such as [0], [0, 1] or [1,2,..];\n            0 means the first variable;\n            tensor is [seq, time, var] or [time, seq, var]\n        item_weight\n            use different weight for each item's loss;\n            for example, the default values [0.5, 0.5] means 0.5 * loss1 + 0.5 * loss2\n        \"\"\"\n        if data_gap is None:\n            data_gap = [0, 2]\n        if device is None:\n            device = [0]\n        if item_weight is None:\n            item_weight = [0.5, 0.5]\n        super(MultiOutLoss, self).__init__()\n        self.loss_funcs = loss_funcs\n        self.data_gap = data_gap\n        self.device = get_the_device(device)\n        self.limit_part = limit_part\n        self.item_weight = item_weight\n\n    def forward(self, output: Tensor, target: Tensor):\n        \"\"\"\n        Calculate the sum of losses for different variables\n\n        When there are NaN values in observation, we will perform a \"reduce\" operation on prediction.\n        For example, pred = [0,1,2,3,4], obs=[5, nan, nan, 6, nan]; the \"reduce\" is sum;\n        then, pred_sum = [0+1+2, 3+4], obs_sum=[5,6], loss = loss_func(pred_sum, obs_sum).\n        Notice: when \"sum\", actually final index is not chosen,\n        because the whole observation may be [5, nan, nan, 6, nan, nan, 7, nan, nan], 6 means sum of three elements.\n        Just as the rho is 5, the final one is not chosen\n\n\n        Parameters\n        ----------\n        output\n            the prediction tensor; 3-dims are time sequence, batch and feature, respectively\n        target\n            the observation tensor\n\n        Returns\n        -------\n        Tensor\n            Whole loss\n        \"\"\"\n        n_out = target.shape[-1]\n        loss = 0\n        for k in range(n_out):\n            if self.limit_part is not None and k in self.limit_part:\n                continue\n            p0 = output[:, :, k]\n            t0 = target[:, :, k]\n            mask = t0 == t0\n            p = p0[mask]\n            t = t0[mask]\n            if self.data_gap[k] &gt; 0:\n                p, t = deal_gap_data(p0, t0, self.data_gap[k], self.device)\n            if type(self.loss_funcs) is list:\n                temp = self.item_weight[k] * self.loss_funcs[k](p, t)\n            else:\n                temp = self.item_weight[k] * self.loss_funcs(p, t)\n            # sum of all k-th loss\n            if torch.isnan(temp).any():\n                continue\n            loss = loss + temp\n        return loss\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.MultiOutLoss.__init__","title":"<code>__init__(self, loss_funcs, data_gap=None, device=None, limit_part=None, item_weight=None)</code>  <code>special</code>","text":"<p>Loss function for multiple output</p>"},{"location":"api/models/#torchhydro.models.crits.MultiOutLoss.__init__--parameters","title":"Parameters","text":"<p>loss_funcs     The loss functions for each output data_gap     It belongs to the feature dim.     If 1, then the corresponding value is uniformly-spaced with NaN values filling the gap;     in addition, the first non-nan value means the aggregated value of the following interval,     for example, in [5, nan, nan, nan], 5 means all four data's sum, although the next 3 values are nan     hence the calculation is a little different;     if 2, the first non-nan value means the average value of the following interval,     for example, in [5, nan, nan, nan], 5 means all four data's mean value;     default is [0, 2] device     the number of device: -1 -&gt; \"cpu\" or \"cuda:x\" (x is 0, 1 or ...) limit_part     when transfer learning, we may ignore some part;     the default is None, which means no ignorance;     other choices are list, such as [0], [0, 1] or [1,2,..];     0 means the first variable;     tensor is [seq, time, var] or [time, seq, var] item_weight     use different weight for each item's loss;     for example, the default values [0.5, 0.5] means 0.5 * loss1 + 0.5 * loss2</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def __init__(\n    self,\n    loss_funcs: Union[torch.nn.Module, list],\n    data_gap: list = None,\n    device: list = None,\n    limit_part: list = None,\n    item_weight: list = None,\n):\n    \"\"\"\n    Loss function for multiple output\n\n    Parameters\n    ----------\n    loss_funcs\n        The loss functions for each output\n    data_gap\n        It belongs to the feature dim.\n        If 1, then the corresponding value is uniformly-spaced with NaN values filling the gap;\n        in addition, the first non-nan value means the aggregated value of the following interval,\n        for example, in [5, nan, nan, nan], 5 means all four data's sum, although the next 3 values are nan\n        hence the calculation is a little different;\n        if 2, the first non-nan value means the average value of the following interval,\n        for example, in [5, nan, nan, nan], 5 means all four data's mean value;\n        default is [0, 2]\n    device\n        the number of device: -1 -&gt; \"cpu\" or \"cuda:x\" (x is 0, 1 or ...)\n    limit_part\n        when transfer learning, we may ignore some part;\n        the default is None, which means no ignorance;\n        other choices are list, such as [0], [0, 1] or [1,2,..];\n        0 means the first variable;\n        tensor is [seq, time, var] or [time, seq, var]\n    item_weight\n        use different weight for each item's loss;\n        for example, the default values [0.5, 0.5] means 0.5 * loss1 + 0.5 * loss2\n    \"\"\"\n    if data_gap is None:\n        data_gap = [0, 2]\n    if device is None:\n        device = [0]\n    if item_weight is None:\n        item_weight = [0.5, 0.5]\n    super(MultiOutLoss, self).__init__()\n    self.loss_funcs = loss_funcs\n    self.data_gap = data_gap\n    self.device = get_the_device(device)\n    self.limit_part = limit_part\n    self.item_weight = item_weight\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.MultiOutLoss.forward","title":"<code>forward(self, output, target)</code>","text":"<p>Calculate the sum of losses for different variables</p> <p>When there are NaN values in observation, we will perform a \"reduce\" operation on prediction. For example, pred = [0,1,2,3,4], obs=[5, nan, nan, 6, nan]; the \"reduce\" is sum; then, pred_sum = [0+1+2, 3+4], obs_sum=[5,6], loss = loss_func(pred_sum, obs_sum). Notice: when \"sum\", actually final index is not chosen, because the whole observation may be [5, nan, nan, 6, nan, nan, 7, nan, nan], 6 means sum of three elements. Just as the rho is 5, the final one is not chosen</p>"},{"location":"api/models/#torchhydro.models.crits.MultiOutLoss.forward--parameters","title":"Parameters","text":"<p>output     the prediction tensor; 3-dims are time sequence, batch and feature, respectively target     the observation tensor</p>"},{"location":"api/models/#torchhydro.models.crits.MultiOutLoss.forward--returns","title":"Returns","text":"<p>Tensor     Whole loss</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(self, output: Tensor, target: Tensor):\n    \"\"\"\n    Calculate the sum of losses for different variables\n\n    When there are NaN values in observation, we will perform a \"reduce\" operation on prediction.\n    For example, pred = [0,1,2,3,4], obs=[5, nan, nan, 6, nan]; the \"reduce\" is sum;\n    then, pred_sum = [0+1+2, 3+4], obs_sum=[5,6], loss = loss_func(pred_sum, obs_sum).\n    Notice: when \"sum\", actually final index is not chosen,\n    because the whole observation may be [5, nan, nan, 6, nan, nan, 7, nan, nan], 6 means sum of three elements.\n    Just as the rho is 5, the final one is not chosen\n\n\n    Parameters\n    ----------\n    output\n        the prediction tensor; 3-dims are time sequence, batch and feature, respectively\n    target\n        the observation tensor\n\n    Returns\n    -------\n    Tensor\n        Whole loss\n    \"\"\"\n    n_out = target.shape[-1]\n    loss = 0\n    for k in range(n_out):\n        if self.limit_part is not None and k in self.limit_part:\n            continue\n        p0 = output[:, :, k]\n        t0 = target[:, :, k]\n        mask = t0 == t0\n        p = p0[mask]\n        t = t0[mask]\n        if self.data_gap[k] &gt; 0:\n            p, t = deal_gap_data(p0, t0, self.data_gap[k], self.device)\n        if type(self.loss_funcs) is list:\n            temp = self.item_weight[k] * self.loss_funcs[k](p, t)\n        else:\n            temp = self.item_weight[k] * self.loss_funcs(p, t)\n        # sum of all k-th loss\n        if torch.isnan(temp).any():\n            continue\n        loss = loss + temp\n    return loss\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.MultiOutWaterBalanceLoss","title":"<code> MultiOutWaterBalanceLoss            (Module)         </code>","text":"Source code in <code>torchhydro/models/crits.py</code> <pre><code>class MultiOutWaterBalanceLoss(torch.nn.Module):\n    def __init__(\n        self,\n        loss_funcs: Union[torch.nn.Module, list],\n        data_gap: list = None,\n        device: list = None,\n        limit_part: list = None,\n        item_weight: list = None,\n        alpha=0.5,\n        beta=0.0,\n        wb_loss_func=None,\n        means=None,\n        stds=None,\n    ):\n        \"\"\"\n        Loss function for multiple output considering water balance\n\n        loss = alpha * water_balance_loss + (1-alpha) * mtl_loss\n\n        This loss function is only for p, q, et now\n        we use the difference between p_obs_mean-q_obs_mean-et_obs_mean and p_pred_mean-q_pred_mean-et_pred_mean as water balance loss\n        which is the difference between (q_obs_mean + et_obs_mean) and (q_pred_mean + et_pred_mean)\n\n        Parameters\n        ----------\n        loss_funcs\n            The loss functions for each output\n        data_gap\n            It belongs to the feature dim.\n            If 1, then the corresponding value is uniformly-spaced with NaN values filling the gap;\n            in addition, the first non-nan value means the aggregated value of the following interval,\n            for example, in [5, nan, nan, nan], 5 means all four data's sum, although the next 3 values are nan\n            hence the calculation is a little different;\n            if 2, the first non-nan value means the average value of the following interval,\n            for example, in [5, nan, nan, nan], 5 means all four data's mean value;\n            default is [0, 2]\n        device\n            the number of device: -1 -&gt; \"cpu\" or \"cuda:x\" (x is 0, 1 or ...)\n        limit_part\n            when transfer learning, we may ignore some part;\n            the default is None, which means no ignorance;\n            other choices are list, such as [0], [0, 1] or [1,2,..];\n            0 means the first variable;\n            tensor is [seq, time, var] or [time, seq, var]\n        item_weight\n            use different weight for each item's loss;\n            for example, the default values [0.5, 0.5] means 0.5 * loss1 + 0.5 * loss2\n        alpha\n            the weight of the water-balance item's loss\n        beta\n            the weight of real water-balance item's loss, et_mean/p_mean + q_mean/p_mean = 1 can be a loss.\n            It is not strictly correct as training batch only have about one year data, but still could be a constraint\n        wb_loss_func\n            the loss function for water balance item, by default it is None, which means we use function in loss_funcs\n        \"\"\"\n        if data_gap is None:\n            data_gap = [0, 2]\n        if device is None:\n            device = [0]\n        if item_weight is None:\n            item_weight = [0.5, 0.5]\n        super(MultiOutWaterBalanceLoss, self).__init__()\n        self.loss_funcs = loss_funcs\n        self.data_gap = data_gap\n        self.device = get_the_device(device)\n        self.limit_part = limit_part\n        self.item_weight = item_weight\n        self.alpha = alpha\n        self.beta = beta\n        self.wb_loss_func = wb_loss_func\n        self.means = means\n        self.stds = stds\n\n    def forward(self, output: Tensor, target: Tensor):\n        \"\"\"\n        Calculate the sum of losses for different variables and water-balance loss\n\n        When there are NaN values in observation, we will perform a \"reduce\" operation on prediction.\n        For example, pred = [0,1,2,3,4], obs=[5, nan, nan, 6, nan]; the \"reduce\" is sum;\n        then, pred_sum = [0+1+2, 3+4], obs_sum=[5,6], loss = loss_func(pred_sum, obs_sum).\n        Notice: when \"sum\", actually final index is not chosen,\n        because the whole observation may be [5, nan, nan, 6, nan, nan, 7, nan, nan], 6 means sum of three elements.\n        Just as the rho is 5, the final one is not chosen\n\n\n        Parameters\n        ----------\n        output\n            the prediction tensor; 3-dims are time sequence, batch and feature, respectively\n        target\n            the observation tensor\n\n        Returns\n        -------\n        Tensor\n            Whole loss\n        \"\"\"\n        n_out = target.shape[-1]\n        loss = 0\n        p_means = []\n        t_means = []\n        all_means = self.means\n        all_stds = self.stds\n        for k in range(n_out):\n            if self.limit_part is not None and k in self.limit_part:\n                continue\n            p0 = output[:, :, k]\n            t0 = target[:, :, k]\n            # for water balance loss\n            if all_means is not None:\n                # denormalize for q and et\n                p1 = p0 * all_stds[k] + all_means[k]\n                t1 = t0 * all_stds[k] + all_means[k]\n                p2 = (10**p1 - 0.1) ** 2\n                t2 = (10**t1 - 0.1) ** 2\n                p_mean = torch.nanmean(p2, dim=0)\n                t_mean = torch.nanmean(t2, dim=0)\n            else:\n                p_mean = torch.nanmean(p0, dim=0)\n                t_mean = torch.nanmean(t0, dim=0)\n            p_means.append(p_mean)\n            t_means.append(t_mean)\n            # for mtl normal loss\n            mask = t0 == t0\n            p = p0[mask]\n            t = t0[mask]\n            if self.data_gap[k] &gt; 0:\n                p, t = deal_gap_data(p0, t0, self.data_gap[k], self.device)\n            if type(self.loss_funcs) is list:\n                temp = self.item_weight[k] * self.loss_funcs[k](p, t)\n            else:\n                temp = self.item_weight[k] * self.loss_funcs(p, t)\n            # sum of all k-th loss\n            loss = loss + temp\n        # water balance loss\n        p_mean_q_plus_et = torch.sum(torch.stack(p_means), dim=0)\n        t_mean_q_plus_et = torch.sum(torch.stack(t_means), dim=0)\n        wb_ones = torch.ones_like(t_mean_q_plus_et)\n        if self.wb_loss_func is None:\n            if type(self.loss_funcs) is list:\n                # if wb_loss_func is None, we use the first loss function in loss_funcs\n                wb_loss = self.loss_funcs[0](p_mean_q_plus_et, t_mean_q_plus_et)\n                wb_1loss = self.loss_funcs[0](p_mean_q_plus_et, wb_ones)\n            else:\n                wb_loss = self.loss_funcs(p_mean_q_plus_et, t_mean_q_plus_et)\n                wb_1loss = self.loss_funcs(p_mean_q_plus_et, wb_ones)\n        else:\n            wb_loss = self.wb_loss_func(p_mean_q_plus_et, t_mean_q_plus_et)\n            wb_1loss = self.wb_loss_func(p_mean_q_plus_et, wb_ones)\n        return (\n            self.alpha * wb_loss\n            + (1 - self.alpha - self.beta) * loss\n            + self.beta * wb_1loss\n        )\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.MultiOutWaterBalanceLoss.__init__","title":"<code>__init__(self, loss_funcs, data_gap=None, device=None, limit_part=None, item_weight=None, alpha=0.5, beta=0.0, wb_loss_func=None, means=None, stds=None)</code>  <code>special</code>","text":"<p>Loss function for multiple output considering water balance</p> <p>loss = alpha * water_balance_loss + (1-alpha) * mtl_loss</p> <p>This loss function is only for p, q, et now we use the difference between p_obs_mean-q_obs_mean-et_obs_mean and p_pred_mean-q_pred_mean-et_pred_mean as water balance loss which is the difference between (q_obs_mean + et_obs_mean) and (q_pred_mean + et_pred_mean)</p>"},{"location":"api/models/#torchhydro.models.crits.MultiOutWaterBalanceLoss.__init__--parameters","title":"Parameters","text":"<p>loss_funcs     The loss functions for each output data_gap     It belongs to the feature dim.     If 1, then the corresponding value is uniformly-spaced with NaN values filling the gap;     in addition, the first non-nan value means the aggregated value of the following interval,     for example, in [5, nan, nan, nan], 5 means all four data's sum, although the next 3 values are nan     hence the calculation is a little different;     if 2, the first non-nan value means the average value of the following interval,     for example, in [5, nan, nan, nan], 5 means all four data's mean value;     default is [0, 2] device     the number of device: -1 -&gt; \"cpu\" or \"cuda:x\" (x is 0, 1 or ...) limit_part     when transfer learning, we may ignore some part;     the default is None, which means no ignorance;     other choices are list, such as [0], [0, 1] or [1,2,..];     0 means the first variable;     tensor is [seq, time, var] or [time, seq, var] item_weight     use different weight for each item's loss;     for example, the default values [0.5, 0.5] means 0.5 * loss1 + 0.5 * loss2 alpha     the weight of the water-balance item's loss beta     the weight of real water-balance item's loss, et_mean/p_mean + q_mean/p_mean = 1 can be a loss.     It is not strictly correct as training batch only have about one year data, but still could be a constraint wb_loss_func     the loss function for water balance item, by default it is None, which means we use function in loss_funcs</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def __init__(\n    self,\n    loss_funcs: Union[torch.nn.Module, list],\n    data_gap: list = None,\n    device: list = None,\n    limit_part: list = None,\n    item_weight: list = None,\n    alpha=0.5,\n    beta=0.0,\n    wb_loss_func=None,\n    means=None,\n    stds=None,\n):\n    \"\"\"\n    Loss function for multiple output considering water balance\n\n    loss = alpha * water_balance_loss + (1-alpha) * mtl_loss\n\n    This loss function is only for p, q, et now\n    we use the difference between p_obs_mean-q_obs_mean-et_obs_mean and p_pred_mean-q_pred_mean-et_pred_mean as water balance loss\n    which is the difference between (q_obs_mean + et_obs_mean) and (q_pred_mean + et_pred_mean)\n\n    Parameters\n    ----------\n    loss_funcs\n        The loss functions for each output\n    data_gap\n        It belongs to the feature dim.\n        If 1, then the corresponding value is uniformly-spaced with NaN values filling the gap;\n        in addition, the first non-nan value means the aggregated value of the following interval,\n        for example, in [5, nan, nan, nan], 5 means all four data's sum, although the next 3 values are nan\n        hence the calculation is a little different;\n        if 2, the first non-nan value means the average value of the following interval,\n        for example, in [5, nan, nan, nan], 5 means all four data's mean value;\n        default is [0, 2]\n    device\n        the number of device: -1 -&gt; \"cpu\" or \"cuda:x\" (x is 0, 1 or ...)\n    limit_part\n        when transfer learning, we may ignore some part;\n        the default is None, which means no ignorance;\n        other choices are list, such as [0], [0, 1] or [1,2,..];\n        0 means the first variable;\n        tensor is [seq, time, var] or [time, seq, var]\n    item_weight\n        use different weight for each item's loss;\n        for example, the default values [0.5, 0.5] means 0.5 * loss1 + 0.5 * loss2\n    alpha\n        the weight of the water-balance item's loss\n    beta\n        the weight of real water-balance item's loss, et_mean/p_mean + q_mean/p_mean = 1 can be a loss.\n        It is not strictly correct as training batch only have about one year data, but still could be a constraint\n    wb_loss_func\n        the loss function for water balance item, by default it is None, which means we use function in loss_funcs\n    \"\"\"\n    if data_gap is None:\n        data_gap = [0, 2]\n    if device is None:\n        device = [0]\n    if item_weight is None:\n        item_weight = [0.5, 0.5]\n    super(MultiOutWaterBalanceLoss, self).__init__()\n    self.loss_funcs = loss_funcs\n    self.data_gap = data_gap\n    self.device = get_the_device(device)\n    self.limit_part = limit_part\n    self.item_weight = item_weight\n    self.alpha = alpha\n    self.beta = beta\n    self.wb_loss_func = wb_loss_func\n    self.means = means\n    self.stds = stds\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.MultiOutWaterBalanceLoss.forward","title":"<code>forward(self, output, target)</code>","text":"<p>Calculate the sum of losses for different variables and water-balance loss</p> <p>When there are NaN values in observation, we will perform a \"reduce\" operation on prediction. For example, pred = [0,1,2,3,4], obs=[5, nan, nan, 6, nan]; the \"reduce\" is sum; then, pred_sum = [0+1+2, 3+4], obs_sum=[5,6], loss = loss_func(pred_sum, obs_sum). Notice: when \"sum\", actually final index is not chosen, because the whole observation may be [5, nan, nan, 6, nan, nan, 7, nan, nan], 6 means sum of three elements. Just as the rho is 5, the final one is not chosen</p>"},{"location":"api/models/#torchhydro.models.crits.MultiOutWaterBalanceLoss.forward--parameters","title":"Parameters","text":"<p>output     the prediction tensor; 3-dims are time sequence, batch and feature, respectively target     the observation tensor</p>"},{"location":"api/models/#torchhydro.models.crits.MultiOutWaterBalanceLoss.forward--returns","title":"Returns","text":"<p>Tensor     Whole loss</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(self, output: Tensor, target: Tensor):\n    \"\"\"\n    Calculate the sum of losses for different variables and water-balance loss\n\n    When there are NaN values in observation, we will perform a \"reduce\" operation on prediction.\n    For example, pred = [0,1,2,3,4], obs=[5, nan, nan, 6, nan]; the \"reduce\" is sum;\n    then, pred_sum = [0+1+2, 3+4], obs_sum=[5,6], loss = loss_func(pred_sum, obs_sum).\n    Notice: when \"sum\", actually final index is not chosen,\n    because the whole observation may be [5, nan, nan, 6, nan, nan, 7, nan, nan], 6 means sum of three elements.\n    Just as the rho is 5, the final one is not chosen\n\n\n    Parameters\n    ----------\n    output\n        the prediction tensor; 3-dims are time sequence, batch and feature, respectively\n    target\n        the observation tensor\n\n    Returns\n    -------\n    Tensor\n        Whole loss\n    \"\"\"\n    n_out = target.shape[-1]\n    loss = 0\n    p_means = []\n    t_means = []\n    all_means = self.means\n    all_stds = self.stds\n    for k in range(n_out):\n        if self.limit_part is not None and k in self.limit_part:\n            continue\n        p0 = output[:, :, k]\n        t0 = target[:, :, k]\n        # for water balance loss\n        if all_means is not None:\n            # denormalize for q and et\n            p1 = p0 * all_stds[k] + all_means[k]\n            t1 = t0 * all_stds[k] + all_means[k]\n            p2 = (10**p1 - 0.1) ** 2\n            t2 = (10**t1 - 0.1) ** 2\n            p_mean = torch.nanmean(p2, dim=0)\n            t_mean = torch.nanmean(t2, dim=0)\n        else:\n            p_mean = torch.nanmean(p0, dim=0)\n            t_mean = torch.nanmean(t0, dim=0)\n        p_means.append(p_mean)\n        t_means.append(t_mean)\n        # for mtl normal loss\n        mask = t0 == t0\n        p = p0[mask]\n        t = t0[mask]\n        if self.data_gap[k] &gt; 0:\n            p, t = deal_gap_data(p0, t0, self.data_gap[k], self.device)\n        if type(self.loss_funcs) is list:\n            temp = self.item_weight[k] * self.loss_funcs[k](p, t)\n        else:\n            temp = self.item_weight[k] * self.loss_funcs(p, t)\n        # sum of all k-th loss\n        loss = loss + temp\n    # water balance loss\n    p_mean_q_plus_et = torch.sum(torch.stack(p_means), dim=0)\n    t_mean_q_plus_et = torch.sum(torch.stack(t_means), dim=0)\n    wb_ones = torch.ones_like(t_mean_q_plus_et)\n    if self.wb_loss_func is None:\n        if type(self.loss_funcs) is list:\n            # if wb_loss_func is None, we use the first loss function in loss_funcs\n            wb_loss = self.loss_funcs[0](p_mean_q_plus_et, t_mean_q_plus_et)\n            wb_1loss = self.loss_funcs[0](p_mean_q_plus_et, wb_ones)\n        else:\n            wb_loss = self.loss_funcs(p_mean_q_plus_et, t_mean_q_plus_et)\n            wb_1loss = self.loss_funcs(p_mean_q_plus_et, wb_ones)\n    else:\n        wb_loss = self.wb_loss_func(p_mean_q_plus_et, t_mean_q_plus_et)\n        wb_1loss = self.wb_loss_func(p_mean_q_plus_et, wb_ones)\n    return (\n        self.alpha * wb_loss\n        + (1 - self.alpha - self.beta) * loss\n        + self.beta * wb_1loss\n    )\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.NSELoss","title":"<code> NSELoss            (Module)         </code>","text":"Source code in <code>torchhydro/models/crits.py</code> <pre><code>class NSELoss(torch.nn.Module):\n    # Same as Fredrick 2019\n    def __init__(self):\n        super(NSELoss, self).__init__()\n\n    def forward(self, output, target):\n        Ngage = target.shape[1]\n        losssum = 0\n        nsample = 0\n        for ii in range(Ngage):\n            t0 = target[:, ii, 0]\n            mask = t0 == t0\n            if len(mask[mask]) &gt; 0:\n                p0 = output[:, ii, 0]\n                p = p0[mask]\n                t = t0[mask]\n                tmean = t.mean()\n                SST = torch.sum((t - tmean) ** 2)\n                SSRes = torch.sum((t - p) ** 2)\n                temp = SSRes / ((torch.sqrt(SST) + 0.1) ** 2)\n                # original NSE\n                # temp = SSRes / SST\n                losssum = losssum + temp\n                nsample = nsample + 1\n        return losssum / nsample\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.NSELoss.forward","title":"<code>forward(self, output, target)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(self, output, target):\n    Ngage = target.shape[1]\n    losssum = 0\n    nsample = 0\n    for ii in range(Ngage):\n        t0 = target[:, ii, 0]\n        mask = t0 == t0\n        if len(mask[mask]) &gt; 0:\n            p0 = output[:, ii, 0]\n            p = p0[mask]\n            t = t0[mask]\n            tmean = t.mean()\n            SST = torch.sum((t - tmean) ** 2)\n            SSRes = torch.sum((t - p) ** 2)\n            temp = SSRes / ((torch.sqrt(SST) + 0.1) ** 2)\n            # original NSE\n            # temp = SSRes / SST\n            losssum = losssum + temp\n            nsample = nsample + 1\n    return losssum / nsample\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.NegativeLogLikelihood","title":"<code> NegativeLogLikelihood            (Module)         </code>","text":"<p>target -&gt; True y output -&gt; predicted distribution</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>class NegativeLogLikelihood(torch.nn.Module):\n    \"\"\"\n    target -&gt; True y\n    output -&gt; predicted distribution\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, output: torch.distributions, target: torch.Tensor):\n        \"\"\"\n        calculates NegativeLogLikelihood\n        \"\"\"\n        return -output.log_prob(target).sum()\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.NegativeLogLikelihood.forward","title":"<code>forward(self, output, target)</code>","text":"<p>calculates NegativeLogLikelihood</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(self, output: torch.distributions, target: torch.Tensor):\n    \"\"\"\n    calculates NegativeLogLikelihood\n    \"\"\"\n    return -output.log_prob(target).sum()\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.PESLoss","title":"<code> PESLoss            (Module)         </code>","text":"Source code in <code>torchhydro/models/crits.py</code> <pre><code>class PESLoss(torch.nn.Module):\n    def __init__(self):\n        \"\"\"\n        PES Loss: MSE \u00d7 sigmoid(MSE)\n\n        This loss function applies a sigmoid activation to MSE and then multiplies it with MSE,\n        creating a non-linear penalty that increases more gradually for larger errors.\n        \"\"\"\n        super(PESLoss, self).__init__()\n        self.mse = torch.nn.MSELoss(reduction=\"none\")\n\n    def forward(self, output: torch.Tensor, target: torch.Tensor):\n        mse_value = self.mse(output, target)\n        sigmoid_mse = torch.sigmoid(mse_value)\n        return mse_value * sigmoid_mse\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.PESLoss.__init__","title":"<code>__init__(self)</code>  <code>special</code>","text":"<p>PES Loss: MSE \u00d7 sigmoid(MSE)</p> <p>This loss function applies a sigmoid activation to MSE and then multiplies it with MSE, creating a non-linear penalty that increases more gradually for larger errors.</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    PES Loss: MSE \u00d7 sigmoid(MSE)\n\n    This loss function applies a sigmoid activation to MSE and then multiplies it with MSE,\n    creating a non-linear penalty that increases more gradually for larger errors.\n    \"\"\"\n    super(PESLoss, self).__init__()\n    self.mse = torch.nn.MSELoss(reduction=\"none\")\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.PESLoss.forward","title":"<code>forward(self, output, target)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(self, output: torch.Tensor, target: torch.Tensor):\n    mse_value = self.mse(output, target)\n    sigmoid_mse = torch.sigmoid(mse_value)\n    return mse_value * sigmoid_mse\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.PenalizedMSELoss","title":"<code> PenalizedMSELoss            (Module)         </code>","text":"<p>Returns MSE using: target -&gt; True y output -&gt; Predtion by model source: https://discuss.pytorch.org/t/rmse-loss-function/16540/3</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>class PenalizedMSELoss(torch.nn.Module):\n    \"\"\"\n    Returns MSE using:\n    target -&gt; True y\n    output -&gt; Predtion by model\n    source: https://discuss.pytorch.org/t/rmse-loss-function/16540/3\n    \"\"\"\n\n    def __init__(self, variance_penalty=0.0):\n        super().__init__()\n        self.mse = torch.nn.MSELoss()\n        self.variance_penalty = variance_penalty\n\n    def forward(self, output: torch.Tensor, target: torch.Tensor):\n        return self.mse(target, output) + self.variance_penalty * torch.std(\n            torch.sub(target, output)\n        )\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.PenalizedMSELoss.forward","title":"<code>forward(self, output, target)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(self, output: torch.Tensor, target: torch.Tensor):\n    return self.mse(target, output) + self.variance_penalty * torch.std(\n        torch.sub(target, output)\n    )\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.QuantileLoss","title":"<code> QuantileLoss            (Module)         </code>","text":"<p>From https://medium.com/the-artificial-impostor/quantile-regression-part-2-6fdbc26b2629</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>class QuantileLoss(torch.nn.Module):\n    \"\"\"From https://medium.com/the-artificial-impostor/quantile-regression-part-2-6fdbc26b2629\"\"\"\n\n    def __init__(self, quantiles):\n        super().__init__()\n        self.quantiles = quantiles\n\n    def forward(self, preds, target):\n        assert not target.requires_grad\n        assert preds.size(0) == target.size(0)\n        losses = []\n        for i, q in enumerate(self.quantiles):\n            mask = ~torch.isnan(target[:, :, i])\n            errors = target[:, :, i][mask] - preds[:, :, i][mask]\n            losses.append(torch.max((q - 1) * errors, q * errors))\n        return torch.mean(torch.cat(losses, dim=0), dim=0)\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.QuantileLoss.forward","title":"<code>forward(self, preds, target)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(self, preds, target):\n    assert not target.requires_grad\n    assert preds.size(0) == target.size(0)\n    losses = []\n    for i, q in enumerate(self.quantiles):\n        mask = ~torch.isnan(target[:, :, i])\n        errors = target[:, :, i][mask] - preds[:, :, i][mask]\n        losses.append(torch.max((q - 1) * errors, q * errors))\n    return torch.mean(torch.cat(losses, dim=0), dim=0)\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.RMSELoss","title":"<code> RMSELoss            (Module)         </code>","text":"Source code in <code>torchhydro/models/crits.py</code> <pre><code>class RMSELoss(torch.nn.Module):\n    def __init__(self, variance_penalty=0.0):\n        \"\"\"\n        Calculate RMSE\n\n        using:\n            target -&gt; True y\n            output -&gt; Prediction by model\n            source: https://discuss.pytorch.org/t/rmse-loss-function/16540/3\n\n        Parameters\n        ----------\n        variance_penalty\n            penalty for big variance; default is 0\n        \"\"\"\n        super().__init__()\n        self.mse = torch.nn.MSELoss()\n        self.variance_penalty = variance_penalty\n\n    def forward(self, output: torch.Tensor, target: torch.Tensor):\n        if len(output) &lt;= 1 or self.variance_penalty &lt;= 0.0:\n            return torch.sqrt(self.mse(target, output))\n        diff = torch.sub(target, output)\n        std_dev = torch.std(diff)\n        var_penalty = self.variance_penalty * std_dev\n\n        return torch.sqrt(self.mse(target, output)) + var_penalty\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.RMSELoss.__init__","title":"<code>__init__(self, variance_penalty=0.0)</code>  <code>special</code>","text":"<p>Calculate RMSE</p> <p>!!! using     target -&gt; True y     output -&gt; Prediction by model     source: https://discuss.pytorch.org/t/rmse-loss-function/16540/3</p>"},{"location":"api/models/#torchhydro.models.crits.RMSELoss.__init__--parameters","title":"Parameters","text":"<p>variance_penalty     penalty for big variance; default is 0</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def __init__(self, variance_penalty=0.0):\n    \"\"\"\n    Calculate RMSE\n\n    using:\n        target -&gt; True y\n        output -&gt; Prediction by model\n        source: https://discuss.pytorch.org/t/rmse-loss-function/16540/3\n\n    Parameters\n    ----------\n    variance_penalty\n        penalty for big variance; default is 0\n    \"\"\"\n    super().__init__()\n    self.mse = torch.nn.MSELoss()\n    self.variance_penalty = variance_penalty\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.RMSELoss.forward","title":"<code>forward(self, output, target)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(self, output: torch.Tensor, target: torch.Tensor):\n    if len(output) &lt;= 1 or self.variance_penalty &lt;= 0.0:\n        return torch.sqrt(self.mse(target, output))\n    diff = torch.sub(target, output)\n    std_dev = torch.std(diff)\n    var_penalty = self.variance_penalty * std_dev\n\n    return torch.sqrt(self.mse(target, output)) + var_penalty\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.RmseLoss","title":"<code> RmseLoss            (Module)         </code>","text":"Source code in <code>torchhydro/models/crits.py</code> <pre><code>class RmseLoss(torch.nn.Module):\n    def __init__(self):\n        \"\"\"\n        RMSE loss which could ignore NaN values\n\n        Now we only support 3-d tensor and 1-d tensor\n        \"\"\"\n        super(RmseLoss, self).__init__()\n\n    def forward(self, output, target):\n        if target.dim() == 1:\n            mask = target == target\n            p = output[mask]\n            t = target[mask]\n            return torch.sqrt(((p - t) ** 2).mean())\n        ny = target.shape[2]\n        loss = 0\n        for k in range(ny):\n            p0 = output[:, :, k]\n            t0 = target[:, :, k]\n            mask = t0 == t0\n            p = p0[mask]\n            p = torch.where(torch.isnan(p), torch.full_like(p, 0), p)\n            t = t0[mask]\n            t = torch.where(torch.isnan(t), torch.full_like(t, 0), t)\n            temp = torch.sqrt(((p - t) ** 2).mean())\n            loss = loss + temp\n        return loss\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.RmseLoss.__init__","title":"<code>__init__(self)</code>  <code>special</code>","text":"<p>RMSE loss which could ignore NaN values</p> <p>Now we only support 3-d tensor and 1-d tensor</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    RMSE loss which could ignore NaN values\n\n    Now we only support 3-d tensor and 1-d tensor\n    \"\"\"\n    super(RmseLoss, self).__init__()\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.RmseLoss.forward","title":"<code>forward(self, output, target)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(self, output, target):\n    if target.dim() == 1:\n        mask = target == target\n        p = output[mask]\n        t = target[mask]\n        return torch.sqrt(((p - t) ** 2).mean())\n    ny = target.shape[2]\n    loss = 0\n    for k in range(ny):\n        p0 = output[:, :, k]\n        t0 = target[:, :, k]\n        mask = t0 == t0\n        p = p0[mask]\n        p = torch.where(torch.isnan(p), torch.full_like(p, 0), p)\n        t = t0[mask]\n        t = torch.where(torch.isnan(t), torch.full_like(t, 0), t)\n        temp = torch.sqrt(((p - t) ** 2).mean())\n        loss = loss + temp\n    return loss\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.SigmaLoss","title":"<code> SigmaLoss            (Module)         </code>","text":"Source code in <code>torchhydro/models/crits.py</code> <pre><code>class SigmaLoss(torch.nn.Module):\n    def __init__(self, prior=\"gauss\"):\n        super(SigmaLoss, self).__init__()\n        self.reduction = \"elementwise_mean\"\n        self.prior = None if prior == \"\" else prior.split(\"+\")\n\n    def forward(self, output, target):\n        ny = target.shape[-1]\n        lossMean = 0\n        for k in range(ny):\n            p0 = output[:, :, k * 2]\n            s0 = output[:, :, k * 2 + 1]\n            t0 = target[:, :, k]\n            mask = t0 == t0\n            p = p0[mask]\n            s = s0[mask]\n            t = t0[mask]\n            if self.prior[0] == \"gauss\":\n                loss = torch.exp(-s).mul((p - t) ** 2) / 2 + s / 2\n            elif self.prior[0] == \"invGamma\":\n                c1 = float(self.prior[1])\n                c2 = float(self.prior[2])\n                nt = p.shape[0]\n                loss = (\n                    torch.exp(-s).mul((p - t) ** 2 + c2 / nt) / 2\n                    + (1 / 2 + c1 / nt) * s\n                )\n            lossMean = lossMean + torch.mean(loss)\n        return lossMean\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.SigmaLoss.forward","title":"<code>forward(self, output, target)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(self, output, target):\n    ny = target.shape[-1]\n    lossMean = 0\n    for k in range(ny):\n        p0 = output[:, :, k * 2]\n        s0 = output[:, :, k * 2 + 1]\n        t0 = target[:, :, k]\n        mask = t0 == t0\n        p = p0[mask]\n        s = s0[mask]\n        t = t0[mask]\n        if self.prior[0] == \"gauss\":\n            loss = torch.exp(-s).mul((p - t) ** 2) / 2 + s / 2\n        elif self.prior[0] == \"invGamma\":\n            c1 = float(self.prior[1])\n            c2 = float(self.prior[2])\n            nt = p.shape[0]\n            loss = (\n                torch.exp(-s).mul((p - t) ** 2 + c2 / nt) / 2\n                + (1 / 2 + c1 / nt) * s\n            )\n        lossMean = lossMean + torch.mean(loss)\n    return lossMean\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.UncertaintyWeights","title":"<code> UncertaintyWeights            (Module)         </code>","text":"<p>Uncertainty Weights (UW).</p> <p>This method is proposed in <code>Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics (CVPR 2018) &lt;https://openaccess.thecvf.com/content_cvpr_2018/papers/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.pdf&gt;</code>_ \\ and implemented by us.</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>class UncertaintyWeights(torch.nn.Module):\n    r\"\"\"Uncertainty Weights (UW).\n\n    This method is proposed in `Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics (CVPR 2018) &lt;https://openaccess.thecvf.com/content_cvpr_2018/papers/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.pdf&gt;`_ \\\n    and implemented by us.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        loss_funcs: Union[torch.nn.Module, list],\n        data_gap: list = None,\n        device: list = None,\n        limit_part: list = None,\n    ):\n        if data_gap is None:\n            data_gap = [0, 2]\n        if device is None:\n            device = [0]\n        super(UncertaintyWeights, self).__init__()\n        self.loss_funcs = loss_funcs\n        self.data_gap = data_gap\n        self.device = get_the_device(device)\n        self.limit_part = limit_part\n\n    def forward(self, output, target, log_vars):\n        \"\"\"\n\n        Parameters\n        ----------\n        output\n        target\n        log_vars\n            sigma in uncertainty weighting;\n            default is None, meaning we manually set weights for different target's loss;\n            more info could be seen in\n            https://libmtl.readthedocs.io/en/latest/docs/_autoapi/LibMTL/weighting/index.html#LibMTL.weighting.UW\n\n        Returns\n        -------\n        torch.Tensor\n            multi-task loss by uncertainty weighting method\n        \"\"\"\n        n_out = target.shape[-1]\n        loss = 0\n        for k in range(n_out):\n            precision = torch.exp(-log_vars[k])\n            if self.limit_part is not None and k in self.limit_part:\n                continue\n            p0 = output[:, :, k]\n            t0 = target[:, :, k]\n            mask = t0 == t0\n            p = p0[mask]\n            t = t0[mask]\n            if self.data_gap[k] &gt; 0:\n                p, t = deal_gap_data(p0, t0, self.data_gap[k], self.device)\n            if type(self.loss_funcs) is list:\n                temp = self.loss_funcs[k](p, t)\n            else:\n                temp = self.loss_funcs(p, t)\n            loss += torch.sum(precision * temp + log_vars[k], -1)\n        return loss\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.UncertaintyWeights.forward","title":"<code>forward(self, output, target, log_vars)</code>","text":""},{"location":"api/models/#torchhydro.models.crits.UncertaintyWeights.forward--parameters","title":"Parameters","text":"<p>output target log_vars     sigma in uncertainty weighting;     default is None, meaning we manually set weights for different target's loss;     more info could be seen in     https://libmtl.readthedocs.io/en/latest/docs/_autoapi/LibMTL/weighting/index.html#LibMTL.weighting.UW</p>"},{"location":"api/models/#torchhydro.models.crits.UncertaintyWeights.forward--returns","title":"Returns","text":"<p>torch.Tensor     multi-task loss by uncertainty weighting method</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def forward(self, output, target, log_vars):\n    \"\"\"\n\n    Parameters\n    ----------\n    output\n    target\n    log_vars\n        sigma in uncertainty weighting;\n        default is None, meaning we manually set weights for different target's loss;\n        more info could be seen in\n        https://libmtl.readthedocs.io/en/latest/docs/_autoapi/LibMTL/weighting/index.html#LibMTL.weighting.UW\n\n    Returns\n    -------\n    torch.Tensor\n        multi-task loss by uncertainty weighting method\n    \"\"\"\n    n_out = target.shape[-1]\n    loss = 0\n    for k in range(n_out):\n        precision = torch.exp(-log_vars[k])\n        if self.limit_part is not None and k in self.limit_part:\n            continue\n        p0 = output[:, :, k]\n        t0 = target[:, :, k]\n        mask = t0 == t0\n        p = p0[mask]\n        t = t0[mask]\n        if self.data_gap[k] &gt; 0:\n            p, t = deal_gap_data(p0, t0, self.data_gap[k], self.device)\n        if type(self.loss_funcs) is list:\n            temp = self.loss_funcs[k](p, t)\n        else:\n            temp = self.loss_funcs(p, t)\n        loss += torch.sum(precision * temp + log_vars[k], -1)\n    return loss\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.deal_gap_data","title":"<code>deal_gap_data(output, target, data_gap, device)</code>","text":"<p>How to handle with gap data</p> <p>When there are NaN values in observation, we will perform a \"reduce\" operation on prediction. For example, pred = [0,1,2,3,4], obs=[5, nan, nan, 6, nan]; the \"reduce\" is sum; then, pred_sum = [0+1+2, 3+4], obs_sum=[5,6], loss = loss_func(pred_sum, obs_sum). Notice: when \"sum\", actually final index is not chosen, because the whole observation may be [5, nan, nan, 6, nan, nan, 7, nan, nan], 6 means sum of three elements. Just as the rho is 5, the final one is not chosen</p>"},{"location":"api/models/#torchhydro.models.crits.deal_gap_data--parameters","title":"Parameters","text":"<p>output     model output for k-th variable target     target for k-th variable data_gap     data_gap=1: reduce is sum     data_gap=2: reduce is mean device     where to save the data</p>"},{"location":"api/models/#torchhydro.models.crits.deal_gap_data--returns","title":"Returns","text":"<p>tuple[tensor, tensor]     output and target after dealing with gap</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def deal_gap_data(output, target, data_gap, device):\n    \"\"\"\n    How to handle with gap data\n\n    When there are NaN values in observation, we will perform a \"reduce\" operation on prediction.\n    For example, pred = [0,1,2,3,4], obs=[5, nan, nan, 6, nan]; the \"reduce\" is sum;\n    then, pred_sum = [0+1+2, 3+4], obs_sum=[5,6], loss = loss_func(pred_sum, obs_sum).\n    Notice: when \"sum\", actually final index is not chosen,\n    because the whole observation may be [5, nan, nan, 6, nan, nan, 7, nan, nan], 6 means sum of three elements.\n    Just as the rho is 5, the final one is not chosen\n\n    Parameters\n    ----------\n    output\n        model output for k-th variable\n    target\n        target for k-th variable\n    data_gap\n        data_gap=1: reduce is sum\n        data_gap=2: reduce is mean\n    device\n        where to save the data\n\n    Returns\n    -------\n    tuple[tensor, tensor]\n        output and target after dealing with gap\n    \"\"\"\n    # all members in a batch has different NaN-gap, so we need a loop\n    seg_p_lst = []\n    seg_t_lst = []\n    for j in range(target.shape[1]):\n        non_nan_idx = torch.nonzero(\n            ~torch.isnan(target[:, j]), as_tuple=False\n        ).squeeze()\n        if len(non_nan_idx) &lt; 1:\n            raise ArithmeticError(\"All NaN elements, please check your data\")\n\n        # \u4f7f\u7528 cumsum \u751f\u6210 scatter_index\n        is_not_nan = ~torch.isnan(target[:, j])\n        cumsum_is_not_nan = torch.cumsum(is_not_nan.to(torch.int), dim=0)\n        first_non_nan = non_nan_idx[0]\n        scatter_index = torch.full_like(\n            target[:, j], fill_value=-1, dtype=torch.long\n        )  # \u5c06\u6240\u6709\u503c\u521d\u59cb\u5316\u4e3a -1\n        scatter_index[first_non_nan:] = cumsum_is_not_nan[first_non_nan:] - 1\n        scatter_index = scatter_index.to(device=device)\n\n        # \u521b\u5efa\u63a9\u7801\uff0c\u53ea\u4fdd\u7559\u6709\u6548\u7684\u7d22\u5f15\n        valid_mask = scatter_index &gt;= 0\n\n        if data_gap == 1:\n            seg = torch.zeros(\n                len(non_nan_idx), device=device, dtype=output.dtype\n            ).scatter_add_(0, scatter_index[valid_mask], output[valid_mask, j])\n            # for sum, better exclude final non-nan value as it didn't include all necessary periods\n            seg_p_lst.append(seg[:-1])\n            seg_t_lst.append(target[non_nan_idx[:-1], j])\n\n        elif data_gap == 2:\n            counts = torch.zeros(\n                len(non_nan_idx), device=device, dtype=output.dtype\n            ).scatter_add_(\n                0,\n                scatter_index[valid_mask],\n                torch.ones_like(output[valid_mask, j], dtype=output.dtype),\n            )\n            seg = torch.zeros(\n                len(non_nan_idx), device=device, dtype=output.dtype\n            ).scatter_add_(0, scatter_index[valid_mask], output[valid_mask, j])\n            seg = seg / counts.clamp(min=1)\n            # for mean, we can include all periods\n            seg_p_lst.append(seg)\n            seg_t_lst.append(target[non_nan_idx, j])\n        else:\n            raise NotImplementedError(\n                \"We have not provided this reduce way now!! Please choose 1 or 2!!\"\n            )\n\n    p = torch.cat(seg_p_lst)\n    t = torch.cat(seg_t_lst)\n    return p, t\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.l1_regularizer","title":"<code>l1_regularizer(model, lambda_l1=0.01)</code>","text":"<p>source: https://stackoverflow.com/questions/58172188/how-to-add-l1-regularization-to-pytorch-nn-model</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def l1_regularizer(model, lambda_l1=0.01):\n    \"\"\"\n    source: https://stackoverflow.com/questions/58172188/how-to-add-l1-regularization-to-pytorch-nn-model\n    \"\"\"\n    lossl1 = 0\n    for model_param_name, model_param_value in model.named_parameters():\n        if model_param_name.endswith(\"weight\"):\n            lossl1 += lambda_l1 * model_param_value.abs().sum()\n        return lossl1\n</code></pre>"},{"location":"api/models/#torchhydro.models.crits.orth_regularizer","title":"<code>orth_regularizer(model, lambda_orth=0.01)</code>","text":"<p>source: https://stackoverflow.com/questions/58172188/how-to-add-l1-regularization-to-pytorch-nn-model</p> Source code in <code>torchhydro/models/crits.py</code> <pre><code>def orth_regularizer(model, lambda_orth=0.01):\n    \"\"\"\n    source: https://stackoverflow.com/questions/58172188/how-to-add-l1-regularization-to-pytorch-nn-model\n    \"\"\"\n    lossorth = 0\n    for model_param_name, model_param_value in model.named_parameters():\n        if model_param_name.endswith(\"weight\"):\n            param_flat = model_param_value.view(model_param_value.shape[0], -1)\n            sym = torch.mm(param_flat, torch.t(param_flat))\n            sym -= torch.eye(param_flat.shape[0])\n            lossorth += lambda_orth * sym.sum()\n\n        return lossorth\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm","title":"<code>cudnnlstm</code>","text":"<p>Author: MHPI group, Wenyu Ouyang Date: 2021-12-31 11:08:29 LastEditTime: 2024-10-09 16:36:34 LastEditors: Wenyu Ouyang Description: LSTM with dropout implemented by Kuai Fang and more LSTMs using it FilePath:       orchhydro       orchhydro\\models\\cudnnlstm.py Copyright (c) 2021-2022 MHPI group, Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CNN1dKernel","title":"<code> CNN1dKernel            (Module)         </code>","text":"Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>class CNN1dKernel(torch.nn.Module):\n    def __init__(self, *, ninchannel=1, nkernel=3, kernelSize=3, stride=1, padding=0):\n        super(CNN1dKernel, self).__init__()\n        self.cnn1d = torch.nn.Conv1d(\n            in_channels=ninchannel,\n            out_channels=nkernel,\n            kernel_size=kernelSize,\n            padding=padding,\n            stride=stride,\n        )\n        self.name = \"CNN1dkernel\"\n        self.is_legacy = True\n\n    def forward(self, x):\n        return F.relu(self.cnn1d(x))\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CNN1dKernel.forward","title":"<code>forward(self, x)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def forward(self, x):\n    return F.relu(self.cnn1d(x))\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CNN1dLCmodel","title":"<code> CNN1dLCmodel            (Module)         </code>","text":"Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>class CNN1dLCmodel(nn.Module):\n    # Directly add the CNN extracted features into LSTM inputSize\n    def __init__(\n        self,\n        nx,\n        ny,\n        nobs,\n        hidden_size,\n        n_kernel: Union[list, tuple] = (10, 5),\n        kernel_size: Union[list, tuple] = (3, 3),\n        stride: Union[list, tuple] = (2, 1),\n        dr=0.5,\n        pool_opt=None,\n        cnn_dr=0.5,\n        cat_first=True,\n    ):\n        \"\"\"cat_first means: we will concatenate the CNN output with the x, then input them to the CudnnLstm model;\n        if not cat_first, it is relu_first, meaning we will relu the CNN output firstly, then concatenate it with x\n        \"\"\"\n        # two convolutional layer\n        super(CNN1dLCmodel, self).__init__()\n        # N_cnn_out\u4ee3\u8868\u8f93\u51fa\u7684\u7279\u5f81\u6570\u91cf\n        # nx\u4ee3\u8868\u5386\u53f2\u7684\u8f93\u5165\n        # ny\u4ee3\u8868\u6700\u540e\u7ebf\u6027\u5c42\u8f93\u51fa\u7684\u7ef4\u5ea6\uff0c\u5982\u679c\u53ea\u9884\u62a5\u6d41\u91cf\uff0c\u5219\u4e3a1\n        # nobs\u4ee3\u8868\u8981\u8f93\u5165\u5230CNN\u7684\u7ef4\u5ea6\n        # hidden_size\u662f\u7ebf\u6027\u5c42\u7684\u9690\u85cf\u5c42\u7684\u8282\u70b9\u6570\n        self.nx = nx\n        self.ny = ny\n        self.obs = nobs\n        self.hiddenSize = hidden_size\n        n_layer = len(n_kernel)\n        self.features = nn.Sequential()\n        n_in_chan = 1\n        lout = nobs\n        for ii in range(n_layer):\n            conv_layer = CNN1dKernel(\n                ninchannel=n_in_chan,\n                nkernel=n_kernel[ii],\n                kernelSize=kernel_size[ii],\n                stride=stride[ii],\n            )\n            self.features.add_module(\"CnnLayer%d\" % (ii + 1), conv_layer)\n            if cnn_dr != 0.0:\n                self.features.add_module(\"dropout%d\" % (ii + 1), nn.Dropout(p=cnn_dr))\n            n_in_chan = n_kernel[ii]\n            lout = cal_conv_size(lin=lout, kernel=kernel_size[ii], stride=stride[ii])\n            self.features.add_module(\"Relu%d\" % (ii + 1), nn.ReLU())\n            if pool_opt is not None:\n                self.features.add_module(\n                    \"Pooling%d\" % (ii + 1), nn.MaxPool1d(pool_opt[ii])\n                )\n                lout = cal_pool_size(lin=lout, kernel=pool_opt[ii])\n        self.N_cnn_out = int(\n            lout * n_kernel[-1]\n        )  # total CNN feature number after convolution\n        self.cat_first = cat_first\n        # \u8981\u4e0d\u8981\u5148\u62fc\u63a5\uff1f\n        # \u5148\u62fc\u63a5\uff0c\u5219\u4ee3\u8868\u7ebf\u6027\u5c42\u4e2d\uff0c\u8f93\u5165\u7684\u7ef4\u5ea6\u662f\u672a\u6765\u7684\u964d\u6c34\u7b49\u8f93\u5165\u8f93\u51fa\u7684CNN\u7279\u5f81\u7ef4\u5ea6\uff0c\u548c\u5386\u53f2\u89c2\u6d4b\u7684\u7b49\u65f6\u95f4\u5e8f\u5217\u7684\u7279\u5f81\u6570\u91cf\uff0c\u901a\u8fc7\u7ebf\u6027\u5c42\u5408\u5e76\u6210\u4e00\u4e2a\uff0c\u7136\u540e\u518d\u628a\u8fd9\u4e9b\u7279\u5f81\u8f93\u51fa\u5230\u4e00\u4e2a\u7ebf\u6027\u5c42\u4e2d\n        # \u5982\u679c\u4e0d\u62fc\u63a5\uff0c\u90a3\u4e48\u5386\u53f2\u89c2\u6d4b\u6570\u636e\u5148\u8fdb\u5165\u4e00\u4e2a\u7ebf\u6027\u5c42\n        if cat_first:\n            nf = self.N_cnn_out + nx\n            self.linearIn = torch.nn.Linear(nf, hidden_size)\n            # CudnnLstm\u9664\u4e86\u6700\u57fa\u7840\u7684\u90e8\u5206\u4ee5\u5916\uff0c\u4e3b\u8981\u662f\u6709\u4e2ah\u548cc\u4e24\u4e2a\u95e8\u4e3a\u7a7a\u7684\u7ea0\u9519\uff0c\u8fd9\u4e2a\u5728\u8bba\u6587\u91cc\u8bb2\u8ff0\u7684\u662f\u56e0\u4e3a\u53ef\u80fd\u8f93\u5165\u7f3a\u5931\uff0c\u4f46\u662f\u53c8\u4e0d\u60f3\u7528\u63d2\u503c\u5904\u7406\n            # \u4e0d\u60f3\u7528\u63d2\u503c\u5904\u7406\u662f\u56e0\u4e3a\u8ba4\u4e3a\u4f1a\u66b4\u9732\u672a\u6765\u4fe1\u606f\n            # \u91c7\u7528\u4e86\u7f6e\u96f6\u64cd\u4f5c\uff0c\u539f\u6587\u7684\u8868\u8ff0\u662f\u8fd9\u79cd\u7f3a\u5931\u70b9\u8f83\u5c11\uff0c\u5728\u6a21\u578b\u7684\u4e0d\u65ad\u66f4\u65b0\u53c2\u6570\u540e\uff0c\u8fd9\u79cd\u7f6e\u96f6\u7684\u5f71\u54cd\u5bf9\u4e8e\u6a21\u578b\u7684\u8f93\u51fa\u5f71\u54cd\u5f88\u5c0f\n            self.lstm = CudnnLstm(\n                input_size=hidden_size, hidden_size=hidden_size, dr=dr\n            )\n        else:\n            nf = self.N_cnn_out + hidden_size\n            self.linearIn = torch.nn.Linear(nx, hidden_size)\n            self.lstm = CudnnLstm(input_size=nf, hidden_size=hidden_size, dr=dr)\n        self.linearOut = torch.nn.Linear(hidden_size, ny)\n        self.gpu = 1\n\n    def forward(self, x, z, do_drop_mc=False):\n        # z = n_grid*nVar add a channel dimension\n        # z = z.t()\n        n_grid, nobs, _ = z.shape\n        z = z.reshape(n_grid * nobs, 1)\n        n_t, bs, n_var = x.shape\n        # add a channel dimension\n        z = torch.unsqueeze(z, dim=1)\n        z0 = self.features(z)\n        # z0 = (n_grid) * n_kernel * sizeafterconv\n        z0 = z0.view(n_grid, self.N_cnn_out).repeat(n_t, 1, 1)\n        if self.cat_first:\n            x = torch.cat((x, z0), dim=2)\n            x0 = F.relu(self.linearIn(x))\n        else:\n            x = F.relu(self.linearIn(x))\n            x0 = torch.cat((x, z0), dim=2)\n        out_lstm, (hn, cn) = self.lstm(x0, do_drop_mc=do_drop_mc)\n        return self.linearOut(out_lstm)\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CNN1dLCmodel.__init__","title":"<code>__init__(self, nx, ny, nobs, hidden_size, n_kernel=(10, 5), kernel_size=(3, 3), stride=(2, 1), dr=0.5, pool_opt=None, cnn_dr=0.5, cat_first=True)</code>  <code>special</code>","text":"<p>cat_first means: we will concatenate the CNN output with the x, then input them to the CudnnLstm model; if not cat_first, it is relu_first, meaning we will relu the CNN output firstly, then concatenate it with x</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def __init__(\n    self,\n    nx,\n    ny,\n    nobs,\n    hidden_size,\n    n_kernel: Union[list, tuple] = (10, 5),\n    kernel_size: Union[list, tuple] = (3, 3),\n    stride: Union[list, tuple] = (2, 1),\n    dr=0.5,\n    pool_opt=None,\n    cnn_dr=0.5,\n    cat_first=True,\n):\n    \"\"\"cat_first means: we will concatenate the CNN output with the x, then input them to the CudnnLstm model;\n    if not cat_first, it is relu_first, meaning we will relu the CNN output firstly, then concatenate it with x\n    \"\"\"\n    # two convolutional layer\n    super(CNN1dLCmodel, self).__init__()\n    # N_cnn_out\u4ee3\u8868\u8f93\u51fa\u7684\u7279\u5f81\u6570\u91cf\n    # nx\u4ee3\u8868\u5386\u53f2\u7684\u8f93\u5165\n    # ny\u4ee3\u8868\u6700\u540e\u7ebf\u6027\u5c42\u8f93\u51fa\u7684\u7ef4\u5ea6\uff0c\u5982\u679c\u53ea\u9884\u62a5\u6d41\u91cf\uff0c\u5219\u4e3a1\n    # nobs\u4ee3\u8868\u8981\u8f93\u5165\u5230CNN\u7684\u7ef4\u5ea6\n    # hidden_size\u662f\u7ebf\u6027\u5c42\u7684\u9690\u85cf\u5c42\u7684\u8282\u70b9\u6570\n    self.nx = nx\n    self.ny = ny\n    self.obs = nobs\n    self.hiddenSize = hidden_size\n    n_layer = len(n_kernel)\n    self.features = nn.Sequential()\n    n_in_chan = 1\n    lout = nobs\n    for ii in range(n_layer):\n        conv_layer = CNN1dKernel(\n            ninchannel=n_in_chan,\n            nkernel=n_kernel[ii],\n            kernelSize=kernel_size[ii],\n            stride=stride[ii],\n        )\n        self.features.add_module(\"CnnLayer%d\" % (ii + 1), conv_layer)\n        if cnn_dr != 0.0:\n            self.features.add_module(\"dropout%d\" % (ii + 1), nn.Dropout(p=cnn_dr))\n        n_in_chan = n_kernel[ii]\n        lout = cal_conv_size(lin=lout, kernel=kernel_size[ii], stride=stride[ii])\n        self.features.add_module(\"Relu%d\" % (ii + 1), nn.ReLU())\n        if pool_opt is not None:\n            self.features.add_module(\n                \"Pooling%d\" % (ii + 1), nn.MaxPool1d(pool_opt[ii])\n            )\n            lout = cal_pool_size(lin=lout, kernel=pool_opt[ii])\n    self.N_cnn_out = int(\n        lout * n_kernel[-1]\n    )  # total CNN feature number after convolution\n    self.cat_first = cat_first\n    # \u8981\u4e0d\u8981\u5148\u62fc\u63a5\uff1f\n    # \u5148\u62fc\u63a5\uff0c\u5219\u4ee3\u8868\u7ebf\u6027\u5c42\u4e2d\uff0c\u8f93\u5165\u7684\u7ef4\u5ea6\u662f\u672a\u6765\u7684\u964d\u6c34\u7b49\u8f93\u5165\u8f93\u51fa\u7684CNN\u7279\u5f81\u7ef4\u5ea6\uff0c\u548c\u5386\u53f2\u89c2\u6d4b\u7684\u7b49\u65f6\u95f4\u5e8f\u5217\u7684\u7279\u5f81\u6570\u91cf\uff0c\u901a\u8fc7\u7ebf\u6027\u5c42\u5408\u5e76\u6210\u4e00\u4e2a\uff0c\u7136\u540e\u518d\u628a\u8fd9\u4e9b\u7279\u5f81\u8f93\u51fa\u5230\u4e00\u4e2a\u7ebf\u6027\u5c42\u4e2d\n    # \u5982\u679c\u4e0d\u62fc\u63a5\uff0c\u90a3\u4e48\u5386\u53f2\u89c2\u6d4b\u6570\u636e\u5148\u8fdb\u5165\u4e00\u4e2a\u7ebf\u6027\u5c42\n    if cat_first:\n        nf = self.N_cnn_out + nx\n        self.linearIn = torch.nn.Linear(nf, hidden_size)\n        # CudnnLstm\u9664\u4e86\u6700\u57fa\u7840\u7684\u90e8\u5206\u4ee5\u5916\uff0c\u4e3b\u8981\u662f\u6709\u4e2ah\u548cc\u4e24\u4e2a\u95e8\u4e3a\u7a7a\u7684\u7ea0\u9519\uff0c\u8fd9\u4e2a\u5728\u8bba\u6587\u91cc\u8bb2\u8ff0\u7684\u662f\u56e0\u4e3a\u53ef\u80fd\u8f93\u5165\u7f3a\u5931\uff0c\u4f46\u662f\u53c8\u4e0d\u60f3\u7528\u63d2\u503c\u5904\u7406\n        # \u4e0d\u60f3\u7528\u63d2\u503c\u5904\u7406\u662f\u56e0\u4e3a\u8ba4\u4e3a\u4f1a\u66b4\u9732\u672a\u6765\u4fe1\u606f\n        # \u91c7\u7528\u4e86\u7f6e\u96f6\u64cd\u4f5c\uff0c\u539f\u6587\u7684\u8868\u8ff0\u662f\u8fd9\u79cd\u7f3a\u5931\u70b9\u8f83\u5c11\uff0c\u5728\u6a21\u578b\u7684\u4e0d\u65ad\u66f4\u65b0\u53c2\u6570\u540e\uff0c\u8fd9\u79cd\u7f6e\u96f6\u7684\u5f71\u54cd\u5bf9\u4e8e\u6a21\u578b\u7684\u8f93\u51fa\u5f71\u54cd\u5f88\u5c0f\n        self.lstm = CudnnLstm(\n            input_size=hidden_size, hidden_size=hidden_size, dr=dr\n        )\n    else:\n        nf = self.N_cnn_out + hidden_size\n        self.linearIn = torch.nn.Linear(nx, hidden_size)\n        self.lstm = CudnnLstm(input_size=nf, hidden_size=hidden_size, dr=dr)\n    self.linearOut = torch.nn.Linear(hidden_size, ny)\n    self.gpu = 1\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CNN1dLCmodel.forward","title":"<code>forward(self, x, z, do_drop_mc=False)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def forward(self, x, z, do_drop_mc=False):\n    # z = n_grid*nVar add a channel dimension\n    # z = z.t()\n    n_grid, nobs, _ = z.shape\n    z = z.reshape(n_grid * nobs, 1)\n    n_t, bs, n_var = x.shape\n    # add a channel dimension\n    z = torch.unsqueeze(z, dim=1)\n    z0 = self.features(z)\n    # z0 = (n_grid) * n_kernel * sizeafterconv\n    z0 = z0.view(n_grid, self.N_cnn_out).repeat(n_t, 1, 1)\n    if self.cat_first:\n        x = torch.cat((x, z0), dim=2)\n        x0 = F.relu(self.linearIn(x))\n    else:\n        x = F.relu(self.linearIn(x))\n        x0 = torch.cat((x, z0), dim=2)\n    out_lstm, (hn, cn) = self.lstm(x0, do_drop_mc=do_drop_mc)\n    return self.linearOut(out_lstm)\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CpuLstmModel","title":"<code> CpuLstmModel            (Module)         </code>","text":"<p>Cpu version of CudnnLstmModel</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>class CpuLstmModel(nn.Module):\n    \"\"\"Cpu version of CudnnLstmModel\"\"\"\n\n    def __init__(self, *, n_input_features, n_output_features, n_hidden_states, dr=0.5):\n        super(CpuLstmModel, self).__init__()\n        self.nx = n_input_features\n        self.ny = n_output_features\n        self.hiddenSize = n_hidden_states\n        self.ct = 0\n        self.nLayer = 1\n        self.linearIn = torch.nn.Linear(n_input_features, n_hidden_states)\n        self.lstm = LstmCellTied(\n            input_size=n_hidden_states,\n            hidden_size=n_hidden_states,\n            dr=dr,\n            dr_method=\"drW\",\n            gpu=-1,\n        )\n        self.linearOut = torch.nn.Linear(n_hidden_states, n_output_features)\n        self.gpu = -1\n\n    def forward(self, x, do_drop_mc=False):\n        # x0 = F.relu(self.linearIn(x))\n        # outLSTM, (hn, cn) = self.lstm(x0, do_drop_mc=do_drop_mc)\n        # out = self.linearOut(outLSTM)\n        # return out\n        nt, ngrid, nx = x.shape\n        yt = torch.zeros(ngrid, 1)\n        out = torch.zeros(nt, ngrid, self.ny)\n        ht = None\n        ct = None\n        reset_mask = True\n        for t in range(nt):\n            xt = x[t, :, :]\n            xt = torch.where(torch.isnan(xt), torch.full_like(xt, 0), xt)\n            x0 = F.relu(self.linearIn(xt))\n            ht, ct = self.lstm(x0, hidden=(ht, ct), do_reset_mask=reset_mask)\n            yt = self.linearOut(ht)\n            reset_mask = False\n            out[t, :, :] = yt\n        return out\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CpuLstmModel.forward","title":"<code>forward(self, x, do_drop_mc=False)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def forward(self, x, do_drop_mc=False):\n    # x0 = F.relu(self.linearIn(x))\n    # outLSTM, (hn, cn) = self.lstm(x0, do_drop_mc=do_drop_mc)\n    # out = self.linearOut(outLSTM)\n    # return out\n    nt, ngrid, nx = x.shape\n    yt = torch.zeros(ngrid, 1)\n    out = torch.zeros(nt, ngrid, self.ny)\n    ht = None\n    ct = None\n    reset_mask = True\n    for t in range(nt):\n        xt = x[t, :, :]\n        xt = torch.where(torch.isnan(xt), torch.full_like(xt, 0), xt)\n        x0 = F.relu(self.linearIn(xt))\n        ht, ct = self.lstm(x0, hidden=(ht, ct), do_reset_mask=reset_mask)\n        yt = self.linearOut(ht)\n        reset_mask = False\n        out[t, :, :] = yt\n    return out\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstm","title":"<code> CudnnLstm            (Module)         </code>","text":"<p>LSTM with dropout implemented by Kuai Fang: https://github.com/mhpi/hydroDL/blob/release/hydroDL/model/rnn.py</p> <p>Only run in GPU; the CPU version is LstmCellTied in this file</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>class CudnnLstm(nn.Module):\n    \"\"\"\n    LSTM with dropout implemented by Kuai Fang: https://github.com/mhpi/hydroDL/blob/release/hydroDL/model/rnn.py\n\n    Only run in GPU; the CPU version is LstmCellTied in this file\n    \"\"\"\n\n    def __init__(self, *, input_size, hidden_size, dr=0.5):\n        \"\"\"\n\n        Parameters\n        ----------\n        input_size\n            number of neurons in input layer\n        hidden_size\n            number of neurons in hidden layer\n        dr\n            dropout rate\n        \"\"\"\n        super(CudnnLstm, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.dr = dr\n        self.w_ih = Parameter(torch.Tensor(hidden_size * 4, input_size))\n        self.w_hh = Parameter(torch.Tensor(hidden_size * 4, hidden_size))\n        self.b_ih = Parameter(torch.Tensor(hidden_size * 4))\n        self.b_hh = Parameter(torch.Tensor(hidden_size * 4))\n        self._all_weights = [[\"w_ih\", \"w_hh\", \"b_ih\", \"b_hh\"]]\n        # self.cuda()\n        # set the mask\n        self.reset_mask()\n        # initialize the weights and bias of the model\n        self.reset_parameters()\n\n    def _apply(self, fn):\n        \"\"\"just use the default _apply function\n\n        Parameters\n        ----------\n        fn : function\n            _description_\n\n        Returns\n        -------\n        _type_\n            _description_\n        \"\"\"\n        # _apply is always recursively applied to all submodules and the module itself such as move all to GPU\n        return super()._apply(fn)\n\n    def __setstate__(self, d):\n        \"\"\"a python magic function to set the state of the object used for deserialization\n\n        Parameters\n        ----------\n        d : _type_\n            _description_\n        \"\"\"\n        super().__setstate__(d)\n        # set a default value for _data_ptrs\n        self.__dict__.setdefault(\"_data_ptrs\", [])\n        if \"all_weights\" in d:\n            self._all_weights = d[\"all_weights\"]\n        if isinstance(self._all_weights[0][0], str):\n            return\n        self._all_weights = [[\"w_ih\", \"w_hh\", \"b_ih\", \"b_hh\"]]\n\n    def reset_mask(self):\n        \"\"\"generate mask for dropout\"\"\"\n        self.mask_w_ih = create_mask(self.w_ih, self.dr)\n        self.mask_w_hh = create_mask(self.w_hh, self.dr)\n\n    def reset_parameters(self):\n        \"\"\"initialize the weights and bias of the model using Xavier initialization\"\"\"\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            # uniform distribution between -stdv and stdv for the weights and bias initialization\n            weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, hx=None, cx=None, do_drop_mc=False, dropout_false=False):\n        # dropout_false: it will ensure do_drop is false, unless do_drop_mc is true\n        if dropout_false and (not do_drop_mc):\n            do_drop = False\n        elif self.dr &gt; 0 and (do_drop_mc is True or self.training is True):\n            # if train mode and set self.dr &gt; 0, then do_drop is true\n            # so each time the model forward function is called, the dropout is applied\n            do_drop = True\n        else:\n            do_drop = False\n        # input must be a tensor with shape (seq_len, batch, input_size)\n        batch_size = input.size(1)\n\n        if hx is None:\n            hx = input.new_zeros(1, batch_size, self.hidden_size, requires_grad=False)\n        if cx is None:\n            cx = input.new_zeros(1, batch_size, self.hidden_size, requires_grad=False)\n\n        # handle = torch.backends.cudnn.get_handle()\n        if do_drop is True:\n            # cuDNN backend - disabled flat weight\n            # NOTE: each time the mask is newly generated, so for each batch the mask is different\n            self.reset_mask()\n            # apply the mask to the weights\n            weight = [\n                DropMask.apply(self.w_ih, self.mask_w_ih, True),\n                DropMask.apply(self.w_hh, self.mask_w_hh, True),\n                self.b_ih,\n                self.b_hh,\n            ]\n        else:\n            weight = [self.w_ih, self.w_hh, self.b_ih, self.b_hh]\n        if torch.__version__ &lt; \"1.8\":\n            output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n                input,\n                weight,\n                4,\n                None,\n                hx,\n                cx,\n                2,  # 2 means LSTM\n                self.hidden_size,\n                1,\n                False,\n                0,\n                self.training,\n                False,\n                (),\n                None,\n            )\n        else:\n            output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n                input,\n                weight,\n                4,\n                None,\n                hx,\n                cx,\n                2,  # 2 means LSTM\n                self.hidden_size,\n                0,\n                1,\n                False,\n                0,\n                self.training,\n                False,\n                (),\n                None,\n            )\n        return output, (hy, cy)\n\n    @property\n    def all_weights(self):\n        \"\"\"return all weights and bias of the model as a list\"\"\"\n        # getattr() is used to get the value of an object's attribute\n        return [\n            [getattr(self, weight) for weight in weights]\n            for weights in self._all_weights\n        ]\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstm.all_weights","title":"<code>all_weights</code>  <code>property</code> <code>readonly</code>","text":"<p>return all weights and bias of the model as a list</p>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstm.__init__","title":"<code>__init__(self, *, input_size, hidden_size, dr=0.5)</code>  <code>special</code>","text":""},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstm.__init__--parameters","title":"Parameters","text":"<p>input_size     number of neurons in input layer hidden_size     number of neurons in hidden layer dr     dropout rate</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def __init__(self, *, input_size, hidden_size, dr=0.5):\n    \"\"\"\n\n    Parameters\n    ----------\n    input_size\n        number of neurons in input layer\n    hidden_size\n        number of neurons in hidden layer\n    dr\n        dropout rate\n    \"\"\"\n    super(CudnnLstm, self).__init__()\n    self.input_size = input_size\n    self.hidden_size = hidden_size\n    self.dr = dr\n    self.w_ih = Parameter(torch.Tensor(hidden_size * 4, input_size))\n    self.w_hh = Parameter(torch.Tensor(hidden_size * 4, hidden_size))\n    self.b_ih = Parameter(torch.Tensor(hidden_size * 4))\n    self.b_hh = Parameter(torch.Tensor(hidden_size * 4))\n    self._all_weights = [[\"w_ih\", \"w_hh\", \"b_ih\", \"b_hh\"]]\n    # self.cuda()\n    # set the mask\n    self.reset_mask()\n    # initialize the weights and bias of the model\n    self.reset_parameters()\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstm.__setstate__","title":"<code>__setstate__(self, d)</code>  <code>special</code>","text":"<p>a python magic function to set the state of the object used for deserialization</p>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstm.__setstate__--parameters","title":"Parameters","text":"<p>d : type description</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def __setstate__(self, d):\n    \"\"\"a python magic function to set the state of the object used for deserialization\n\n    Parameters\n    ----------\n    d : _type_\n        _description_\n    \"\"\"\n    super().__setstate__(d)\n    # set a default value for _data_ptrs\n    self.__dict__.setdefault(\"_data_ptrs\", [])\n    if \"all_weights\" in d:\n        self._all_weights = d[\"all_weights\"]\n    if isinstance(self._all_weights[0][0], str):\n        return\n    self._all_weights = [[\"w_ih\", \"w_hh\", \"b_ih\", \"b_hh\"]]\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstm.forward","title":"<code>forward(self, input, hx=None, cx=None, do_drop_mc=False, dropout_false=False)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def forward(self, input, hx=None, cx=None, do_drop_mc=False, dropout_false=False):\n    # dropout_false: it will ensure do_drop is false, unless do_drop_mc is true\n    if dropout_false and (not do_drop_mc):\n        do_drop = False\n    elif self.dr &gt; 0 and (do_drop_mc is True or self.training is True):\n        # if train mode and set self.dr &gt; 0, then do_drop is true\n        # so each time the model forward function is called, the dropout is applied\n        do_drop = True\n    else:\n        do_drop = False\n    # input must be a tensor with shape (seq_len, batch, input_size)\n    batch_size = input.size(1)\n\n    if hx is None:\n        hx = input.new_zeros(1, batch_size, self.hidden_size, requires_grad=False)\n    if cx is None:\n        cx = input.new_zeros(1, batch_size, self.hidden_size, requires_grad=False)\n\n    # handle = torch.backends.cudnn.get_handle()\n    if do_drop is True:\n        # cuDNN backend - disabled flat weight\n        # NOTE: each time the mask is newly generated, so for each batch the mask is different\n        self.reset_mask()\n        # apply the mask to the weights\n        weight = [\n            DropMask.apply(self.w_ih, self.mask_w_ih, True),\n            DropMask.apply(self.w_hh, self.mask_w_hh, True),\n            self.b_ih,\n            self.b_hh,\n        ]\n    else:\n        weight = [self.w_ih, self.w_hh, self.b_ih, self.b_hh]\n    if torch.__version__ &lt; \"1.8\":\n        output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n            input,\n            weight,\n            4,\n            None,\n            hx,\n            cx,\n            2,  # 2 means LSTM\n            self.hidden_size,\n            1,\n            False,\n            0,\n            self.training,\n            False,\n            (),\n            None,\n        )\n    else:\n        output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n            input,\n            weight,\n            4,\n            None,\n            hx,\n            cx,\n            2,  # 2 means LSTM\n            self.hidden_size,\n            0,\n            1,\n            False,\n            0,\n            self.training,\n            False,\n            (),\n            None,\n        )\n    return output, (hy, cy)\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstm.reset_mask","title":"<code>reset_mask(self)</code>","text":"<p>generate mask for dropout</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def reset_mask(self):\n    \"\"\"generate mask for dropout\"\"\"\n    self.mask_w_ih = create_mask(self.w_ih, self.dr)\n    self.mask_w_hh = create_mask(self.w_hh, self.dr)\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstm.reset_parameters","title":"<code>reset_parameters(self)</code>","text":"<p>initialize the weights and bias of the model using Xavier initialization</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def reset_parameters(self):\n    \"\"\"initialize the weights and bias of the model using Xavier initialization\"\"\"\n    stdv = 1.0 / math.sqrt(self.hidden_size)\n    for weight in self.parameters():\n        # uniform distribution between -stdv and stdv for the weights and bias initialization\n        weight.data.uniform_(-stdv, stdv)\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstmModel","title":"<code> CudnnLstmModel            (Module)         </code>","text":"Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>class CudnnLstmModel(nn.Module):\n    def __init__(self, n_input_features, n_output_features, n_hidden_states, dr=0.5):\n        \"\"\"\n        An LSTM model writen by Kuai Fang from this paper: https://doi.org/10.1002/2017GL075619\n\n        only gpu version\n\n        Parameters\n        ----------\n        n_input_features\n            the number of input features\n        n_output_features\n            the number of output features\n        n_hidden_states\n            the number of hidden features\n        dr\n            dropout rate and its default is 0.5\n        \"\"\"\n        super(CudnnLstmModel, self).__init__()\n        self.nx = n_input_features\n        self.ny = n_output_features\n        self.hidden_size = n_hidden_states\n        self.ct = 0\n        self.nLayer = 1\n        self.linearIn = torch.nn.Linear(self.nx, self.hidden_size)\n        self.lstm = CudnnLstm(\n            input_size=self.hidden_size, hidden_size=self.hidden_size, dr=dr\n        )\n        self.linearOut = torch.nn.Linear(self.hidden_size, self.ny)\n\n    def forward(self, x, do_drop_mc=False, dropout_false=False, return_h_c=False):\n        x0 = F.relu(self.linearIn(x))\n        out_lstm, (hn, cn) = self.lstm(\n            x0, do_drop_mc=do_drop_mc, dropout_false=dropout_false\n        )\n        out = self.linearOut(out_lstm)\n        return (out, (hn, cn)) if return_h_c else out\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstmModel.__init__","title":"<code>__init__(self, n_input_features, n_output_features, n_hidden_states, dr=0.5)</code>  <code>special</code>","text":"<p>An LSTM model writen by Kuai Fang from this paper: https://doi.org/10.1002/2017GL075619</p> <p>only gpu version</p>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstmModel.__init__--parameters","title":"Parameters","text":"<p>n_input_features     the number of input features n_output_features     the number of output features n_hidden_states     the number of hidden features dr     dropout rate and its default is 0.5</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def __init__(self, n_input_features, n_output_features, n_hidden_states, dr=0.5):\n    \"\"\"\n    An LSTM model writen by Kuai Fang from this paper: https://doi.org/10.1002/2017GL075619\n\n    only gpu version\n\n    Parameters\n    ----------\n    n_input_features\n        the number of input features\n    n_output_features\n        the number of output features\n    n_hidden_states\n        the number of hidden features\n    dr\n        dropout rate and its default is 0.5\n    \"\"\"\n    super(CudnnLstmModel, self).__init__()\n    self.nx = n_input_features\n    self.ny = n_output_features\n    self.hidden_size = n_hidden_states\n    self.ct = 0\n    self.nLayer = 1\n    self.linearIn = torch.nn.Linear(self.nx, self.hidden_size)\n    self.lstm = CudnnLstm(\n        input_size=self.hidden_size, hidden_size=self.hidden_size, dr=dr\n    )\n    self.linearOut = torch.nn.Linear(self.hidden_size, self.ny)\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstmModel.forward","title":"<code>forward(self, x, do_drop_mc=False, dropout_false=False, return_h_c=False)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def forward(self, x, do_drop_mc=False, dropout_false=False, return_h_c=False):\n    x0 = F.relu(self.linearIn(x))\n    out_lstm, (hn, cn) = self.lstm(\n        x0, do_drop_mc=do_drop_mc, dropout_false=dropout_false\n    )\n    out = self.linearOut(out_lstm)\n    return (out, (hn, cn)) if return_h_c else out\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstmModelLstmKernel","title":"<code> CudnnLstmModelLstmKernel            (Module)         </code>","text":"<p>use a trained/un-trained CudnnLstm as a kernel generator before another CudnnLstm.</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>class CudnnLstmModelLstmKernel(nn.Module):\n    \"\"\"use a trained/un-trained CudnnLstm as a kernel generator before another CudnnLstm.\"\"\"\n\n    def __init__(\n        self,\n        nx,\n        ny,\n        hidden_size,\n        nk=None,\n        hidden_size_later=None,\n        cut=False,\n        dr=0.5,\n        delta_s=False,\n    ):\n        \"\"\"delta_s means we will use the difference of the first lstm's output and the second's as the final output\"\"\"\n        super(CudnnLstmModelLstmKernel, self).__init__()\n        # These three layers are same with CudnnLstmModel to be used for transfer learning or just vanilla-use\n        self.linearIn = torch.nn.Linear(nx, hidden_size)\n        self.lstm = CudnnLstm(input_size=hidden_size, hidden_size=hidden_size, dr=dr)\n        self.linearOut = torch.nn.Linear(hidden_size, ny)\n        # if cut is True, we will only select the final index in nk, and repeat it, then concatenate with x\n        self.cut = cut\n        # the second lstm has more input than the previous\n        if nk is None:\n            nk = ny\n        if hidden_size_later is None:\n            hidden_size_later = hidden_size\n        self.linear_in_later = torch.nn.Linear(nx + nk, hidden_size_later)\n        self.lstm_later = CudnnLstm(\n            input_size=hidden_size_later, hidden_size=hidden_size_later, dr=dr\n        )\n        self.linear_out_later = torch.nn.Linear(hidden_size_later, ny)\n\n        self.delta_s = delta_s\n        # when delta_s is true, cut cannot be true, because they have to have same number params\n        assert not (cut and delta_s)\n\n    def forward(self, x, do_drop_mc=False, dropout_false=False):\n        x0 = F.relu(self.linearIn(x))\n        out_lstm1, (hn1, cn1) = self.lstm(\n            x0, do_drop_mc=do_drop_mc, dropout_false=dropout_false\n        )\n        gen = self.linearOut(out_lstm1)\n        if self.cut:\n            gen = gen[-1, :, :].repeat(x.shape[0], 1, 1)\n        x1 = torch.cat((x, gen), dim=len(gen.shape) - 1)\n        x2 = F.relu(self.linear_in_later(x1))\n        out_lstm2, (hn2, cn2) = self.lstm_later(\n            x2, do_drop_mc=do_drop_mc, dropout_false=dropout_false\n        )\n        out = self.linear_out_later(out_lstm2)\n        return gen - out if self.delta_s else (out, gen)\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstmModelLstmKernel.__init__","title":"<code>__init__(self, nx, ny, hidden_size, nk=None, hidden_size_later=None, cut=False, dr=0.5, delta_s=False)</code>  <code>special</code>","text":"<p>delta_s means we will use the difference of the first lstm's output and the second's as the final output</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def __init__(\n    self,\n    nx,\n    ny,\n    hidden_size,\n    nk=None,\n    hidden_size_later=None,\n    cut=False,\n    dr=0.5,\n    delta_s=False,\n):\n    \"\"\"delta_s means we will use the difference of the first lstm's output and the second's as the final output\"\"\"\n    super(CudnnLstmModelLstmKernel, self).__init__()\n    # These three layers are same with CudnnLstmModel to be used for transfer learning or just vanilla-use\n    self.linearIn = torch.nn.Linear(nx, hidden_size)\n    self.lstm = CudnnLstm(input_size=hidden_size, hidden_size=hidden_size, dr=dr)\n    self.linearOut = torch.nn.Linear(hidden_size, ny)\n    # if cut is True, we will only select the final index in nk, and repeat it, then concatenate with x\n    self.cut = cut\n    # the second lstm has more input than the previous\n    if nk is None:\n        nk = ny\n    if hidden_size_later is None:\n        hidden_size_later = hidden_size\n    self.linear_in_later = torch.nn.Linear(nx + nk, hidden_size_later)\n    self.lstm_later = CudnnLstm(\n        input_size=hidden_size_later, hidden_size=hidden_size_later, dr=dr\n    )\n    self.linear_out_later = torch.nn.Linear(hidden_size_later, ny)\n\n    self.delta_s = delta_s\n    # when delta_s is true, cut cannot be true, because they have to have same number params\n    assert not (cut and delta_s)\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstmModelLstmKernel.forward","title":"<code>forward(self, x, do_drop_mc=False, dropout_false=False)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def forward(self, x, do_drop_mc=False, dropout_false=False):\n    x0 = F.relu(self.linearIn(x))\n    out_lstm1, (hn1, cn1) = self.lstm(\n        x0, do_drop_mc=do_drop_mc, dropout_false=dropout_false\n    )\n    gen = self.linearOut(out_lstm1)\n    if self.cut:\n        gen = gen[-1, :, :].repeat(x.shape[0], 1, 1)\n    x1 = torch.cat((x, gen), dim=len(gen.shape) - 1)\n    x2 = F.relu(self.linear_in_later(x1))\n    out_lstm2, (hn2, cn2) = self.lstm_later(\n        x2, do_drop_mc=do_drop_mc, dropout_false=dropout_false\n    )\n    out = self.linear_out_later(out_lstm2)\n    return gen - out if self.delta_s else (out, gen)\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstmModelMultiOutput","title":"<code> CudnnLstmModelMultiOutput            (Module)         </code>","text":"Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>class CudnnLstmModelMultiOutput(nn.Module):\n    def __init__(\n        self,\n        n_input_features,\n        n_output_features,\n        n_hidden_states,\n        layer_hidden_size=(128, 64),\n        dr=0.5,\n        dr_hidden=0.0,\n    ):\n        \"\"\"\n        Multiple output CudnnLSTM.\n\n        It has multiple output layers, each for one output, so that we can easily freeze any output layer.\n\n        Parameters\n        ----------\n        n_input_features\n            the size of input features\n        n_output_features\n            the size of output features; in this model, we set different nonlinear layer for each output\n        n_hidden_states\n            the size of LSTM's hidden features\n        layer_hidden_size\n            hidden_size for multi-layers\n        dr\n            dropout rate\n        dr_hidden\n            dropout rates of hidden layers\n        \"\"\"\n        super(CudnnLstmModelMultiOutput, self).__init__()\n        self.ct = 0\n        multi_layers = torch.nn.ModuleList()\n        for i in range(n_output_features):\n            multi_layers.add_module(\n                \"layer%d\" % (i + 1),\n                SimpleAnn(n_hidden_states, 1, layer_hidden_size, dr=dr_hidden),\n            )\n        self.multi_layers = multi_layers\n        self.linearIn = torch.nn.Linear(n_input_features, n_hidden_states)\n        self.lstm = CudnnLstm(\n            input_size=n_hidden_states, hidden_size=n_hidden_states, dr=dr\n        )\n\n    def forward(self, x, do_drop_mc=False, dropout_false=False, return_h_c=False):\n        x0 = F.relu(self.linearIn(x))\n        out_lstm, (hn, cn) = self.lstm(\n            x0, do_drop_mc=do_drop_mc, dropout_false=dropout_false\n        )\n        outs = [mod(out_lstm) for mod in self.multi_layers]\n        final = torch.cat(outs, dim=-1)\n        return (final, (hn, cn)) if return_h_c else final\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstmModelMultiOutput.__init__","title":"<code>__init__(self, n_input_features, n_output_features, n_hidden_states, layer_hidden_size=(128, 64), dr=0.5, dr_hidden=0.0)</code>  <code>special</code>","text":"<p>Multiple output CudnnLSTM.</p> <p>It has multiple output layers, each for one output, so that we can easily freeze any output layer.</p>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstmModelMultiOutput.__init__--parameters","title":"Parameters","text":"<p>n_input_features     the size of input features n_output_features     the size of output features; in this model, we set different nonlinear layer for each output n_hidden_states     the size of LSTM's hidden features layer_hidden_size     hidden_size for multi-layers dr     dropout rate dr_hidden     dropout rates of hidden layers</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def __init__(\n    self,\n    n_input_features,\n    n_output_features,\n    n_hidden_states,\n    layer_hidden_size=(128, 64),\n    dr=0.5,\n    dr_hidden=0.0,\n):\n    \"\"\"\n    Multiple output CudnnLSTM.\n\n    It has multiple output layers, each for one output, so that we can easily freeze any output layer.\n\n    Parameters\n    ----------\n    n_input_features\n        the size of input features\n    n_output_features\n        the size of output features; in this model, we set different nonlinear layer for each output\n    n_hidden_states\n        the size of LSTM's hidden features\n    layer_hidden_size\n        hidden_size for multi-layers\n    dr\n        dropout rate\n    dr_hidden\n        dropout rates of hidden layers\n    \"\"\"\n    super(CudnnLstmModelMultiOutput, self).__init__()\n    self.ct = 0\n    multi_layers = torch.nn.ModuleList()\n    for i in range(n_output_features):\n        multi_layers.add_module(\n            \"layer%d\" % (i + 1),\n            SimpleAnn(n_hidden_states, 1, layer_hidden_size, dr=dr_hidden),\n        )\n    self.multi_layers = multi_layers\n    self.linearIn = torch.nn.Linear(n_input_features, n_hidden_states)\n    self.lstm = CudnnLstm(\n        input_size=n_hidden_states, hidden_size=n_hidden_states, dr=dr\n    )\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.CudnnLstmModelMultiOutput.forward","title":"<code>forward(self, x, do_drop_mc=False, dropout_false=False, return_h_c=False)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def forward(self, x, do_drop_mc=False, dropout_false=False, return_h_c=False):\n    x0 = F.relu(self.linearIn(x))\n    out_lstm, (hn, cn) = self.lstm(\n        x0, do_drop_mc=do_drop_mc, dropout_false=dropout_false\n    )\n    outs = [mod(out_lstm) for mod in self.multi_layers]\n    final = torch.cat(outs, dim=-1)\n    return (final, (hn, cn)) if return_h_c else final\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.LinearCudnnLstmModel","title":"<code> LinearCudnnLstmModel            (CudnnLstmModel)         </code>","text":"<p>This model is nonlinear layer + CudnnLSTM/CudnnLstm-MultiOutput-Model. kai_tl: model from this paper by Ma et al. -- https://doi.org/10.1029/2020WR028600</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>class LinearCudnnLstmModel(CudnnLstmModel):\n    \"\"\"This model is nonlinear layer + CudnnLSTM/CudnnLstm-MultiOutput-Model.\n    kai_tl: model from this paper by Ma et al. -- https://doi.org/10.1029/2020WR028600\n    \"\"\"\n\n    def __init__(self, linear_size, **kwargs):\n        \"\"\"\n\n        Parameters\n        ----------\n        linear_size\n            the number of input features for the first input linear layer\n        \"\"\"\n        super(LinearCudnnLstmModel, self).__init__(**kwargs)\n        self.former_linear = torch.nn.Linear(linear_size, kwargs[\"n_input_features\"])\n\n    def forward(self, x, do_drop_mc=False, dropout_false=False):\n        x0 = F.relu(self.former_linear(x))\n        return super(LinearCudnnLstmModel, self).forward(\n            x0, do_drop_mc=do_drop_mc, dropout_false=dropout_false\n        )\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.LinearCudnnLstmModel.__init__","title":"<code>__init__(self, linear_size, **kwargs)</code>  <code>special</code>","text":""},{"location":"api/models/#torchhydro.models.cudnnlstm.LinearCudnnLstmModel.__init__--parameters","title":"Parameters","text":"<p>linear_size     the number of input features for the first input linear layer</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def __init__(self, linear_size, **kwargs):\n    \"\"\"\n\n    Parameters\n    ----------\n    linear_size\n        the number of input features for the first input linear layer\n    \"\"\"\n    super(LinearCudnnLstmModel, self).__init__(**kwargs)\n    self.former_linear = torch.nn.Linear(linear_size, kwargs[\"n_input_features\"])\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.LinearCudnnLstmModel.forward","title":"<code>forward(self, x, do_drop_mc=False, dropout_false=False)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def forward(self, x, do_drop_mc=False, dropout_false=False):\n    x0 = F.relu(self.former_linear(x))\n    return super(LinearCudnnLstmModel, self).forward(\n        x0, do_drop_mc=do_drop_mc, dropout_false=dropout_false\n    )\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.LstmCellTied","title":"<code> LstmCellTied            (Module)         </code>","text":"<p>LSTM with dropout implemented by Kuai Fang: https://github.com/mhpi/hydroDL/blob/release/hydroDL/model/rnn.py</p> <p>the name of \"Tied\" comes from this paper: http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.pdf which means the weights of all gates will be tied together to be used (eq. 6 in this paper). this code is mainly used as a CPU version of CudnnLstm</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>class LstmCellTied(nn.Module):\n    \"\"\"\n    LSTM with dropout implemented by Kuai Fang: https://github.com/mhpi/hydroDL/blob/release/hydroDL/model/rnn.py\n\n    the name of \"Tied\" comes from this paper:\n    http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.pdf\n    which means the weights of all gates will be tied together to be used (eq. 6 in this paper).\n    this code is mainly used as a CPU version of CudnnLstm\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        input_size,\n        hidden_size,\n        mode=\"train\",\n        dr=0.5,\n        dr_method=\"drX+drW+drC\",\n        gpu=1,\n    ):\n        super(LstmCellTied, self).__init__()\n\n        self.inputSize = input_size\n        self.hiddenSize = hidden_size\n        self.dr = dr\n\n        self.w_ih = Parameter(torch.Tensor(hidden_size * 4, input_size))\n        self.w_hh = Parameter(torch.Tensor(hidden_size * 4, hidden_size))\n        self.b_ih = Parameter(torch.Tensor(hidden_size * 4))\n        self.b_hh = Parameter(torch.Tensor(hidden_size * 4))\n\n        self.drMethod = dr_method.split(\"+\")\n        self.gpu = gpu\n        self.mode = mode\n        if mode == \"train\":\n            self.train(mode=True)\n        elif mode in [\"test\", \"drMC\"]:\n            self.train(mode=False)\n        if gpu &gt;= 0:\n            self = self.cuda()\n            self.is_cuda = True\n        else:\n            self.is_cuda = False\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.hiddenSize)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, stdv)\n\n    def reset_mask(self, x, h, c):\n        self.mask_x = create_mask(x, self.dr)\n        self.mask_h = create_mask(h, self.dr)\n        self.mask_c = create_mask(c, self.dr)\n        self.mask_w_ih = create_mask(self.w_ih, self.dr)\n        self.mask_w_hh = create_mask(self.w_hh, self.dr)\n\n    def forward(self, x, hidden, *, do_reset_mask=True, do_drop_mc=False):\n        do_drop = self.dr &gt; 0 and (do_drop_mc is True or self.training is True)\n        batch_size = x.size(0)\n        h0, c0 = hidden\n        if h0 is None:\n            h0 = x.new_zeros(batch_size, self.hiddenSize, requires_grad=False)\n        if c0 is None:\n            c0 = x.new_zeros(batch_size, self.hiddenSize, requires_grad=False)\n\n        if self.dr &gt; 0 and self.training is True and do_reset_mask is True:\n            self.reset_mask(x, h0, c0)\n\n        if do_drop and \"drH\" in self.drMethod:\n            h0 = DropMask.apply(h0, self.mask_h, True)\n\n        if do_drop and \"drX\" in self.drMethod:\n            x = DropMask.apply(x, self.mask_x, True)\n\n        if do_drop and \"drW\" in self.drMethod:\n            w_ih = DropMask.apply(self.w_ih, self.mask_w_ih, True)\n            w_hh = DropMask.apply(self.w_hh, self.mask_w_hh, True)\n        else:\n            # self.w are parameters, while w are not\n            w_ih = self.w_ih\n            w_hh = self.w_hh\n\n        gates = F.linear(x, w_ih, self.b_ih) + F.linear(h0, w_hh, self.b_hh)\n        gate_i, gate_f, gate_c, gate_o = gates.chunk(4, 1)\n\n        gate_i = torch.sigmoid(gate_i)\n        gate_f = torch.sigmoid(gate_f)\n        gate_c = torch.tanh(gate_c)\n        gate_o = torch.sigmoid(gate_o)\n\n        if self.training is True and \"drC\" in self.drMethod:\n            gate_c = gate_c.mul(self.mask_c)\n\n        c1 = (gate_f * c0) + (gate_i * gate_c)\n        h1 = gate_o * torch.tanh(c1)\n\n        return h1, c1\n</code></pre>"},{"location":"api/models/#torchhydro.models.cudnnlstm.LstmCellTied.forward","title":"<code>forward(self, x, hidden, *, do_reset_mask=True, do_drop_mc=False)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/cudnnlstm.py</code> <pre><code>def forward(self, x, hidden, *, do_reset_mask=True, do_drop_mc=False):\n    do_drop = self.dr &gt; 0 and (do_drop_mc is True or self.training is True)\n    batch_size = x.size(0)\n    h0, c0 = hidden\n    if h0 is None:\n        h0 = x.new_zeros(batch_size, self.hiddenSize, requires_grad=False)\n    if c0 is None:\n        c0 = x.new_zeros(batch_size, self.hiddenSize, requires_grad=False)\n\n    if self.dr &gt; 0 and self.training is True and do_reset_mask is True:\n        self.reset_mask(x, h0, c0)\n\n    if do_drop and \"drH\" in self.drMethod:\n        h0 = DropMask.apply(h0, self.mask_h, True)\n\n    if do_drop and \"drX\" in self.drMethod:\n        x = DropMask.apply(x, self.mask_x, True)\n\n    if do_drop and \"drW\" in self.drMethod:\n        w_ih = DropMask.apply(self.w_ih, self.mask_w_ih, True)\n        w_hh = DropMask.apply(self.w_hh, self.mask_w_hh, True)\n    else:\n        # self.w are parameters, while w are not\n        w_ih = self.w_ih\n        w_hh = self.w_hh\n\n    gates = F.linear(x, w_ih, self.b_ih) + F.linear(h0, w_hh, self.b_hh)\n    gate_i, gate_f, gate_c, gate_o = gates.chunk(4, 1)\n\n    gate_i = torch.sigmoid(gate_i)\n    gate_f = torch.sigmoid(gate_f)\n    gate_c = torch.tanh(gate_c)\n    gate_o = torch.sigmoid(gate_o)\n\n    if self.training is True and \"drC\" in self.drMethod:\n        gate_c = gate_c.mul(self.mask_c)\n\n    c1 = (gate_f * c0) + (gate_i * gate_c)\n    h1 = gate_o * torch.tanh(c1)\n\n    return h1, c1\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4gr4j","title":"<code>dpl4gr4j</code>","text":"<p>Simulates streamflow over time using the model logic from GR4J as implemented in PyTorch. This function can be used to offer up the functionality of GR4J with added gradient information.</p>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.DplAnnGr4j","title":"<code> DplAnnGr4j            (Module)         </code>","text":"Source code in <code>torchhydro/models/dpl4gr4j.py</code> <pre><code>class DplAnnGr4j(nn.Module):\n    def __init__(\n        self,\n        n_input_features: int,\n        n_output_features: int,\n        n_hidden_states: Union[int, tuple, list],\n        warmup_length: int,\n        param_limit_func=\"sigmoid\",\n        param_test_way=\"final\",\n    ):\n        \"\"\"\n        Differential Parameter learning model only with attributes as DL model's input: ANN -&gt; Param -&gt; Gr4j\n\n        The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z\n\n        Parameters\n        ----------\n        n_input_features\n            the number of input features of ANN\n        n_output_features\n            the number of output features of ANN, and it should be equal to the number of learning parameters in XAJ\n        n_hidden_states\n            the number of hidden features of ANN; it could be Union[int, tuple, list]\n        warmup_length\n            the length of warmup periods;\n            hydrologic models need a warmup period to generate reasonable initial state values\n        param_limit_func\n            function used to limit the range of params; now it is sigmoid or clamp function\n        param_test_way\n            how we use parameters from dl model when testing;\n            now we have three ways:\n            1. \"final\" -- use the final period's parameter for each period\n            2. \"mean_time\" -- Mean values of all periods' parameters is used\n            3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used\n        \"\"\"\n        super(DplAnnGr4j, self).__init__()\n        self.dl_model = SimpleAnn(n_input_features, n_output_features, n_hidden_states)\n        self.pb_model = Gr4j4Dpl(warmup_length)\n        self.param_func = param_limit_func\n        self.param_test_way = param_test_way\n\n    def forward(self, x, z):\n        \"\"\"\n        Differential parameter learning\n\n        z (normalized input) -&gt; ANN -&gt; param -&gt; + x (not normalized) -&gt; gr4j -&gt; q\n        Parameters will be denormalized in gr4j model\n\n        Parameters\n        ----------\n        x\n            not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n        z\n            normalized data used for DL model; a 2-dim tensor. [batch, feature]\n\n        Returns\n        -------\n        torch.Tensor\n            one time forward result\n        \"\"\"\n        return ann_pbm(self.dl_model, self.pb_model, self.param_func, x, z)\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.DplAnnGr4j.__init__","title":"<code>__init__(self, n_input_features, n_output_features, n_hidden_states, warmup_length, param_limit_func='sigmoid', param_test_way='final')</code>  <code>special</code>","text":"<p>Differential Parameter learning model only with attributes as DL model's input: ANN -&gt; Param -&gt; Gr4j</p> <p>The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z</p>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.DplAnnGr4j.__init__--parameters","title":"Parameters","text":"<p>n_input_features     the number of input features of ANN n_output_features     the number of output features of ANN, and it should be equal to the number of learning parameters in XAJ n_hidden_states     the number of hidden features of ANN; it could be Union[int, tuple, list] warmup_length     the length of warmup periods;     hydrologic models need a warmup period to generate reasonable initial state values param_limit_func     function used to limit the range of params; now it is sigmoid or clamp function param_test_way     how we use parameters from dl model when testing;     now we have three ways:     1. \"final\" -- use the final period's parameter for each period     2. \"mean_time\" -- Mean values of all periods' parameters is used     3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used</p> Source code in <code>torchhydro/models/dpl4gr4j.py</code> <pre><code>def __init__(\n    self,\n    n_input_features: int,\n    n_output_features: int,\n    n_hidden_states: Union[int, tuple, list],\n    warmup_length: int,\n    param_limit_func=\"sigmoid\",\n    param_test_way=\"final\",\n):\n    \"\"\"\n    Differential Parameter learning model only with attributes as DL model's input: ANN -&gt; Param -&gt; Gr4j\n\n    The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z\n\n    Parameters\n    ----------\n    n_input_features\n        the number of input features of ANN\n    n_output_features\n        the number of output features of ANN, and it should be equal to the number of learning parameters in XAJ\n    n_hidden_states\n        the number of hidden features of ANN; it could be Union[int, tuple, list]\n    warmup_length\n        the length of warmup periods;\n        hydrologic models need a warmup period to generate reasonable initial state values\n    param_limit_func\n        function used to limit the range of params; now it is sigmoid or clamp function\n    param_test_way\n        how we use parameters from dl model when testing;\n        now we have three ways:\n        1. \"final\" -- use the final period's parameter for each period\n        2. \"mean_time\" -- Mean values of all periods' parameters is used\n        3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used\n    \"\"\"\n    super(DplAnnGr4j, self).__init__()\n    self.dl_model = SimpleAnn(n_input_features, n_output_features, n_hidden_states)\n    self.pb_model = Gr4j4Dpl(warmup_length)\n    self.param_func = param_limit_func\n    self.param_test_way = param_test_way\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.DplAnnGr4j.forward","title":"<code>forward(self, x, z)</code>","text":"<p>Differential parameter learning</p> <p>z (normalized input) -&gt; ANN -&gt; param -&gt; + x (not normalized) -&gt; gr4j -&gt; q Parameters will be denormalized in gr4j model</p>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.DplAnnGr4j.forward--parameters","title":"Parameters","text":"<p>x     not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature] z     normalized data used for DL model; a 2-dim tensor. [batch, feature]</p>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.DplAnnGr4j.forward--returns","title":"Returns","text":"<p>torch.Tensor     one time forward result</p> Source code in <code>torchhydro/models/dpl4gr4j.py</code> <pre><code>def forward(self, x, z):\n    \"\"\"\n    Differential parameter learning\n\n    z (normalized input) -&gt; ANN -&gt; param -&gt; + x (not normalized) -&gt; gr4j -&gt; q\n    Parameters will be denormalized in gr4j model\n\n    Parameters\n    ----------\n    x\n        not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n    z\n        normalized data used for DL model; a 2-dim tensor. [batch, feature]\n\n    Returns\n    -------\n    torch.Tensor\n        one time forward result\n    \"\"\"\n    return ann_pbm(self.dl_model, self.pb_model, self.param_func, x, z)\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.DplLstmGr4j","title":"<code> DplLstmGr4j            (Module)         </code>","text":"Source code in <code>torchhydro/models/dpl4gr4j.py</code> <pre><code>class DplLstmGr4j(nn.Module):\n    def __init__(\n        self,\n        n_input_features,\n        n_output_features,\n        n_hidden_states,\n        warmup_length,\n        param_limit_func=\"sigmoid\",\n        param_test_way=\"final\",\n    ):\n        \"\"\"\n        Differential Parameter learning model: LSTM -&gt; Param -&gt; Gr4j\n\n        The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z\n\n        Parameters\n        ----------\n        n_input_features\n            the number of input features of LSTM\n        n_output_features\n            the number of output features of LSTM, and it should be equal to the number of learning parameters in XAJ\n        n_hidden_states\n            the number of hidden features of LSTM\n        warmup_length\n            the length of warmup periods;\n            hydrologic models need a warmup period to generate reasonable initial state values\n        param_limit_func\n            function used to limit the range of params; now it is sigmoid or clamp function\n        param_test_way\n            how we use parameters from dl model when testing;\n            now we have three ways:\n            1. \"final\" -- use the final period's parameter for each period\n            2. \"mean_time\" -- Mean values of all periods' parameters is used\n            3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used\n        \"\"\"\n        super(DplLstmGr4j, self).__init__()\n        self.dl_model = SimpleLSTM(n_input_features, n_output_features, n_hidden_states)\n        self.pb_model = Gr4j4Dpl(warmup_length)\n        self.param_func = param_limit_func\n        self.param_test_way = param_test_way\n\n    def forward(self, x, z):\n        \"\"\"\n        Differential parameter learning\n\n        z (normalized input) -&gt; lstm -&gt; param -&gt; + x (not normalized) -&gt; gr4j -&gt; q\n        Parameters will be denormalized in gr4j model\n\n        Parameters\n        ----------\n        x\n            not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n        z\n            normalized data used for DL model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n\n        Returns\n        -------\n        torch.Tensor\n            one time forward result\n        \"\"\"\n        return lstm_pbm(self.dl_model, self.pb_model, self.param_func, x, z)\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.DplLstmGr4j.__init__","title":"<code>__init__(self, n_input_features, n_output_features, n_hidden_states, warmup_length, param_limit_func='sigmoid', param_test_way='final')</code>  <code>special</code>","text":"<p>Differential Parameter learning model: LSTM -&gt; Param -&gt; Gr4j</p> <p>The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z</p>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.DplLstmGr4j.__init__--parameters","title":"Parameters","text":"<p>n_input_features     the number of input features of LSTM n_output_features     the number of output features of LSTM, and it should be equal to the number of learning parameters in XAJ n_hidden_states     the number of hidden features of LSTM warmup_length     the length of warmup periods;     hydrologic models need a warmup period to generate reasonable initial state values param_limit_func     function used to limit the range of params; now it is sigmoid or clamp function param_test_way     how we use parameters from dl model when testing;     now we have three ways:     1. \"final\" -- use the final period's parameter for each period     2. \"mean_time\" -- Mean values of all periods' parameters is used     3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used</p> Source code in <code>torchhydro/models/dpl4gr4j.py</code> <pre><code>def __init__(\n    self,\n    n_input_features,\n    n_output_features,\n    n_hidden_states,\n    warmup_length,\n    param_limit_func=\"sigmoid\",\n    param_test_way=\"final\",\n):\n    \"\"\"\n    Differential Parameter learning model: LSTM -&gt; Param -&gt; Gr4j\n\n    The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z\n\n    Parameters\n    ----------\n    n_input_features\n        the number of input features of LSTM\n    n_output_features\n        the number of output features of LSTM, and it should be equal to the number of learning parameters in XAJ\n    n_hidden_states\n        the number of hidden features of LSTM\n    warmup_length\n        the length of warmup periods;\n        hydrologic models need a warmup period to generate reasonable initial state values\n    param_limit_func\n        function used to limit the range of params; now it is sigmoid or clamp function\n    param_test_way\n        how we use parameters from dl model when testing;\n        now we have three ways:\n        1. \"final\" -- use the final period's parameter for each period\n        2. \"mean_time\" -- Mean values of all periods' parameters is used\n        3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used\n    \"\"\"\n    super(DplLstmGr4j, self).__init__()\n    self.dl_model = SimpleLSTM(n_input_features, n_output_features, n_hidden_states)\n    self.pb_model = Gr4j4Dpl(warmup_length)\n    self.param_func = param_limit_func\n    self.param_test_way = param_test_way\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.DplLstmGr4j.forward","title":"<code>forward(self, x, z)</code>","text":"<p>Differential parameter learning</p> <p>z (normalized input) -&gt; lstm -&gt; param -&gt; + x (not normalized) -&gt; gr4j -&gt; q Parameters will be denormalized in gr4j model</p>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.DplLstmGr4j.forward--parameters","title":"Parameters","text":"<p>x     not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature] z     normalized data used for DL model; a sequence-first 3-dim tensor. [sequence, batch, feature]</p>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.DplLstmGr4j.forward--returns","title":"Returns","text":"<p>torch.Tensor     one time forward result</p> Source code in <code>torchhydro/models/dpl4gr4j.py</code> <pre><code>def forward(self, x, z):\n    \"\"\"\n    Differential parameter learning\n\n    z (normalized input) -&gt; lstm -&gt; param -&gt; + x (not normalized) -&gt; gr4j -&gt; q\n    Parameters will be denormalized in gr4j model\n\n    Parameters\n    ----------\n    x\n        not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n    z\n        normalized data used for DL model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n\n    Returns\n    -------\n    torch.Tensor\n        one time forward result\n    \"\"\"\n    return lstm_pbm(self.dl_model, self.pb_model, self.param_func, x, z)\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.Gr4j4Dpl","title":"<code> Gr4j4Dpl            (Module)         </code>","text":"<p>the nn.Module style GR4J model</p> Source code in <code>torchhydro/models/dpl4gr4j.py</code> <pre><code>class Gr4j4Dpl(nn.Module):\n    \"\"\"\n    the nn.Module style GR4J model\n    \"\"\"\n\n    def __init__(self, warmup_length: int):\n        \"\"\"\n        Parameters\n        ----------\n        warmup_length\n            length of warmup period\n        \"\"\"\n        super(Gr4j4Dpl, self).__init__()\n        self.params_names = [\"X1\", \"X2\", \"X3\", \"X4\"]\n        self.x1_scale = [100.0, 1200.0]\n        self.x2_sacle = [-5.0, 3.0]\n        self.x3_scale = [20.0, 300.0]\n        self.x4_scale = [1.1, 2.9]\n        self.warmup_length = warmup_length\n        self.feature_size = 2\n\n    def forward(self, p_and_e, parameters, return_state=False):\n        gr4j_device = p_and_e.device\n        x1 = self.x1_scale[0] + parameters[:, 0] * (self.x1_scale[1] - self.x1_scale[0])\n        x2 = self.x2_sacle[0] + parameters[:, 1] * (self.x2_sacle[1] - self.x2_sacle[0])\n        x3 = self.x3_scale[0] + parameters[:, 2] * (self.x3_scale[1] - self.x3_scale[0])\n        x4 = self.x4_scale[0] + parameters[:, 3] * (self.x4_scale[1] - self.x4_scale[0])\n\n        warmup_length = self.warmup_length\n        if warmup_length &gt; 0:\n            # set no_grad for warmup periods\n            with torch.no_grad():\n                p_and_e_warmup = p_and_e[0:warmup_length, :, :]\n                cal_init = Gr4j4Dpl(0)\n                if cal_init.warmup_length &gt; 0:\n                    raise RuntimeError(\"Please set init model's warmup length to 0!!!\")\n                _, s0, r0 = cal_init(p_and_e_warmup, parameters, return_state=True)\n        else:\n            # use detach func to make wu0 no_grad as it is an initial value\n            s0 = 0.5 * x1.detach()\n            r0 = 0.5 * x3.detach()\n        inputs = p_and_e[warmup_length:, :, :]\n        streamflow_ = torch.full(inputs.shape[:2], 0.0).to(gr4j_device)\n        prs = torch.full(inputs.shape[:2], 0.0).to(gr4j_device)\n        for i in range(inputs.shape[0]):\n            if i == 0:\n                pr, s = production(inputs[i, :, :], x1, s0)\n            else:\n                pr, s = production(inputs[i, :, :], x1, s)\n            prs[i, :] = pr\n        prs_x = torch.unsqueeze(prs, dim=2)\n        conv_q9, conv_q1 = uh_gr4j(x4)\n        q9 = torch.full([inputs.shape[0], inputs.shape[1], 1], 0.0).to(gr4j_device)\n        q1 = torch.full([inputs.shape[0], inputs.shape[1], 1], 0.0).to(gr4j_device)\n        for j in range(inputs.shape[1]):\n            q9[:, j : j + 1, :] = uh_conv(\n                prs_x[:, j : j + 1, :], conv_q9[j].reshape(-1, 1, 1)\n            )\n            q1[:, j : j + 1, :] = uh_conv(\n                prs_x[:, j : j + 1, :], conv_q1[j].reshape(-1, 1, 1)\n            )\n        for i in range(inputs.shape[0]):\n            if i == 0:\n                q, r = routing(q9[i, :, 0], q1[i, :, 0], x2, x3, r0)\n            else:\n                q, r = routing(q9[i, :, 0], q1[i, :, 0], x2, x3, r)\n            streamflow_[i, :] = q\n        streamflow = torch.unsqueeze(streamflow_, dim=2)\n        return (streamflow, s, r) if return_state else streamflow\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.Gr4j4Dpl.__init__","title":"<code>__init__(self, warmup_length)</code>  <code>special</code>","text":""},{"location":"api/models/#torchhydro.models.dpl4gr4j.Gr4j4Dpl.__init__--parameters","title":"Parameters","text":"<p>warmup_length     length of warmup period</p> Source code in <code>torchhydro/models/dpl4gr4j.py</code> <pre><code>def __init__(self, warmup_length: int):\n    \"\"\"\n    Parameters\n    ----------\n    warmup_length\n        length of warmup period\n    \"\"\"\n    super(Gr4j4Dpl, self).__init__()\n    self.params_names = [\"X1\", \"X2\", \"X3\", \"X4\"]\n    self.x1_scale = [100.0, 1200.0]\n    self.x2_sacle = [-5.0, 3.0]\n    self.x3_scale = [20.0, 300.0]\n    self.x4_scale = [1.1, 2.9]\n    self.warmup_length = warmup_length\n    self.feature_size = 2\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.Gr4j4Dpl.forward","title":"<code>forward(self, p_and_e, parameters, return_state=False)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/dpl4gr4j.py</code> <pre><code>def forward(self, p_and_e, parameters, return_state=False):\n    gr4j_device = p_and_e.device\n    x1 = self.x1_scale[0] + parameters[:, 0] * (self.x1_scale[1] - self.x1_scale[0])\n    x2 = self.x2_sacle[0] + parameters[:, 1] * (self.x2_sacle[1] - self.x2_sacle[0])\n    x3 = self.x3_scale[0] + parameters[:, 2] * (self.x3_scale[1] - self.x3_scale[0])\n    x4 = self.x4_scale[0] + parameters[:, 3] * (self.x4_scale[1] - self.x4_scale[0])\n\n    warmup_length = self.warmup_length\n    if warmup_length &gt; 0:\n        # set no_grad for warmup periods\n        with torch.no_grad():\n            p_and_e_warmup = p_and_e[0:warmup_length, :, :]\n            cal_init = Gr4j4Dpl(0)\n            if cal_init.warmup_length &gt; 0:\n                raise RuntimeError(\"Please set init model's warmup length to 0!!!\")\n            _, s0, r0 = cal_init(p_and_e_warmup, parameters, return_state=True)\n    else:\n        # use detach func to make wu0 no_grad as it is an initial value\n        s0 = 0.5 * x1.detach()\n        r0 = 0.5 * x3.detach()\n    inputs = p_and_e[warmup_length:, :, :]\n    streamflow_ = torch.full(inputs.shape[:2], 0.0).to(gr4j_device)\n    prs = torch.full(inputs.shape[:2], 0.0).to(gr4j_device)\n    for i in range(inputs.shape[0]):\n        if i == 0:\n            pr, s = production(inputs[i, :, :], x1, s0)\n        else:\n            pr, s = production(inputs[i, :, :], x1, s)\n        prs[i, :] = pr\n    prs_x = torch.unsqueeze(prs, dim=2)\n    conv_q9, conv_q1 = uh_gr4j(x4)\n    q9 = torch.full([inputs.shape[0], inputs.shape[1], 1], 0.0).to(gr4j_device)\n    q1 = torch.full([inputs.shape[0], inputs.shape[1], 1], 0.0).to(gr4j_device)\n    for j in range(inputs.shape[1]):\n        q9[:, j : j + 1, :] = uh_conv(\n            prs_x[:, j : j + 1, :], conv_q9[j].reshape(-1, 1, 1)\n        )\n        q1[:, j : j + 1, :] = uh_conv(\n            prs_x[:, j : j + 1, :], conv_q1[j].reshape(-1, 1, 1)\n        )\n    for i in range(inputs.shape[0]):\n        if i == 0:\n            q, r = routing(q9[i, :, 0], q1[i, :, 0], x2, x3, r0)\n        else:\n            q, r = routing(q9[i, :, 0], q1[i, :, 0], x2, x3, r)\n        streamflow_[i, :] = q\n    streamflow = torch.unsqueeze(streamflow_, dim=2)\n    return (streamflow, s, r) if return_state else streamflow\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.calculate_evap_store","title":"<code>calculate_evap_store(s, evap_net, x1)</code>","text":"<p>Calculates the amount of evaporation out of the storage reservoir.</p> Source code in <code>torchhydro/models/dpl4gr4j.py</code> <pre><code>def calculate_evap_store(s, evap_net, x1):\n    \"\"\"Calculates the amount of evaporation out of the storage reservoir.\"\"\"\n    n = s * (2.0 - s / x1) * torch.tanh(evap_net / x1)\n    d = 1.0 + (1.0 - s / x1) * torch.tanh(evap_net / x1)\n    return n / d\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.calculate_perc","title":"<code>calculate_perc(current_store, x1)</code>","text":"<p>Calculates the percolation from the storage reservoir into streamflow.</p> Source code in <code>torchhydro/models/dpl4gr4j.py</code> <pre><code>def calculate_perc(current_store, x1):\n    \"\"\"Calculates the percolation from the storage reservoir into streamflow.\"\"\"\n    return current_store * (\n        1.0 - (1.0 + (4.0 / 9.0 * current_store / x1) ** 4) ** -0.25\n    )\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.calculate_precip_store","title":"<code>calculate_precip_store(s, precip_net, x1)</code>","text":"<p>Calculates the amount of rainfall which enters the storage reservoir.</p> Source code in <code>torchhydro/models/dpl4gr4j.py</code> <pre><code>def calculate_precip_store(s, precip_net, x1):\n    \"\"\"Calculates the amount of rainfall which enters the storage reservoir.\"\"\"\n    n = x1 * (1.0 - (s / x1) ** 2) * torch.tanh(precip_net / x1)\n    d = 1.0 + (s / x1) * torch.tanh(precip_net / x1)\n    return n / d\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.production","title":"<code>production(p_and_e, x1, s_level=None)</code>","text":"<p>an one-step calculation for production store in GR4J the dimension of the cell: [batch, feature]</p>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.production--parameters","title":"Parameters","text":"<p>p_and_e     P is pe[:, 0] and E is pe[:, 1]; similar with the \"input\" in the RNNCell !!! x1     Storage reservoir parameter; s_level     s_level means S in the GR4J Model; similar with the \"hx\" in the RNNCell     Initial value of storage in the storage reservoir.</p>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.production--returns","title":"Returns","text":"<p>tuple     contains the Pr and updated S</p> Source code in <code>torchhydro/models/dpl4gr4j.py</code> <pre><code>def production(\n    p_and_e: Tensor, x1: Tensor, s_level: Optional[Tensor] = None\n) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"\n    an one-step calculation for production store in GR4J\n    the dimension of the cell: [batch, feature]\n\n    Parameters\n    ----------\n    p_and_e\n        P is pe[:, 0] and E is pe[:, 1]; similar with the \"input\" in the RNNCell\n    x1:\n        Storage reservoir parameter;\n    s_level\n        s_level means S in the GR4J Model; similar with the \"hx\" in the RNNCell\n        Initial value of storage in the storage reservoir.\n\n    Returns\n    -------\n    tuple\n        contains the Pr and updated S\n    \"\"\"\n    gr4j_device = p_and_e.device\n    # Calculate net precipitation and evapotranspiration\n    precip_difference = p_and_e[:, 0] - p_and_e[:, 1]\n    precip_net = torch.maximum(precip_difference, Tensor([0.0]).to(gr4j_device))\n    evap_net = torch.maximum(-precip_difference, Tensor([0.0]).to(gr4j_device))\n\n    if s_level is None:\n        s_level = 0.6 * (x1.detach())\n\n    # s_level should not be larger than x1\n    s_level = torch.clamp(s_level, torch.full(s_level.shape, 0.0).to(gr4j_device), x1)\n\n    # Calculate the fraction of net precipitation that is stored\n    precip_store = calculate_precip_store(s_level, precip_net, x1)\n\n    # Calculate the amount of evaporation from storage\n    evap_store = calculate_evap_store(s_level, evap_net, x1)\n\n    # Update the storage by adding effective precipitation and\n    # removing evaporation\n    s_update = s_level - evap_store + precip_store\n    # s_level should not be larger than self.x1\n    s_update = torch.clamp(\n        s_update, torch.full(s_update.shape, 0.0).to(gr4j_device), x1\n    )\n\n    # Update the storage again to reflect percolation out of the store\n    perc = calculate_perc(s_update, x1)\n    s_update = s_update - perc\n    # perc is always lower than S because of the calculation itself, so we don't need clamp here anymore.\n\n    # The precip. for routing is the sum of the rainfall which\n    # did not make it to storage and the percolation from the store\n    current_runoff = perc + (precip_net - precip_store)\n    return current_runoff, s_update\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.routing","title":"<code>routing(q9, q1, x2, x3, r_level=None)</code>","text":"<p>the GR4J routing-module unit cell for time-sequence loop</p>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.routing--parameters","title":"Parameters","text":"<p>q9</p> <p>q1</p> <p>x2     Catchment water exchange parameter x3     Routing reservoir parameters r_level     Beginning value of storage in the routing reservoir.</p>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.routing--returns","title":"Returns","text":"Source code in <code>torchhydro/models/dpl4gr4j.py</code> <pre><code>def routing(q9: Tensor, q1: Tensor, x2, x3, r_level: Optional[Tensor] = None):\n    \"\"\"\n    the GR4J routing-module unit cell for time-sequence loop\n\n    Parameters\n    ----------\n    q9\n\n    q1\n\n    x2\n        Catchment water exchange parameter\n    x3\n        Routing reservoir parameters\n    r_level\n        Beginning value of storage in the routing reservoir.\n\n    Returns\n    -------\n\n    \"\"\"\n    gr4j_device = q9.device\n    if r_level is None:\n        r_level = 0.7 * (x3.detach())\n    # r_level should not be larger than self.x3\n    r_level = torch.clamp(r_level, torch.full(r_level.shape, 0.0).to(gr4j_device), x3)\n    groundwater_ex = x2 * (r_level / x3) ** 3.5\n    r_updated = torch.maximum(\n        torch.full(r_level.shape, 0.0).to(gr4j_device), r_level + q9 + groundwater_ex\n    )\n\n    qr = r_updated * (1.0 - (1.0 + (r_updated / x3) ** 4) ** -0.25)\n    r_updated = r_updated - qr\n\n    qd = torch.maximum(\n        torch.full(groundwater_ex.shape, 0.0).to(gr4j_device), q1 + groundwater_ex\n    )\n    q = qr + qd\n    return q, r_updated\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.uh_gr4j","title":"<code>uh_gr4j(x4)</code>","text":"<p>Generate the convolution kernel for the convolution operation in routing module of GR4J</p>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.uh_gr4j--parameters","title":"Parameters","text":"<p>x4     the dim of x4 is [batch]</p>"},{"location":"api/models/#torchhydro.models.dpl4gr4j.uh_gr4j--returns","title":"Returns","text":"<p>list     UH1s and UH2s for all basins</p> Source code in <code>torchhydro/models/dpl4gr4j.py</code> <pre><code>def uh_gr4j(x4):\n    \"\"\"\n    Generate the convolution kernel for the convolution operation in routing module of GR4J\n\n    Parameters\n    ----------\n    x4\n        the dim of x4 is [batch]\n\n    Returns\n    -------\n    list\n        UH1s and UH2s for all basins\n    \"\"\"\n    gr4j_device = x4.device\n    uh1_ordinates = []\n    uh2_ordinates = []\n    for i in range(len(x4)):\n        # for SH1, the pieces are: 0, 0&lt;t&lt;x4, t&gt;=x4\n        uh1_ordinates_t1 = torch.arange(\n            0.0, torch.ceil(x4[i]).detach().cpu().numpy().item()\n        ).to(gr4j_device)\n        uh1_ordinates_t = torch.arange(\n            1.0, torch.ceil(x4[i] + 1.0).detach().cpu().numpy().item()\n        ).to(gr4j_device)\n        # for SH2, the pieces are: 0, 0&lt;t&lt;=x4, x4&lt;t&lt;2x4, t&gt;=2x4\n        uh2_ords_t1_seq_x4 = torch.arange(\n            0.0, torch.floor(x4[i] + 1).detach().cpu().numpy().item()\n        ).to(gr4j_device)\n        uh2_ords_t1_larger_x4 = torch.arange(\n            torch.floor(x4[i] + 1).detach().cpu().numpy().item(),\n            torch.ceil(2 * x4[i]).detach().cpu().numpy().item(),\n        ).to(gr4j_device)\n        uh2_ords_t_seq_x4 = torch.arange(\n            1.0, torch.floor(x4[i] + 1).detach().cpu().numpy().item()\n        ).to(gr4j_device)\n        uh2_ords_t_larger_x4 = torch.arange(\n            torch.floor(x4[i] + 1).detach().cpu().numpy().item(),\n            torch.ceil(2 * x4[i] + 1.0).detach().cpu().numpy().item(),\n        ).to(gr4j_device)\n        s_curve1t1 = (uh1_ordinates_t1 / x4[i]) ** 2.5\n        s_curve21t1 = 0.5 * (uh2_ords_t1_seq_x4 / x4[i]) ** 2.5\n        s_curve22t1 = 1.0 - 0.5 * (2 - uh2_ords_t1_larger_x4 / x4[i]) ** 2.5\n        s_curve2t1 = torch.cat([s_curve21t1, s_curve22t1])\n        # t1 cannot be larger than x4, but t can, so we should set (uh1_ordinates_t / x4[i]) &lt;=1\n        # we don't use torch.clamp, because it seems we have to use mask, or we will get nan for grad. More details\n        # could be seen here: https://github.com/waterDLut/hydro-dl-basic/tree/dev/3-more-knowledge/5-grad-problem.ipynb\n        uh1_x4 = uh1_ordinates_t / x4[i]\n        limit_uh1_x4 = 1 - F.relu(1 - uh1_x4)\n        limit_uh2_smaller_x4 = uh2_ords_t_seq_x4 / x4[i]\n        uh2_larger_x4 = 2 - uh2_ords_t_larger_x4 / x4[i]\n        limit_uh2_larger_x4 = F.relu(uh2_larger_x4)\n        s_curve1t = limit_uh1_x4**2.5\n        s_curve21t = 0.5 * limit_uh2_smaller_x4**2.5\n        s_curve22t = 1.0 - 0.5 * limit_uh2_larger_x4**2.5\n        s_curve2t = torch.cat([s_curve21t, s_curve22t])\n        uh1_ordinate = s_curve1t - s_curve1t1\n        uh2_ordinate = s_curve2t - s_curve2t1\n        uh1_ordinates.append(uh1_ordinate)\n        uh2_ordinates.append(uh2_ordinate)\n\n    return uh1_ordinates, uh2_ordinates\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4hbv","title":"<code>dpl4hbv</code>","text":""},{"location":"api/models/#torchhydro.models.dpl4hbv.DplAnnHbv","title":"<code> DplAnnHbv            (Module)         </code>","text":"Source code in <code>torchhydro/models/dpl4hbv.py</code> <pre><code>class DplAnnHbv(nn.Module):\n    def __init__(\n        self,\n        n_input_features: int,\n        n_output_features: int,\n        n_hidden_states: Union[int, tuple, list],\n        kernel_size,\n        warmup_length: int,\n        param_limit_func=\"sigmoid\",\n        param_test_way=\"final\",\n    ):\n        \"\"\"\n        Differential Parameter learning model only with attributes as DL model's input: ANN -&gt; Param -&gt; Gr4j\n\n        The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z\n\n        Parameters\n        ----------\n        n_input_features\n            the number of input features of ANN\n        n_output_features\n            the number of output features of ANN, and it should be equal to the number of learning parameters in XAJ\n        n_hidden_states\n            the number of hidden features of ANN; it could be Union[int, tuple, list]\n        kernel_size\n            size for unit hydrograph\n        warmup_length\n            the length of warmup periods;\n            hydrologic models need a warmup period to generate reasonable initial state values\n        param_limit_func\n            function used to limit the range of params; now it is sigmoid or clamp function\n        param_test_way\n            how we use parameters from dl model when testing;\n            now we have three ways:\n            1. \"final\" -- use the final period's parameter for each period\n            2. \"mean_time\" -- Mean values of all periods' parameters is used\n            3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used\n        \"\"\"\n        super(DplAnnHbv, self).__init__()\n        self.dl_model = SimpleAnn(n_input_features, n_output_features, n_hidden_states)\n        self.pb_model = Hbv4Dpl(warmup_length, kernel_size)\n        self.param_func = param_limit_func\n        self.param_test_way = param_test_way\n\n    def forward(self, x, z):\n        \"\"\"\n        Differential parameter learning\n\n        z (normalized input) -&gt; ANN -&gt; param -&gt; + x (not normalized) -&gt; gr4j -&gt; q\n        Parameters will be denormalized in gr4j model\n\n        Parameters\n        ----------\n        x\n            not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n        z\n            normalized data used for DL model; a 2-dim tensor. [batch, feature]\n\n        Returns\n        -------\n        torch.Tensor\n            one time forward result\n        \"\"\"\n        return ann_pbm(self.dl_model, self.pb_model, self.param_func, x, z)\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4hbv.DplAnnHbv.__init__","title":"<code>__init__(self, n_input_features, n_output_features, n_hidden_states, kernel_size, warmup_length, param_limit_func='sigmoid', param_test_way='final')</code>  <code>special</code>","text":"<p>Differential Parameter learning model only with attributes as DL model's input: ANN -&gt; Param -&gt; Gr4j</p> <p>The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z</p>"},{"location":"api/models/#torchhydro.models.dpl4hbv.DplAnnHbv.__init__--parameters","title":"Parameters","text":"<p>n_input_features     the number of input features of ANN n_output_features     the number of output features of ANN, and it should be equal to the number of learning parameters in XAJ n_hidden_states     the number of hidden features of ANN; it could be Union[int, tuple, list] kernel_size     size for unit hydrograph warmup_length     the length of warmup periods;     hydrologic models need a warmup period to generate reasonable initial state values param_limit_func     function used to limit the range of params; now it is sigmoid or clamp function param_test_way     how we use parameters from dl model when testing;     now we have three ways:     1. \"final\" -- use the final period's parameter for each period     2. \"mean_time\" -- Mean values of all periods' parameters is used     3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used</p> Source code in <code>torchhydro/models/dpl4hbv.py</code> <pre><code>def __init__(\n    self,\n    n_input_features: int,\n    n_output_features: int,\n    n_hidden_states: Union[int, tuple, list],\n    kernel_size,\n    warmup_length: int,\n    param_limit_func=\"sigmoid\",\n    param_test_way=\"final\",\n):\n    \"\"\"\n    Differential Parameter learning model only with attributes as DL model's input: ANN -&gt; Param -&gt; Gr4j\n\n    The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z\n\n    Parameters\n    ----------\n    n_input_features\n        the number of input features of ANN\n    n_output_features\n        the number of output features of ANN, and it should be equal to the number of learning parameters in XAJ\n    n_hidden_states\n        the number of hidden features of ANN; it could be Union[int, tuple, list]\n    kernel_size\n        size for unit hydrograph\n    warmup_length\n        the length of warmup periods;\n        hydrologic models need a warmup period to generate reasonable initial state values\n    param_limit_func\n        function used to limit the range of params; now it is sigmoid or clamp function\n    param_test_way\n        how we use parameters from dl model when testing;\n        now we have three ways:\n        1. \"final\" -- use the final period's parameter for each period\n        2. \"mean_time\" -- Mean values of all periods' parameters is used\n        3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used\n    \"\"\"\n    super(DplAnnHbv, self).__init__()\n    self.dl_model = SimpleAnn(n_input_features, n_output_features, n_hidden_states)\n    self.pb_model = Hbv4Dpl(warmup_length, kernel_size)\n    self.param_func = param_limit_func\n    self.param_test_way = param_test_way\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4hbv.DplAnnHbv.forward","title":"<code>forward(self, x, z)</code>","text":"<p>Differential parameter learning</p> <p>z (normalized input) -&gt; ANN -&gt; param -&gt; + x (not normalized) -&gt; gr4j -&gt; q Parameters will be denormalized in gr4j model</p>"},{"location":"api/models/#torchhydro.models.dpl4hbv.DplAnnHbv.forward--parameters","title":"Parameters","text":"<p>x     not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature] z     normalized data used for DL model; a 2-dim tensor. [batch, feature]</p>"},{"location":"api/models/#torchhydro.models.dpl4hbv.DplAnnHbv.forward--returns","title":"Returns","text":"<p>torch.Tensor     one time forward result</p> Source code in <code>torchhydro/models/dpl4hbv.py</code> <pre><code>def forward(self, x, z):\n    \"\"\"\n    Differential parameter learning\n\n    z (normalized input) -&gt; ANN -&gt; param -&gt; + x (not normalized) -&gt; gr4j -&gt; q\n    Parameters will be denormalized in gr4j model\n\n    Parameters\n    ----------\n    x\n        not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n    z\n        normalized data used for DL model; a 2-dim tensor. [batch, feature]\n\n    Returns\n    -------\n    torch.Tensor\n        one time forward result\n    \"\"\"\n    return ann_pbm(self.dl_model, self.pb_model, self.param_func, x, z)\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4hbv.DplLstmHbv","title":"<code> DplLstmHbv            (Module)         </code>","text":"Source code in <code>torchhydro/models/dpl4hbv.py</code> <pre><code>class DplLstmHbv(nn.Module):\n    def __init__(\n        self,\n        n_input_features,\n        n_output_features,\n        n_hidden_states,\n        kernel_size,\n        warmup_length,\n        param_limit_func=\"sigmoid\",\n        param_test_way=\"final\",\n    ):\n        \"\"\"\n        Differential Parameter learning model: LSTM -&gt; Param -&gt; XAJ\n\n        The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z\n\n        Parameters\n        ----------\n        n_input_features\n            the number of input features of LSTM\n        n_output_features\n            the number of output features of LSTM, and it should be equal to the number of learning parameters in XAJ\n        n_hidden_states\n            the number of hidden features of LSTM\n        kernel_size\n            size for unit hydrograph\n        warmup_length\n            the time length of warmup period\n        param_limit_func\n            function used to limit the range of params; now it is sigmoid or clamp function\n        param_test_way\n            how we use parameters from dl model when testing;\n            now we have three ways:\n            1. \"final\" -- use the final period's parameter for each period\n            2. \"mean_time\" -- Mean values of all periods' parameters is used\n            3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used\n        \"\"\"\n        super(DplLstmHbv, self).__init__()\n        self.dl_model = SimpleLSTM(n_input_features, n_output_features, n_hidden_states)\n        self.pb_model = Hbv4Dpl(warmup_length, kernel_size)\n        self.param_func = param_limit_func\n        self.param_test_way = param_test_way\n\n    def forward(self, x, z):\n        \"\"\"\n        Differential parameter learning\n\n        z (normalized input) -&gt; lstm -&gt; param -&gt; + x (not normalized) -&gt; xaj -&gt; q\n        Parameters will be denormalized in xaj model\n\n        Parameters\n        ----------\n        x\n            not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n        z\n            normalized data used for DL model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n\n        Returns\n        -------\n        torch.Tensor\n            one time forward result\n        \"\"\"\n        return lstm_pbm(self.dl_model, self.pb_model, self.param_func, x, z)\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4hbv.DplLstmHbv.__init__","title":"<code>__init__(self, n_input_features, n_output_features, n_hidden_states, kernel_size, warmup_length, param_limit_func='sigmoid', param_test_way='final')</code>  <code>special</code>","text":"<p>Differential Parameter learning model: LSTM -&gt; Param -&gt; XAJ</p> <p>The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z</p>"},{"location":"api/models/#torchhydro.models.dpl4hbv.DplLstmHbv.__init__--parameters","title":"Parameters","text":"<p>n_input_features     the number of input features of LSTM n_output_features     the number of output features of LSTM, and it should be equal to the number of learning parameters in XAJ n_hidden_states     the number of hidden features of LSTM kernel_size     size for unit hydrograph warmup_length     the time length of warmup period param_limit_func     function used to limit the range of params; now it is sigmoid or clamp function param_test_way     how we use parameters from dl model when testing;     now we have three ways:     1. \"final\" -- use the final period's parameter for each period     2. \"mean_time\" -- Mean values of all periods' parameters is used     3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used</p> Source code in <code>torchhydro/models/dpl4hbv.py</code> <pre><code>def __init__(\n    self,\n    n_input_features,\n    n_output_features,\n    n_hidden_states,\n    kernel_size,\n    warmup_length,\n    param_limit_func=\"sigmoid\",\n    param_test_way=\"final\",\n):\n    \"\"\"\n    Differential Parameter learning model: LSTM -&gt; Param -&gt; XAJ\n\n    The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z\n\n    Parameters\n    ----------\n    n_input_features\n        the number of input features of LSTM\n    n_output_features\n        the number of output features of LSTM, and it should be equal to the number of learning parameters in XAJ\n    n_hidden_states\n        the number of hidden features of LSTM\n    kernel_size\n        size for unit hydrograph\n    warmup_length\n        the time length of warmup period\n    param_limit_func\n        function used to limit the range of params; now it is sigmoid or clamp function\n    param_test_way\n        how we use parameters from dl model when testing;\n        now we have three ways:\n        1. \"final\" -- use the final period's parameter for each period\n        2. \"mean_time\" -- Mean values of all periods' parameters is used\n        3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used\n    \"\"\"\n    super(DplLstmHbv, self).__init__()\n    self.dl_model = SimpleLSTM(n_input_features, n_output_features, n_hidden_states)\n    self.pb_model = Hbv4Dpl(warmup_length, kernel_size)\n    self.param_func = param_limit_func\n    self.param_test_way = param_test_way\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4hbv.DplLstmHbv.forward","title":"<code>forward(self, x, z)</code>","text":"<p>Differential parameter learning</p> <p>z (normalized input) -&gt; lstm -&gt; param -&gt; + x (not normalized) -&gt; xaj -&gt; q Parameters will be denormalized in xaj model</p>"},{"location":"api/models/#torchhydro.models.dpl4hbv.DplLstmHbv.forward--parameters","title":"Parameters","text":"<p>x     not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature] z     normalized data used for DL model; a sequence-first 3-dim tensor. [sequence, batch, feature]</p>"},{"location":"api/models/#torchhydro.models.dpl4hbv.DplLstmHbv.forward--returns","title":"Returns","text":"<p>torch.Tensor     one time forward result</p> Source code in <code>torchhydro/models/dpl4hbv.py</code> <pre><code>def forward(self, x, z):\n    \"\"\"\n    Differential parameter learning\n\n    z (normalized input) -&gt; lstm -&gt; param -&gt; + x (not normalized) -&gt; xaj -&gt; q\n    Parameters will be denormalized in xaj model\n\n    Parameters\n    ----------\n    x\n        not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n    z\n        normalized data used for DL model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n\n    Returns\n    -------\n    torch.Tensor\n        one time forward result\n    \"\"\"\n    return lstm_pbm(self.dl_model, self.pb_model, self.param_func, x, z)\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4hbv.Hbv4Dpl","title":"<code> Hbv4Dpl            (Module)         </code>","text":"<p>HBV Model Pytorch version</p> Source code in <code>torchhydro/models/dpl4hbv.py</code> <pre><code>class Hbv4Dpl(torch.nn.Module):\n    \"\"\"HBV Model Pytorch version\"\"\"\n\n    def __init__(self, warmup_length, kernel_size=15):\n        \"\"\"Initiate a HBV instance\n\n        Parameters\n        ----------\n        warmup_length : _type_\n            _description_\n        kernel_size : int, optional\n            conv kernel for unit hydrograph, by default 15\n        \"\"\"\n        super(Hbv4Dpl, self).__init__()\n        self.name = \"HBV\"\n        self.params_names = MODEL_PARAM_DICT[\"hbv\"][\"param_name\"]\n        parasca_lst = MODEL_PARAM_DICT[\"hbv\"][\"param_range\"]\n        self.beta_scale = parasca_lst[\"BETA\"]\n        self.fc_scale = parasca_lst[\"FC\"]\n        self.k0_scale = parasca_lst[\"K0\"]\n        self.k1_scale = parasca_lst[\"K1\"]\n        self.k2_scale = parasca_lst[\"K2\"]\n        self.lp_scale = parasca_lst[\"LP\"]\n        self.perc_scale = parasca_lst[\"PERC\"]\n        self.uzl_scale = parasca_lst[\"UZL\"]\n        self.tt_scale = parasca_lst[\"TT\"]\n        self.cfmax_scale = parasca_lst[\"CFMAX\"]\n        self.cfr_scale = parasca_lst[\"CFR\"]\n        self.cwh_scale = parasca_lst[\"CWH\"]\n        self.a_scale = parasca_lst[\"A\"]\n        self.theta_scale = parasca_lst[\"THETA\"]\n        self.warmup_length = warmup_length\n        self.kernel_size = kernel_size\n        # there are 3 input vars in HBV: P, PET and TEMPERATURE\n        self.feature_size = 3\n\n    def forward(\n        self, x, parameters, out_state=False, rout_opt=True\n    ) -&gt; Union[tuple, torch.Tensor]:\n        \"\"\"\n        Runs the HBV-light hydrological model (Seibert, 2005).\n\n        The code comes from mhpi/hydro-dev.\n        NaN values have to be removed from the inputs.\n\n        Parameters\n        ----------\n        x\n            p_all = array with daily values of precipitation (mm/d)\n            pet_all = array with daily values of potential evapotranspiration (mm/d)\n            t_all = array with daily values of air temperature (deg C)\n        parameters\n            array with parameter values having the following structure and scales\n            BETA: parameter in soil routine\n            FC: maximum soil moisture content\n            K0: recession coefficient\n            K1: recession coefficient\n            K2: recession coefficient\n            LP: limit for potential evapotranspiration\n            PERC: percolation from upper to lower response box\n            UZL: upper zone limit\n            TT: temperature limit for snow/rain; distinguish rainfall from snowfall\n            CFMAX: degree day factor; used for melting calculation\n            CFR: refreezing factor\n            CWH: liquid water holding capacity of the snowpack\n            A: parameter of mizuRoute\n            THETA: parameter of mizuRoute\n        out_state\n            if True, the state variables' value will be output\n        rout_opt\n            if True, route module will be performed\n\n        Returns\n        -------\n        Union[tuple, torch.Tensor]\n            q_sim = daily values of simulated streamflow (mm)\n            sm = soil storage (mm)\n            suz = upper zone storage (mm)\n            slz = lower zone storage (mm)\n            snowpack = snow depth (mm)\n            et_act = actual evaporation (mm)\n        \"\"\"\n        hbv_device = x.device\n        precision = 1e-5\n        buffer_time = self.warmup_length\n        # Initialization\n        if buffer_time &gt; 0:\n            with torch.no_grad():\n                x_init = x[0:buffer_time, :, :]\n                warmup_length = 0\n                init_model = Hbv4Dpl(warmup_length, kernel_size=self.kernel_size)\n                if init_model.warmup_length &gt; 0:\n                    raise RuntimeError(\n                        \"Please set warmup_length as 0 when initializing HBV model\"\n                    )\n                _, snowpack, meltwater, sm, suz, slz = init_model(\n                    x_init, parameters, out_state=True, rout_opt=False\n                )\n        else:\n\n            # Without buff time, initialize state variables with zeros\n            n_grid = x.shape[1]\n            snowpack = (torch.zeros(n_grid, dtype=torch.float32) + 0.001).to(hbv_device)\n            meltwater = (torch.zeros(n_grid, dtype=torch.float32) + 0.001).to(\n                hbv_device\n            )\n            sm = (torch.zeros(n_grid, dtype=torch.float32) + 0.001).to(hbv_device)\n            suz = (torch.zeros(n_grid, dtype=torch.float32) + 0.001).to(hbv_device)\n            slz = (torch.zeros(n_grid, dtype=torch.float32) + 0.001).to(hbv_device)\n\n        # the sequence must be p, pet and t\n        p_all = x[buffer_time:, :, 0]\n        pet_all = x[buffer_time:, :, 1]\n        t_all = x[buffer_time:, :, 2]\n\n        # scale the parameters\n        par_beta = self.beta_scale[0] + parameters[:, 0] * (\n            self.beta_scale[1] - self.beta_scale[0]\n        )\n        # parCET = parameters[:,1]\n        par_fc = self.fc_scale[0] + parameters[:, 1] * (\n            self.fc_scale[1] - self.fc_scale[0]\n        )\n        par_k0 = self.k0_scale[0] + parameters[:, 2] * (\n            self.k0_scale[1] - self.k0_scale[0]\n        )\n        par_k1 = self.k1_scale[0] + parameters[:, 3] * (\n            self.k1_scale[1] - self.k1_scale[0]\n        )\n        par_k2 = self.k2_scale[0] + parameters[:, 4] * (\n            self.k2_scale[1] - self.k2_scale[0]\n        )\n        par_lp = self.lp_scale[0] + parameters[:, 5] * (\n            self.lp_scale[1] - self.lp_scale[0]\n        )\n        # parMAXBAS = parameters[:,7]\n        par_perc = self.perc_scale[0] + parameters[:, 6] * (\n            self.perc_scale[1] - self.perc_scale[0]\n        )\n        par_uzl = self.uzl_scale[0] + parameters[:, 7] * (\n            self.uzl_scale[1] - self.uzl_scale[0]\n        )\n        # parPCORR = parameters[:,10]\n        par_tt = self.tt_scale[0] + parameters[:, 8] * (\n            self.tt_scale[1] - self.tt_scale[0]\n        )\n        par_cfmax = self.cfmax_scale[0] + parameters[:, 9] * (\n            self.cfmax_scale[1] - self.cfmax_scale[0]\n        )\n        # parSFCF = parameters[:,13]\n        par_cfr = self.cfr_scale[0] + parameters[:, 10] * (\n            self.cfr_scale[1] - self.cfr_scale[0]\n        )\n        par_cwh = self.cwh_scale[0] + parameters[:, 11] * (\n            self.cwh_scale[1] - self.cwh_scale[0]\n        )\n\n        n_step, n_grid = p_all.size()\n        # Apply correction factor to precipitation\n        # p_all = parPCORR.repeat(n_step, 1) * p_all\n\n        # Initialize time series of model variables\n        q_sim = (torch.zeros(p_all.size(), dtype=torch.float32) + 0.001).to(hbv_device)\n        # # Debug for the state variables\n        # SMlog = np.zeros(p_all.size())\n        log_sm = np.zeros(p_all.size())\n        log_ps = np.zeros(p_all.size())\n        log_swet = np.zeros(p_all.size())\n        log_re = np.zeros(p_all.size())\n\n        for i in range(n_step):\n            # Separate precipitation into liquid and solid components\n            precip = p_all[i, :]\n            tempre = t_all[i, :]\n            potent = pet_all[i, :]\n            rain = torch.mul(precip, (tempre &gt;= par_tt).type(torch.float32))\n            snow = torch.mul(precip, (tempre &lt; par_tt).type(torch.float32))\n            # snow = snow * parSFCF\n\n            # Snow\n            snowpack = snowpack + snow\n            melt = par_cfmax * (tempre - par_tt)\n            # melt[melt &lt; 0.0] = 0.0\n            melt = torch.clamp(melt, min=0.0)\n            # melt[melt &gt; snowpack] = snowpack[melt &gt; snowpack]\n            melt = torch.min(melt, snowpack)\n            meltwater = meltwater + melt\n            snowpack = snowpack - melt\n            refreezing = par_cfr * par_cfmax * (par_tt - tempre)\n            # refreezing[refreezing &lt; 0.0] = 0.0\n            # refreezing[refreezing &gt; meltwater] = meltwater[refreezing &gt; meltwater]\n            refreezing = torch.clamp(refreezing, min=0.0)\n            refreezing = torch.min(refreezing, meltwater)\n            snowpack = snowpack + refreezing\n            meltwater = meltwater - refreezing\n            to_soil = meltwater - (par_cwh * snowpack)\n            # to_soil[to_soil &lt; 0.0] = 0.0\n            to_soil = torch.clamp(to_soil, min=0.0)\n            meltwater = meltwater - to_soil\n\n            # Soil and evaporation\n            soil_wetness = (sm / par_fc) ** par_beta\n            # soil_wetness[soil_wetness &lt; 0.0] = 0.0\n            # soil_wetness[soil_wetness &gt; 1.0] = 1.0\n            soil_wetness = torch.clamp(soil_wetness, min=0.0, max=1.0)\n            recharge = (rain + to_soil) * soil_wetness\n\n            # log for displaying\n            log_sm[i, :] = sm.detach().cpu().numpy()\n            log_ps[i, :] = (rain + to_soil).detach().cpu().numpy()\n            log_swet[i, :] = (sm / par_fc).detach().cpu().numpy()\n            log_re[i, :] = recharge.detach().cpu().numpy()\n\n            sm = sm + rain + to_soil - recharge\n            excess = sm - par_fc\n            # excess[excess &lt; 0.0] = 0.0\n            excess = torch.clamp(excess, min=0.0)\n            sm = sm - excess\n            evap_factor = sm / (par_lp * par_fc)\n            # evap_factor[evap_factor &lt; 0.0] = 0.0\n            # evap_factor[evap_factor &gt; 1.0] = 1.0\n            evap_factor = torch.clamp(evap_factor, min=0.0, max=1.0)\n            et_act = potent * evap_factor\n            et_act = torch.min(sm, et_act)\n            sm = torch.clamp(\n                sm - et_act, min=precision\n            )  # sm can not be zero for gradient tracking\n\n            # Groundwater boxes\n            suz = suz + recharge + excess\n            perc = torch.min(suz, par_perc)\n            suz = suz - perc\n            q0 = par_k0 * torch.clamp(suz - par_uzl, min=0.0)\n            suz = suz - q0\n            q1 = par_k1 * suz\n            suz = suz - q1\n            slz = slz + perc\n            q2 = par_k2 * slz\n            slz = slz - q2\n            q_sim[i, :] = q0 + q1 + q2\n\n            # # for debug state variables\n            # SMlog[t,:] = sm.detach().cpu().numpy()\n\n        if rout_opt is True:  # routing\n            temp_a = self.a_scale[0] + parameters[:, -2] * (\n                self.a_scale[1] - self.a_scale[0]\n            )\n            temp_b = self.theta_scale[0] + parameters[:, -1] * (\n                self.theta_scale[1] - self.theta_scale[0]\n            )\n            rout_a = temp_a.repeat(n_step, 1).unsqueeze(-1)\n            rout_b = temp_b.repeat(n_step, 1).unsqueeze(-1)\n            uh_from_gamma = uh_gamma(rout_a, rout_b, len_uh=self.kernel_size)\n            rf = torch.unsqueeze(q_sim, -1)\n            qs = uh_conv(rf, uh_from_gamma)\n\n        else:\n            qs = torch.unsqueeze(q_sim, -1)  # add a dimension\n        return (qs, snowpack, meltwater, sm, suz, slz) if out_state is True else qs\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4hbv.Hbv4Dpl.__init__","title":"<code>__init__(self, warmup_length, kernel_size=15)</code>  <code>special</code>","text":"<p>Initiate a HBV instance</p>"},{"location":"api/models/#torchhydro.models.dpl4hbv.Hbv4Dpl.__init__--parameters","title":"Parameters","text":"<p>warmup_length : type description kernel_size : int, optional     conv kernel for unit hydrograph, by default 15</p> Source code in <code>torchhydro/models/dpl4hbv.py</code> <pre><code>def __init__(self, warmup_length, kernel_size=15):\n    \"\"\"Initiate a HBV instance\n\n    Parameters\n    ----------\n    warmup_length : _type_\n        _description_\n    kernel_size : int, optional\n        conv kernel for unit hydrograph, by default 15\n    \"\"\"\n    super(Hbv4Dpl, self).__init__()\n    self.name = \"HBV\"\n    self.params_names = MODEL_PARAM_DICT[\"hbv\"][\"param_name\"]\n    parasca_lst = MODEL_PARAM_DICT[\"hbv\"][\"param_range\"]\n    self.beta_scale = parasca_lst[\"BETA\"]\n    self.fc_scale = parasca_lst[\"FC\"]\n    self.k0_scale = parasca_lst[\"K0\"]\n    self.k1_scale = parasca_lst[\"K1\"]\n    self.k2_scale = parasca_lst[\"K2\"]\n    self.lp_scale = parasca_lst[\"LP\"]\n    self.perc_scale = parasca_lst[\"PERC\"]\n    self.uzl_scale = parasca_lst[\"UZL\"]\n    self.tt_scale = parasca_lst[\"TT\"]\n    self.cfmax_scale = parasca_lst[\"CFMAX\"]\n    self.cfr_scale = parasca_lst[\"CFR\"]\n    self.cwh_scale = parasca_lst[\"CWH\"]\n    self.a_scale = parasca_lst[\"A\"]\n    self.theta_scale = parasca_lst[\"THETA\"]\n    self.warmup_length = warmup_length\n    self.kernel_size = kernel_size\n    # there are 3 input vars in HBV: P, PET and TEMPERATURE\n    self.feature_size = 3\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4hbv.Hbv4Dpl.forward","title":"<code>forward(self, x, parameters, out_state=False, rout_opt=True)</code>","text":"<p>Runs the HBV-light hydrological model (Seibert, 2005).</p> <p>The code comes from mhpi/hydro-dev. NaN values have to be removed from the inputs.</p>"},{"location":"api/models/#torchhydro.models.dpl4hbv.Hbv4Dpl.forward--parameters","title":"Parameters","text":"<p>x     p_all = array with daily values of precipitation (mm/d)     pet_all = array with daily values of potential evapotranspiration (mm/d)     t_all = array with daily values of air temperature (deg C) parameters     array with parameter values having the following structure and scales     BETA: parameter in soil routine     FC: maximum soil moisture content     K0: recession coefficient     K1: recession coefficient     K2: recession coefficient     LP: limit for potential evapotranspiration     PERC: percolation from upper to lower response box     UZL: upper zone limit     TT: temperature limit for snow/rain; distinguish rainfall from snowfall     CFMAX: degree day factor; used for melting calculation     CFR: refreezing factor     CWH: liquid water holding capacity of the snowpack     A: parameter of mizuRoute     THETA: parameter of mizuRoute out_state     if True, the state variables' value will be output rout_opt     if True, route module will be performed</p>"},{"location":"api/models/#torchhydro.models.dpl4hbv.Hbv4Dpl.forward--returns","title":"Returns","text":"<p>Union[tuple, torch.Tensor]     q_sim = daily values of simulated streamflow (mm)     sm = soil storage (mm)     suz = upper zone storage (mm)     slz = lower zone storage (mm)     snowpack = snow depth (mm)     et_act = actual evaporation (mm)</p> Source code in <code>torchhydro/models/dpl4hbv.py</code> <pre><code>def forward(\n    self, x, parameters, out_state=False, rout_opt=True\n) -&gt; Union[tuple, torch.Tensor]:\n    \"\"\"\n    Runs the HBV-light hydrological model (Seibert, 2005).\n\n    The code comes from mhpi/hydro-dev.\n    NaN values have to be removed from the inputs.\n\n    Parameters\n    ----------\n    x\n        p_all = array with daily values of precipitation (mm/d)\n        pet_all = array with daily values of potential evapotranspiration (mm/d)\n        t_all = array with daily values of air temperature (deg C)\n    parameters\n        array with parameter values having the following structure and scales\n        BETA: parameter in soil routine\n        FC: maximum soil moisture content\n        K0: recession coefficient\n        K1: recession coefficient\n        K2: recession coefficient\n        LP: limit for potential evapotranspiration\n        PERC: percolation from upper to lower response box\n        UZL: upper zone limit\n        TT: temperature limit for snow/rain; distinguish rainfall from snowfall\n        CFMAX: degree day factor; used for melting calculation\n        CFR: refreezing factor\n        CWH: liquid water holding capacity of the snowpack\n        A: parameter of mizuRoute\n        THETA: parameter of mizuRoute\n    out_state\n        if True, the state variables' value will be output\n    rout_opt\n        if True, route module will be performed\n\n    Returns\n    -------\n    Union[tuple, torch.Tensor]\n        q_sim = daily values of simulated streamflow (mm)\n        sm = soil storage (mm)\n        suz = upper zone storage (mm)\n        slz = lower zone storage (mm)\n        snowpack = snow depth (mm)\n        et_act = actual evaporation (mm)\n    \"\"\"\n    hbv_device = x.device\n    precision = 1e-5\n    buffer_time = self.warmup_length\n    # Initialization\n    if buffer_time &gt; 0:\n        with torch.no_grad():\n            x_init = x[0:buffer_time, :, :]\n            warmup_length = 0\n            init_model = Hbv4Dpl(warmup_length, kernel_size=self.kernel_size)\n            if init_model.warmup_length &gt; 0:\n                raise RuntimeError(\n                    \"Please set warmup_length as 0 when initializing HBV model\"\n                )\n            _, snowpack, meltwater, sm, suz, slz = init_model(\n                x_init, parameters, out_state=True, rout_opt=False\n            )\n    else:\n\n        # Without buff time, initialize state variables with zeros\n        n_grid = x.shape[1]\n        snowpack = (torch.zeros(n_grid, dtype=torch.float32) + 0.001).to(hbv_device)\n        meltwater = (torch.zeros(n_grid, dtype=torch.float32) + 0.001).to(\n            hbv_device\n        )\n        sm = (torch.zeros(n_grid, dtype=torch.float32) + 0.001).to(hbv_device)\n        suz = (torch.zeros(n_grid, dtype=torch.float32) + 0.001).to(hbv_device)\n        slz = (torch.zeros(n_grid, dtype=torch.float32) + 0.001).to(hbv_device)\n\n    # the sequence must be p, pet and t\n    p_all = x[buffer_time:, :, 0]\n    pet_all = x[buffer_time:, :, 1]\n    t_all = x[buffer_time:, :, 2]\n\n    # scale the parameters\n    par_beta = self.beta_scale[0] + parameters[:, 0] * (\n        self.beta_scale[1] - self.beta_scale[0]\n    )\n    # parCET = parameters[:,1]\n    par_fc = self.fc_scale[0] + parameters[:, 1] * (\n        self.fc_scale[1] - self.fc_scale[0]\n    )\n    par_k0 = self.k0_scale[0] + parameters[:, 2] * (\n        self.k0_scale[1] - self.k0_scale[0]\n    )\n    par_k1 = self.k1_scale[0] + parameters[:, 3] * (\n        self.k1_scale[1] - self.k1_scale[0]\n    )\n    par_k2 = self.k2_scale[0] + parameters[:, 4] * (\n        self.k2_scale[1] - self.k2_scale[0]\n    )\n    par_lp = self.lp_scale[0] + parameters[:, 5] * (\n        self.lp_scale[1] - self.lp_scale[0]\n    )\n    # parMAXBAS = parameters[:,7]\n    par_perc = self.perc_scale[0] + parameters[:, 6] * (\n        self.perc_scale[1] - self.perc_scale[0]\n    )\n    par_uzl = self.uzl_scale[0] + parameters[:, 7] * (\n        self.uzl_scale[1] - self.uzl_scale[0]\n    )\n    # parPCORR = parameters[:,10]\n    par_tt = self.tt_scale[0] + parameters[:, 8] * (\n        self.tt_scale[1] - self.tt_scale[0]\n    )\n    par_cfmax = self.cfmax_scale[0] + parameters[:, 9] * (\n        self.cfmax_scale[1] - self.cfmax_scale[0]\n    )\n    # parSFCF = parameters[:,13]\n    par_cfr = self.cfr_scale[0] + parameters[:, 10] * (\n        self.cfr_scale[1] - self.cfr_scale[0]\n    )\n    par_cwh = self.cwh_scale[0] + parameters[:, 11] * (\n        self.cwh_scale[1] - self.cwh_scale[0]\n    )\n\n    n_step, n_grid = p_all.size()\n    # Apply correction factor to precipitation\n    # p_all = parPCORR.repeat(n_step, 1) * p_all\n\n    # Initialize time series of model variables\n    q_sim = (torch.zeros(p_all.size(), dtype=torch.float32) + 0.001).to(hbv_device)\n    # # Debug for the state variables\n    # SMlog = np.zeros(p_all.size())\n    log_sm = np.zeros(p_all.size())\n    log_ps = np.zeros(p_all.size())\n    log_swet = np.zeros(p_all.size())\n    log_re = np.zeros(p_all.size())\n\n    for i in range(n_step):\n        # Separate precipitation into liquid and solid components\n        precip = p_all[i, :]\n        tempre = t_all[i, :]\n        potent = pet_all[i, :]\n        rain = torch.mul(precip, (tempre &gt;= par_tt).type(torch.float32))\n        snow = torch.mul(precip, (tempre &lt; par_tt).type(torch.float32))\n        # snow = snow * parSFCF\n\n        # Snow\n        snowpack = snowpack + snow\n        melt = par_cfmax * (tempre - par_tt)\n        # melt[melt &lt; 0.0] = 0.0\n        melt = torch.clamp(melt, min=0.0)\n        # melt[melt &gt; snowpack] = snowpack[melt &gt; snowpack]\n        melt = torch.min(melt, snowpack)\n        meltwater = meltwater + melt\n        snowpack = snowpack - melt\n        refreezing = par_cfr * par_cfmax * (par_tt - tempre)\n        # refreezing[refreezing &lt; 0.0] = 0.0\n        # refreezing[refreezing &gt; meltwater] = meltwater[refreezing &gt; meltwater]\n        refreezing = torch.clamp(refreezing, min=0.0)\n        refreezing = torch.min(refreezing, meltwater)\n        snowpack = snowpack + refreezing\n        meltwater = meltwater - refreezing\n        to_soil = meltwater - (par_cwh * snowpack)\n        # to_soil[to_soil &lt; 0.0] = 0.0\n        to_soil = torch.clamp(to_soil, min=0.0)\n        meltwater = meltwater - to_soil\n\n        # Soil and evaporation\n        soil_wetness = (sm / par_fc) ** par_beta\n        # soil_wetness[soil_wetness &lt; 0.0] = 0.0\n        # soil_wetness[soil_wetness &gt; 1.0] = 1.0\n        soil_wetness = torch.clamp(soil_wetness, min=0.0, max=1.0)\n        recharge = (rain + to_soil) * soil_wetness\n\n        # log for displaying\n        log_sm[i, :] = sm.detach().cpu().numpy()\n        log_ps[i, :] = (rain + to_soil).detach().cpu().numpy()\n        log_swet[i, :] = (sm / par_fc).detach().cpu().numpy()\n        log_re[i, :] = recharge.detach().cpu().numpy()\n\n        sm = sm + rain + to_soil - recharge\n        excess = sm - par_fc\n        # excess[excess &lt; 0.0] = 0.0\n        excess = torch.clamp(excess, min=0.0)\n        sm = sm - excess\n        evap_factor = sm / (par_lp * par_fc)\n        # evap_factor[evap_factor &lt; 0.0] = 0.0\n        # evap_factor[evap_factor &gt; 1.0] = 1.0\n        evap_factor = torch.clamp(evap_factor, min=0.0, max=1.0)\n        et_act = potent * evap_factor\n        et_act = torch.min(sm, et_act)\n        sm = torch.clamp(\n            sm - et_act, min=precision\n        )  # sm can not be zero for gradient tracking\n\n        # Groundwater boxes\n        suz = suz + recharge + excess\n        perc = torch.min(suz, par_perc)\n        suz = suz - perc\n        q0 = par_k0 * torch.clamp(suz - par_uzl, min=0.0)\n        suz = suz - q0\n        q1 = par_k1 * suz\n        suz = suz - q1\n        slz = slz + perc\n        q2 = par_k2 * slz\n        slz = slz - q2\n        q_sim[i, :] = q0 + q1 + q2\n\n        # # for debug state variables\n        # SMlog[t,:] = sm.detach().cpu().numpy()\n\n    if rout_opt is True:  # routing\n        temp_a = self.a_scale[0] + parameters[:, -2] * (\n            self.a_scale[1] - self.a_scale[0]\n        )\n        temp_b = self.theta_scale[0] + parameters[:, -1] * (\n            self.theta_scale[1] - self.theta_scale[0]\n        )\n        rout_a = temp_a.repeat(n_step, 1).unsqueeze(-1)\n        rout_b = temp_b.repeat(n_step, 1).unsqueeze(-1)\n        uh_from_gamma = uh_gamma(rout_a, rout_b, len_uh=self.kernel_size)\n        rf = torch.unsqueeze(q_sim, -1)\n        qs = uh_conv(rf, uh_from_gamma)\n\n    else:\n        qs = torch.unsqueeze(q_sim, -1)  # add a dimension\n    return (qs, snowpack, meltwater, sm, suz, slz) if out_state is True else qs\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj","title":"<code>dpl4xaj</code>","text":"<p>Author: Wenyu Ouyang Date: 2023-09-19 09:36:25 LastEditTime: 2025-06-25 15:50:57 LastEditors: Wenyu Ouyang Description: The method comes from this paper: https://doi.org/10.1038/s41467-021-26107-z It use Deep Learning (DL) methods to Learn the Parameters of physics-based models (PBM), which is called \"differentiable parameter learning\" (dPL). FilePath:       orchhydro       orchhydro\\models\\dpl4xaj.py Copyright (c) 2023-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.DplAnnXaj","title":"<code> DplAnnXaj            (Module)         </code>","text":"Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>class DplAnnXaj(nn.Module):\n    def __init__(\n        self,\n        n_input_features: int,\n        n_output_features: int,\n        n_hidden_states: Union[int, tuple, list],\n        kernel_size: int,\n        warmup_length: int,\n        dr: Union[int, tuple, list] = 0.1,\n        param_limit_func=\"sigmoid\",\n        param_test_way=\"final\",\n        source_book=\"HF\",\n        source_type=\"sources\",\n        return_et=True,\n    ):\n        \"\"\"\n        Differential Parameter learning model only with attributes as DL model's input: ANN -&gt; Param -&gt; Gr4j\n\n        The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z\n\n        Parameters\n        ----------\n        n_input_features\n            the number of input features of ANN\n        n_output_features\n            the number of output features of ANN, and it should be equal to the number of learning parameters in XAJ\n        n_hidden_states\n            the number of hidden features of ANN; it could be Union[int, tuple, list]\n        kernel_size\n            the time length of unit hydrograph\n        warmup_length\n            the length of warmup periods;\n            hydrologic models need a warmup period to generate reasonable initial state values\n        param_limit_func\n            function used to limit the range of params; now it is sigmoid or clamp function\n        param_test_way\n            how we use parameters from dl model when testing;\n            now we have three ways:\n            1. \"final\" -- use the final period's parameter for each period\n            2. \"mean_time\" -- Mean values of all periods' parameters is used\n            3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used\n        return_et\n            if True, return evapotranspiration\n        \"\"\"\n        super(DplAnnXaj, self).__init__()\n        self.dl_model = SimpleAnn(\n            n_input_features, n_output_features, n_hidden_states, dr\n        )\n        self.pb_model = Xaj4Dpl(\n            kernel_size, warmup_length, source_book=source_book, source_type=source_type\n        )\n        self.param_func = param_limit_func\n        self.param_test_way = param_test_way\n        self.return_et = return_et\n\n    def forward(self, x, z):\n        \"\"\"\n        Differential parameter learning\n\n        z (normalized input) -&gt; ANN -&gt; param -&gt; + x (not normalized) -&gt; gr4j -&gt; q\n        Parameters will be denormalized in gr4j model\n\n        Parameters\n        ----------\n        x\n            not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n        z\n            normalized data used for DL model; a 2-dim tensor. [batch, feature]\n\n        Returns\n        -------\n        torch.Tensor\n            one time forward result\n        \"\"\"\n        q, e = ann_pbm(self.dl_model, self.pb_model, self.param_func, x, z)\n        return torch.cat([q, e], dim=-1) if self.return_et else q\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj.DplAnnXaj.__init__","title":"<code>__init__(self, n_input_features, n_output_features, n_hidden_states, kernel_size, warmup_length, dr=0.1, param_limit_func='sigmoid', param_test_way='final', source_book='HF', source_type='sources', return_et=True)</code>  <code>special</code>","text":"<p>Differential Parameter learning model only with attributes as DL model's input: ANN -&gt; Param -&gt; Gr4j</p> <p>The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.DplAnnXaj.__init__--parameters","title":"Parameters","text":"<p>n_input_features     the number of input features of ANN n_output_features     the number of output features of ANN, and it should be equal to the number of learning parameters in XAJ n_hidden_states     the number of hidden features of ANN; it could be Union[int, tuple, list] kernel_size     the time length of unit hydrograph warmup_length     the length of warmup periods;     hydrologic models need a warmup period to generate reasonable initial state values param_limit_func     function used to limit the range of params; now it is sigmoid or clamp function param_test_way     how we use parameters from dl model when testing;     now we have three ways:     1. \"final\" -- use the final period's parameter for each period     2. \"mean_time\" -- Mean values of all periods' parameters is used     3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used return_et     if True, return evapotranspiration</p> Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>def __init__(\n    self,\n    n_input_features: int,\n    n_output_features: int,\n    n_hidden_states: Union[int, tuple, list],\n    kernel_size: int,\n    warmup_length: int,\n    dr: Union[int, tuple, list] = 0.1,\n    param_limit_func=\"sigmoid\",\n    param_test_way=\"final\",\n    source_book=\"HF\",\n    source_type=\"sources\",\n    return_et=True,\n):\n    \"\"\"\n    Differential Parameter learning model only with attributes as DL model's input: ANN -&gt; Param -&gt; Gr4j\n\n    The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z\n\n    Parameters\n    ----------\n    n_input_features\n        the number of input features of ANN\n    n_output_features\n        the number of output features of ANN, and it should be equal to the number of learning parameters in XAJ\n    n_hidden_states\n        the number of hidden features of ANN; it could be Union[int, tuple, list]\n    kernel_size\n        the time length of unit hydrograph\n    warmup_length\n        the length of warmup periods;\n        hydrologic models need a warmup period to generate reasonable initial state values\n    param_limit_func\n        function used to limit the range of params; now it is sigmoid or clamp function\n    param_test_way\n        how we use parameters from dl model when testing;\n        now we have three ways:\n        1. \"final\" -- use the final period's parameter for each period\n        2. \"mean_time\" -- Mean values of all periods' parameters is used\n        3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used\n    return_et\n        if True, return evapotranspiration\n    \"\"\"\n    super(DplAnnXaj, self).__init__()\n    self.dl_model = SimpleAnn(\n        n_input_features, n_output_features, n_hidden_states, dr\n    )\n    self.pb_model = Xaj4Dpl(\n        kernel_size, warmup_length, source_book=source_book, source_type=source_type\n    )\n    self.param_func = param_limit_func\n    self.param_test_way = param_test_way\n    self.return_et = return_et\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj.DplAnnXaj.forward","title":"<code>forward(self, x, z)</code>","text":"<p>Differential parameter learning</p> <p>z (normalized input) -&gt; ANN -&gt; param -&gt; + x (not normalized) -&gt; gr4j -&gt; q Parameters will be denormalized in gr4j model</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.DplAnnXaj.forward--parameters","title":"Parameters","text":"<p>x     not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature] z     normalized data used for DL model; a 2-dim tensor. [batch, feature]</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.DplAnnXaj.forward--returns","title":"Returns","text":"<p>torch.Tensor     one time forward result</p> Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>def forward(self, x, z):\n    \"\"\"\n    Differential parameter learning\n\n    z (normalized input) -&gt; ANN -&gt; param -&gt; + x (not normalized) -&gt; gr4j -&gt; q\n    Parameters will be denormalized in gr4j model\n\n    Parameters\n    ----------\n    x\n        not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n    z\n        normalized data used for DL model; a 2-dim tensor. [batch, feature]\n\n    Returns\n    -------\n    torch.Tensor\n        one time forward result\n    \"\"\"\n    q, e = ann_pbm(self.dl_model, self.pb_model, self.param_func, x, z)\n    return torch.cat([q, e], dim=-1) if self.return_et else q\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj.DplLstmXaj","title":"<code> DplLstmXaj            (Module)         </code>","text":"Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>class DplLstmXaj(nn.Module):\n    def __init__(\n        self,\n        n_input_features,\n        n_output_features,\n        n_hidden_states,\n        kernel_size,\n        warmup_length,\n        param_limit_func=\"sigmoid\",\n        param_test_way=\"final\",\n        source_book=\"HF\",\n        source_type=\"sources\",\n        return_et=True,\n    ):\n        \"\"\"\n        Differential Parameter learning model: LSTM -&gt; Param -&gt; XAJ\n\n        The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z\n\n        Parameters\n        ----------\n        n_input_features\n            the number of input features of LSTM\n        n_output_features\n            the number of output features of LSTM, and it should be equal to the number of learning parameters in XAJ\n        n_hidden_states\n            the number of hidden features of LSTM\n        kernel_size\n            the time length of unit hydrograph\n        warmup_length\n            the length of warmup periods;\n            hydrologic models need a warmup period to generate reasonable initial state values\n        param_limit_func\n            function used to limit the range of params; now it is sigmoid or clamp function\n        param_test_way\n            how we use parameters from dl model when testing;\n            now we have three ways:\n            1. \"final\" -- use the final period's parameter for each period\n            2. \"mean_time\" -- Mean values of all periods' parameters is used\n            3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used\n        return_et\n            if True, return evapotranspiration\n        \"\"\"\n        super(DplLstmXaj, self).__init__()\n        self.dl_model = SimpleLSTM(n_input_features, n_output_features, n_hidden_states)\n        self.pb_model = Xaj4Dpl(\n            kernel_size, warmup_length, source_book=source_book, source_type=source_type\n        )\n        self.param_func = param_limit_func\n        self.param_test_way = param_test_way\n        self.return_et = return_et\n\n    def forward(self, x, z):\n        \"\"\"\n        Differential parameter learning\n\n        z (normalized input) -&gt; lstm -&gt; param -&gt; + x (not normalized) -&gt; xaj -&gt; q\n        Parameters will be denormalized in xaj model\n\n        Parameters\n        ----------\n        x\n            not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n        z\n            normalized data used for DL model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n\n        Returns\n        -------\n        torch.Tensor\n            one time forward result\n        \"\"\"\n        q, e = lstm_pbm(self.dl_model, self.pb_model, self.param_func, x, z)\n        return torch.cat([q, e], dim=-1) if self.return_et else q\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj.DplLstmXaj.__init__","title":"<code>__init__(self, n_input_features, n_output_features, n_hidden_states, kernel_size, warmup_length, param_limit_func='sigmoid', param_test_way='final', source_book='HF', source_type='sources', return_et=True)</code>  <code>special</code>","text":"<p>Differential Parameter learning model: LSTM -&gt; Param -&gt; XAJ</p> <p>The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.DplLstmXaj.__init__--parameters","title":"Parameters","text":"<p>n_input_features     the number of input features of LSTM n_output_features     the number of output features of LSTM, and it should be equal to the number of learning parameters in XAJ n_hidden_states     the number of hidden features of LSTM kernel_size     the time length of unit hydrograph warmup_length     the length of warmup periods;     hydrologic models need a warmup period to generate reasonable initial state values param_limit_func     function used to limit the range of params; now it is sigmoid or clamp function param_test_way     how we use parameters from dl model when testing;     now we have three ways:     1. \"final\" -- use the final period's parameter for each period     2. \"mean_time\" -- Mean values of all periods' parameters is used     3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used return_et     if True, return evapotranspiration</p> Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>def __init__(\n    self,\n    n_input_features,\n    n_output_features,\n    n_hidden_states,\n    kernel_size,\n    warmup_length,\n    param_limit_func=\"sigmoid\",\n    param_test_way=\"final\",\n    source_book=\"HF\",\n    source_type=\"sources\",\n    return_et=True,\n):\n    \"\"\"\n    Differential Parameter learning model: LSTM -&gt; Param -&gt; XAJ\n\n    The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z\n\n    Parameters\n    ----------\n    n_input_features\n        the number of input features of LSTM\n    n_output_features\n        the number of output features of LSTM, and it should be equal to the number of learning parameters in XAJ\n    n_hidden_states\n        the number of hidden features of LSTM\n    kernel_size\n        the time length of unit hydrograph\n    warmup_length\n        the length of warmup periods;\n        hydrologic models need a warmup period to generate reasonable initial state values\n    param_limit_func\n        function used to limit the range of params; now it is sigmoid or clamp function\n    param_test_way\n        how we use parameters from dl model when testing;\n        now we have three ways:\n        1. \"final\" -- use the final period's parameter for each period\n        2. \"mean_time\" -- Mean values of all periods' parameters is used\n        3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used\n    return_et\n        if True, return evapotranspiration\n    \"\"\"\n    super(DplLstmXaj, self).__init__()\n    self.dl_model = SimpleLSTM(n_input_features, n_output_features, n_hidden_states)\n    self.pb_model = Xaj4Dpl(\n        kernel_size, warmup_length, source_book=source_book, source_type=source_type\n    )\n    self.param_func = param_limit_func\n    self.param_test_way = param_test_way\n    self.return_et = return_et\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj.DplLstmXaj.forward","title":"<code>forward(self, x, z)</code>","text":"<p>Differential parameter learning</p> <p>z (normalized input) -&gt; lstm -&gt; param -&gt; + x (not normalized) -&gt; xaj -&gt; q Parameters will be denormalized in xaj model</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.DplLstmXaj.forward--parameters","title":"Parameters","text":"<p>x     not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature] z     normalized data used for DL model; a sequence-first 3-dim tensor. [sequence, batch, feature]</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.DplLstmXaj.forward--returns","title":"Returns","text":"<p>torch.Tensor     one time forward result</p> Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>def forward(self, x, z):\n    \"\"\"\n    Differential parameter learning\n\n    z (normalized input) -&gt; lstm -&gt; param -&gt; + x (not normalized) -&gt; xaj -&gt; q\n    Parameters will be denormalized in xaj model\n\n    Parameters\n    ----------\n    x\n        not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n    z\n        normalized data used for DL model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n\n    Returns\n    -------\n    torch.Tensor\n        one time forward result\n    \"\"\"\n    q, e = lstm_pbm(self.dl_model, self.pb_model, self.param_func, x, z)\n    return torch.cat([q, e], dim=-1) if self.return_et else q\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj.Xaj4Dpl","title":"<code> Xaj4Dpl            (Module)         </code>","text":"<p>XAJ model for Differential Parameter learning</p> Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>class Xaj4Dpl(nn.Module):\n    \"\"\"\n    XAJ model for Differential Parameter learning\n    \"\"\"\n\n    def __init__(\n        self,\n        kernel_size: int,\n        warmup_length: int,\n        source_book=\"HF\",\n        source_type=\"sources\",\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        kernel_size\n            the time length of unit hydrograph\n        warmup_length\n            the length of warmup periods;\n            XAJ needs a warmup period to generate reasonable initial state values\n        \"\"\"\n        super(Xaj4Dpl, self).__init__()\n        self.params_names = MODEL_PARAM_DICT[\"xaj_mz\"][\"param_name\"]\n        param_range = MODEL_PARAM_DICT[\"xaj_mz\"][\"param_range\"]\n        self.k_scale = param_range[\"K\"]\n        self.b_scale = param_range[\"B\"]\n        self.im_sacle = param_range[\"IM\"]\n        self.um_scale = param_range[\"UM\"]\n        self.lm_scale = param_range[\"LM\"]\n        self.dm_scale = param_range[\"DM\"]\n        self.c_scale = param_range[\"C\"]\n        self.sm_scale = param_range[\"SM\"]\n        self.ex_scale = param_range[\"EX\"]\n        self.ki_scale = param_range[\"KI\"]\n        self.kg_scale = param_range[\"KG\"]\n        self.a_scale = param_range[\"A\"]\n        self.theta_scale = param_range[\"THETA\"]\n        self.ci_scale = param_range[\"CI\"]\n        self.cg_scale = param_range[\"CG\"]\n        self.kernel_size = kernel_size\n        self.warmup_length = warmup_length\n        # there are 2 input variables in XAJ: P and PET\n        self.feature_size = 2\n        self.source_book = source_book\n        self.source_type = source_type\n\n    def forward(self, p_and_e, parameters, return_state=False):\n        \"\"\"\n        run XAJ model\n\n        Parameters\n        ----------\n        p_and_e\n            precipitation and potential evapotranspiration\n        parameters\n            parameters of XAJ model\n        return_state\n            if True, return state values, mainly for warmup periods\n\n        Returns\n        -------\n        torch.Tensor\n            streamflow got by XAJ\n        \"\"\"\n        xaj_device = p_and_e.device\n        # denormalize the parameters to general range\n        k = self.k_scale[0] + parameters[:, 0] * (self.k_scale[1] - self.k_scale[0])\n        b = self.b_scale[0] + parameters[:, 1] * (self.b_scale[1] - self.b_scale[0])\n        im = self.im_sacle[0] + parameters[:, 2] * (self.im_sacle[1] - self.im_sacle[0])\n        um = self.um_scale[0] + parameters[:, 3] * (self.um_scale[1] - self.um_scale[0])\n        lm = self.lm_scale[0] + parameters[:, 4] * (self.lm_scale[1] - self.lm_scale[0])\n        dm = self.dm_scale[0] + parameters[:, 5] * (self.dm_scale[1] - self.dm_scale[0])\n        c = self.c_scale[0] + parameters[:, 6] * (self.c_scale[1] - self.c_scale[0])\n        sm = self.sm_scale[0] + parameters[:, 7] * (self.sm_scale[1] - self.sm_scale[0])\n        ex = self.ex_scale[0] + parameters[:, 8] * (self.ex_scale[1] - self.ex_scale[0])\n        ki_ = self.ki_scale[0] + parameters[:, 9] * (\n            self.ki_scale[1] - self.ki_scale[0]\n        )\n        kg_ = self.kg_scale[0] + parameters[:, 10] * (\n            self.kg_scale[1] - self.kg_scale[0]\n        )\n        # ki+kg should be smaller than 1; if not, we scale them, but note float only contain 4 digits, so we need 0.999\n        ki = torch.where(\n            ki_ + kg_ &lt; 1.0,\n            ki_,\n            (1 - PRECISION) / (ki_ + kg_) * ki_,\n        )\n        kg = torch.where(\n            ki_ + kg_ &lt; 1.0,\n            kg_,\n            (1 - PRECISION) / (ki_ + kg_) * kg_,\n        )\n        a = self.a_scale[0] + parameters[:, 11] * (self.a_scale[1] - self.a_scale[0])\n        theta = self.theta_scale[0] + parameters[:, 12] * (\n            self.theta_scale[1] - self.theta_scale[0]\n        )\n        ci = self.ci_scale[0] + parameters[:, 13] * (\n            self.ci_scale[1] - self.ci_scale[0]\n        )\n        cg = self.cg_scale[0] + parameters[:, 14] * (\n            self.cg_scale[1] - self.cg_scale[0]\n        )\n\n        # initialize state values\n        warmup_length = self.warmup_length\n        if warmup_length &gt; 0:\n            # set no_grad for warmup periods\n            with torch.no_grad():\n                p_and_e_warmup = p_and_e[0:warmup_length, :, :]\n                cal_init_xaj4dpl = Xaj4Dpl(\n                    self.kernel_size, 0, self.source_book, self.source_type\n                )\n                if cal_init_xaj4dpl.warmup_length &gt; 0:\n                    raise RuntimeError(\"Please set init model's warmup length to 0!!!\")\n                _, _, *w0, s0, fr0, qi0, qg0 = cal_init_xaj4dpl(\n                    p_and_e_warmup, parameters, return_state=True\n                )\n        else:\n            # use detach func to make wu0 no_grad as it is an initial value\n            w0 = (0.5 * (um.detach()), 0.5 * (lm.detach()), 0.5 * (dm.detach()))\n            s0 = 0.5 * (sm.detach())\n            fr0 = torch.full(ci.size(), 0.1).to(xaj_device)\n            qi0 = torch.full(ci.size(), 0.1).to(xaj_device)\n            qg0 = torch.full(cg.size(), 0.1).to(xaj_device)\n\n        inputs = p_and_e[warmup_length:, :, :]\n        runoff_ims_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n        rss_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n        ris_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n        rgs_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n        es_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n        for i in range(inputs.shape[0]):\n            if i == 0:\n                (r, rim, e, pe), w = xaj_generation(\n                    inputs[i, :, :], k, b, im, um, lm, dm, c, *w0\n                )\n                if self.source_type == \"sources\":\n                    (rs, ri, rg), (s, fr) = xaj_sources(\n                        pe, r, sm, ex, ki, kg, s0, fr0, book=self.source_book\n                    )\n                elif self.source_type == \"sources5mm\":\n                    (rs, ri, rg), (s, fr) = xaj_sources5mm(\n                        pe, r, sm, ex, ki, kg, s0, fr0, book=self.source_book\n                    )\n                else:\n                    raise NotImplementedError(\"No such divide-sources method\")\n            else:\n                (r, rim, e, pe), w = xaj_generation(\n                    inputs[i, :, :], k, b, im, um, lm, dm, c, *w\n                )\n                if self.source_type == \"sources\":\n                    (rs, ri, rg), (s, fr) = xaj_sources(\n                        pe, r, sm, ex, ki, kg, s, fr, book=self.source_book\n                    )\n                elif self.source_type == \"sources5mm\":\n                    (rs, ri, rg), (s, fr) = xaj_sources5mm(\n                        pe, r, sm, ex, ki, kg, s, fr, book=self.source_book\n                    )\n                else:\n                    raise NotImplementedError(\"No such divide-sources method\")\n            # impevious part is pe * im\n            runoff_ims_[i, :] = rim\n            # so for non-imprvious part, the result should be corrected\n            rss_[i, :] = rs * (1 - im)\n            ris_[i, :] = ri * (1 - im)\n            rgs_[i, :] = rg * (1 - im)\n            es_[i, :] = e\n            # rss_[i, :] = 0.7 * r\n            # ris_[i, :] = 0.2 * r\n            # rgs_[i, :] = 0.1 * r\n        # seq, batch, feature\n        runoff_im = torch.unsqueeze(runoff_ims_, dim=2)\n        rss = torch.unsqueeze(rss_, dim=2)\n        es = torch.unsqueeze(es_, dim=2)\n\n        conv_uh = KernelConv(a, theta, self.kernel_size)\n        qs_ = conv_uh(runoff_im + rss)\n        qs = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n        for i in range(inputs.shape[0]):\n            if i == 0:\n                qi = linear_reservoir(ris_[i], ci, qi0)\n                qg = linear_reservoir(rgs_[i], cg, qg0)\n            else:\n                qi = linear_reservoir(ris_[i], ci, qi)\n                qg = linear_reservoir(rgs_[i], cg, qg)\n            qs[i, :] = qs_[i, :, 0] + qi + qg\n        # seq, batch, feature\n        q_sim = torch.unsqueeze(qs, dim=2)\n        if return_state:\n            return q_sim, es, *w, s, fr, qi, qg\n        return q_sim, es\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj.Xaj4Dpl.__init__","title":"<code>__init__(self, kernel_size, warmup_length, source_book='HF', source_type='sources')</code>  <code>special</code>","text":""},{"location":"api/models/#torchhydro.models.dpl4xaj.Xaj4Dpl.__init__--parameters","title":"Parameters","text":"<p>kernel_size     the time length of unit hydrograph warmup_length     the length of warmup periods;     XAJ needs a warmup period to generate reasonable initial state values</p> Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>def __init__(\n    self,\n    kernel_size: int,\n    warmup_length: int,\n    source_book=\"HF\",\n    source_type=\"sources\",\n):\n    \"\"\"\n    Parameters\n    ----------\n    kernel_size\n        the time length of unit hydrograph\n    warmup_length\n        the length of warmup periods;\n        XAJ needs a warmup period to generate reasonable initial state values\n    \"\"\"\n    super(Xaj4Dpl, self).__init__()\n    self.params_names = MODEL_PARAM_DICT[\"xaj_mz\"][\"param_name\"]\n    param_range = MODEL_PARAM_DICT[\"xaj_mz\"][\"param_range\"]\n    self.k_scale = param_range[\"K\"]\n    self.b_scale = param_range[\"B\"]\n    self.im_sacle = param_range[\"IM\"]\n    self.um_scale = param_range[\"UM\"]\n    self.lm_scale = param_range[\"LM\"]\n    self.dm_scale = param_range[\"DM\"]\n    self.c_scale = param_range[\"C\"]\n    self.sm_scale = param_range[\"SM\"]\n    self.ex_scale = param_range[\"EX\"]\n    self.ki_scale = param_range[\"KI\"]\n    self.kg_scale = param_range[\"KG\"]\n    self.a_scale = param_range[\"A\"]\n    self.theta_scale = param_range[\"THETA\"]\n    self.ci_scale = param_range[\"CI\"]\n    self.cg_scale = param_range[\"CG\"]\n    self.kernel_size = kernel_size\n    self.warmup_length = warmup_length\n    # there are 2 input variables in XAJ: P and PET\n    self.feature_size = 2\n    self.source_book = source_book\n    self.source_type = source_type\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj.Xaj4Dpl.forward","title":"<code>forward(self, p_and_e, parameters, return_state=False)</code>","text":"<p>run XAJ model</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.Xaj4Dpl.forward--parameters","title":"Parameters","text":"<p>p_and_e     precipitation and potential evapotranspiration parameters     parameters of XAJ model return_state     if True, return state values, mainly for warmup periods</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.Xaj4Dpl.forward--returns","title":"Returns","text":"<p>torch.Tensor     streamflow got by XAJ</p> Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>def forward(self, p_and_e, parameters, return_state=False):\n    \"\"\"\n    run XAJ model\n\n    Parameters\n    ----------\n    p_and_e\n        precipitation and potential evapotranspiration\n    parameters\n        parameters of XAJ model\n    return_state\n        if True, return state values, mainly for warmup periods\n\n    Returns\n    -------\n    torch.Tensor\n        streamflow got by XAJ\n    \"\"\"\n    xaj_device = p_and_e.device\n    # denormalize the parameters to general range\n    k = self.k_scale[0] + parameters[:, 0] * (self.k_scale[1] - self.k_scale[0])\n    b = self.b_scale[0] + parameters[:, 1] * (self.b_scale[1] - self.b_scale[0])\n    im = self.im_sacle[0] + parameters[:, 2] * (self.im_sacle[1] - self.im_sacle[0])\n    um = self.um_scale[0] + parameters[:, 3] * (self.um_scale[1] - self.um_scale[0])\n    lm = self.lm_scale[0] + parameters[:, 4] * (self.lm_scale[1] - self.lm_scale[0])\n    dm = self.dm_scale[0] + parameters[:, 5] * (self.dm_scale[1] - self.dm_scale[0])\n    c = self.c_scale[0] + parameters[:, 6] * (self.c_scale[1] - self.c_scale[0])\n    sm = self.sm_scale[0] + parameters[:, 7] * (self.sm_scale[1] - self.sm_scale[0])\n    ex = self.ex_scale[0] + parameters[:, 8] * (self.ex_scale[1] - self.ex_scale[0])\n    ki_ = self.ki_scale[0] + parameters[:, 9] * (\n        self.ki_scale[1] - self.ki_scale[0]\n    )\n    kg_ = self.kg_scale[0] + parameters[:, 10] * (\n        self.kg_scale[1] - self.kg_scale[0]\n    )\n    # ki+kg should be smaller than 1; if not, we scale them, but note float only contain 4 digits, so we need 0.999\n    ki = torch.where(\n        ki_ + kg_ &lt; 1.0,\n        ki_,\n        (1 - PRECISION) / (ki_ + kg_) * ki_,\n    )\n    kg = torch.where(\n        ki_ + kg_ &lt; 1.0,\n        kg_,\n        (1 - PRECISION) / (ki_ + kg_) * kg_,\n    )\n    a = self.a_scale[0] + parameters[:, 11] * (self.a_scale[1] - self.a_scale[0])\n    theta = self.theta_scale[0] + parameters[:, 12] * (\n        self.theta_scale[1] - self.theta_scale[0]\n    )\n    ci = self.ci_scale[0] + parameters[:, 13] * (\n        self.ci_scale[1] - self.ci_scale[0]\n    )\n    cg = self.cg_scale[0] + parameters[:, 14] * (\n        self.cg_scale[1] - self.cg_scale[0]\n    )\n\n    # initialize state values\n    warmup_length = self.warmup_length\n    if warmup_length &gt; 0:\n        # set no_grad for warmup periods\n        with torch.no_grad():\n            p_and_e_warmup = p_and_e[0:warmup_length, :, :]\n            cal_init_xaj4dpl = Xaj4Dpl(\n                self.kernel_size, 0, self.source_book, self.source_type\n            )\n            if cal_init_xaj4dpl.warmup_length &gt; 0:\n                raise RuntimeError(\"Please set init model's warmup length to 0!!!\")\n            _, _, *w0, s0, fr0, qi0, qg0 = cal_init_xaj4dpl(\n                p_and_e_warmup, parameters, return_state=True\n            )\n    else:\n        # use detach func to make wu0 no_grad as it is an initial value\n        w0 = (0.5 * (um.detach()), 0.5 * (lm.detach()), 0.5 * (dm.detach()))\n        s0 = 0.5 * (sm.detach())\n        fr0 = torch.full(ci.size(), 0.1).to(xaj_device)\n        qi0 = torch.full(ci.size(), 0.1).to(xaj_device)\n        qg0 = torch.full(cg.size(), 0.1).to(xaj_device)\n\n    inputs = p_and_e[warmup_length:, :, :]\n    runoff_ims_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n    rss_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n    ris_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n    rgs_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n    es_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n    for i in range(inputs.shape[0]):\n        if i == 0:\n            (r, rim, e, pe), w = xaj_generation(\n                inputs[i, :, :], k, b, im, um, lm, dm, c, *w0\n            )\n            if self.source_type == \"sources\":\n                (rs, ri, rg), (s, fr) = xaj_sources(\n                    pe, r, sm, ex, ki, kg, s0, fr0, book=self.source_book\n                )\n            elif self.source_type == \"sources5mm\":\n                (rs, ri, rg), (s, fr) = xaj_sources5mm(\n                    pe, r, sm, ex, ki, kg, s0, fr0, book=self.source_book\n                )\n            else:\n                raise NotImplementedError(\"No such divide-sources method\")\n        else:\n            (r, rim, e, pe), w = xaj_generation(\n                inputs[i, :, :], k, b, im, um, lm, dm, c, *w\n            )\n            if self.source_type == \"sources\":\n                (rs, ri, rg), (s, fr) = xaj_sources(\n                    pe, r, sm, ex, ki, kg, s, fr, book=self.source_book\n                )\n            elif self.source_type == \"sources5mm\":\n                (rs, ri, rg), (s, fr) = xaj_sources5mm(\n                    pe, r, sm, ex, ki, kg, s, fr, book=self.source_book\n                )\n            else:\n                raise NotImplementedError(\"No such divide-sources method\")\n        # impevious part is pe * im\n        runoff_ims_[i, :] = rim\n        # so for non-imprvious part, the result should be corrected\n        rss_[i, :] = rs * (1 - im)\n        ris_[i, :] = ri * (1 - im)\n        rgs_[i, :] = rg * (1 - im)\n        es_[i, :] = e\n        # rss_[i, :] = 0.7 * r\n        # ris_[i, :] = 0.2 * r\n        # rgs_[i, :] = 0.1 * r\n    # seq, batch, feature\n    runoff_im = torch.unsqueeze(runoff_ims_, dim=2)\n    rss = torch.unsqueeze(rss_, dim=2)\n    es = torch.unsqueeze(es_, dim=2)\n\n    conv_uh = KernelConv(a, theta, self.kernel_size)\n    qs_ = conv_uh(runoff_im + rss)\n    qs = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n    for i in range(inputs.shape[0]):\n        if i == 0:\n            qi = linear_reservoir(ris_[i], ci, qi0)\n            qg = linear_reservoir(rgs_[i], cg, qg0)\n        else:\n            qi = linear_reservoir(ris_[i], ci, qi)\n            qg = linear_reservoir(rgs_[i], cg, qg)\n        qs[i, :] = qs_[i, :, 0] + qi + qg\n    # seq, batch, feature\n    q_sim = torch.unsqueeze(qs, dim=2)\n    if return_state:\n        return q_sim, es, *w, s, fr, qi, qg\n    return q_sim, es\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj.ann_pbm","title":"<code>ann_pbm(dl_model, pb_model, param_func, x, z)</code>","text":"<p>Differential parameter learning</p> <p>z (normalized input) -&gt; ann -&gt; param -&gt; + x (not normalized) -&gt; pbm -&gt; q Parameters will be denormalized in pbm model</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.ann_pbm--parameters","title":"Parameters","text":"<p>dl_model     ann model pb_model     physics-based model param_func     function used to limit the range of params; now it is sigmoid or clamp function x     not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature] z     normalized data used for DL model; a 2-dim tensor. [batch, feature]</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.ann_pbm--returns","title":"Returns","text":"<p>torch.Tensor     one time forward result</p> Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>def ann_pbm(dl_model, pb_model, param_func, x, z):\n    \"\"\"\n    Differential parameter learning\n\n    z (normalized input) -&gt; ann -&gt; param -&gt; + x (not normalized) -&gt; pbm -&gt; q\n    Parameters will be denormalized in pbm model\n\n    Parameters\n    ----------\n    dl_model\n        ann model\n    pb_model\n        physics-based model\n    param_func\n        function used to limit the range of params; now it is sigmoid or clamp function\n    x\n        not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n    z\n        normalized data used for DL model; a 2-dim tensor. [batch, feature]\n\n    Returns\n    -------\n    torch.Tensor\n        one time forward result\n    \"\"\"\n    gen = dl_model(z)\n    if torch.isnan(gen).any():\n        raise ValueError(\"Error: NaN values detected. Check your data firstly!!!\")\n    # we set all params' values in [0, 1] and will scale them when forwarding\n    if param_func == \"sigmoid\":\n        params = F.sigmoid(gen)\n    elif param_func == \"clamp\":\n        params = torch.clamp(gen, min=0.0, max=1.0)\n    else:\n        raise NotImplementedError(\n            \"We don't provide this way to limit parameters' range!! Please choose sigmoid or clamp\"\n        )\n    return pb_model(x[:, :, : pb_model.feature_size], params)\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj.calculate_evap","title":"<code>calculate_evap(lm, c, wu0, wl0, prcp, pet)</code>","text":"<p>Three-layers evaporation model from \"Watershed Hydrologic Simulation\" written by Prof. RenJun Zhao.</p> <p>The book is Chinese, and its real name is \u300a\u6d41\u57df\u6c34\u6587\u6a21\u62df\u300b; The three-layers evaporation model is descibed in Page 76; The method is same with that in Page 22-23 in \"Hydrologic Forecasting (5-th version)\" written by Prof. Weimin Bao. This book's Chinese name is \u300a\u6c34\u6587\u9884\u62a5\u300b</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.calculate_evap--parameters","title":"Parameters","text":"<p>lm     average soil moisture storage capacity of lower layer (mm) c     coefficient of deep layer wu0     initial soil moisture of upper layer; update in each time step (mm) wl0     initial soil moisture of lower layer; update in each time step (mm) prcp     basin mean precipitation (mm/day) pet     potential evapotranspiration (mm/day)</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.calculate_evap--returns","title":"Returns","text":"<p>torch.Tensor     eu/el/ed are evaporation from upper/lower/deeper layer, respectively</p> Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>def calculate_evap(\n    lm, c, wu0, wl0, prcp, pet\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Three-layers evaporation model from \"Watershed Hydrologic Simulation\" written by Prof. RenJun Zhao.\n\n    The book is Chinese, and its real name is \u300a\u6d41\u57df\u6c34\u6587\u6a21\u62df\u300b;\n    The three-layers evaporation model is descibed in Page 76;\n    The method is same with that in Page 22-23 in \"Hydrologic Forecasting (5-th version)\" written by Prof. Weimin Bao.\n    This book's Chinese name is \u300a\u6c34\u6587\u9884\u62a5\u300b\n\n    Parameters\n    ----------\n    lm\n        average soil moisture storage capacity of lower layer (mm)\n    c\n        coefficient of deep layer\n    wu0\n        initial soil moisture of upper layer; update in each time step (mm)\n    wl0\n        initial soil moisture of lower layer; update in each time step (mm)\n    prcp\n        basin mean precipitation (mm/day)\n    pet\n        potential evapotranspiration (mm/day)\n\n    Returns\n    -------\n    torch.Tensor\n        eu/el/ed are evaporation from upper/lower/deeper layer, respectively\n    \"\"\"\n    tensor_min = torch.full(wu0.size(), 0.0).to(prcp.device)\n    # when using torch.where, please see here: https://github.com/pytorch/pytorch/issues/9190\n    # it's element-wise operation, no problem here. For example:\n    # In: torch.where(torch.Tensor([2,1])&gt;torch.Tensor([1,1]),torch.Tensor([1,2]),torch.Tensor([3,4]))\n    # Out: tensor([1., 4.])\n    eu = torch.where(wu0 + prcp &gt;= pet, pet, wu0 + prcp)\n    ed = torch.where(\n        (wl0 &lt; c * lm) &amp; (wl0 &lt; c * (pet - eu)), c * (pet - eu) - wl0, tensor_min\n    )\n    el = torch.where(\n        wu0 + prcp &gt;= pet,\n        tensor_min,\n        torch.where(\n            wl0 &gt;= c * lm,\n            (pet - eu) * wl0 / lm,\n            torch.where(wl0 &gt;= c * (pet - eu), c * (pet - eu), wl0),\n        ),\n    )\n    return eu, el, ed\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj.calculate_prcp_runoff","title":"<code>calculate_prcp_runoff(b, im, wm, w0, pe)</code>","text":"<p>Calculates the amount of runoff generated from rainfall after entering the underlying surface</p> <p>Same in \"Watershed Hydrologic Simulation\" and \"Hydrologic Forecasting (5-th version)\"</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.calculate_prcp_runoff--parameters","title":"Parameters","text":"<p>b     B exponent coefficient im     IMP imperiousness coefficient wm     average soil moisture storage capacity w0     initial soil moisture pe     net precipitation</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.calculate_prcp_runoff--returns","title":"Returns","text":"<p>torch.Tensor     r -- runoff; r_im -- runoff of impervious part</p> Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>def calculate_prcp_runoff(b, im, wm, w0, pe):\n    \"\"\"\n    Calculates the amount of runoff generated from rainfall after entering the underlying surface\n\n    Same in \"Watershed Hydrologic Simulation\" and \"Hydrologic Forecasting (5-th version)\"\n\n    Parameters\n    ----------\n    b\n        B exponent coefficient\n    im\n        IMP imperiousness coefficient\n    wm\n        average soil moisture storage capacity\n    w0\n        initial soil moisture\n    pe\n        net precipitation\n\n    Returns\n    -------\n    torch.Tensor\n        r -- runoff; r_im -- runoff of impervious part\n    \"\"\"\n    wmm = wm * (1 + b)\n    a = wmm * (1 - (1 - w0 / wm) ** (1 / (1 + b)))\n    if any(torch.isnan(a)):\n        raise ValueError(\n            \"Error: NaN values detected. Try set clamp function or check your data!!!\"\n        )\n    r_cal = torch.where(\n        pe &gt; 0.0,\n        torch.where(\n            pe + a &lt; wmm,\n            # torch.clamp is used for gradient not to be NaN, see more in xaj_sources function\n            pe - (wm - w0) + wm * (1 - torch.clamp(a + pe, max=wmm) / wmm) ** (1 + b),\n            pe - (wm - w0),\n        ),\n        torch.full(pe.size(), 0.0).to(pe.device),\n    )\n    if any(torch.isnan(r_cal)):\n        raise ValueError(\n            \"Error: NaN values detected. Try set clamp function or check your data!!!\"\n        )\n    r = torch.clamp(r_cal, min=0.0)\n    r_im_cal = pe * im\n    r_im = torch.clamp(r_im_cal, min=0.0)\n    return r, r_im\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj.calculate_w_storage","title":"<code>calculate_w_storage(um, lm, dm, wu0, wl0, wd0, eu, el, ed, pe, r)</code>","text":"<p>Update the soil moisture values of the three layers.</p> <p>According to the runoff-generation equation 2.60 in the book \"SHUIWENYUBAO\", dW = dPE - dR</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.calculate_w_storage--parameters","title":"Parameters","text":"<p>um     average soil moisture storage capacity of the upper layer (mm) lm     average soil moisture storage capacity of the lower layer (mm) dm     average soil moisture storage capacity of the deep layer (mm) wu0     initial values of soil moisture in upper layer wl0     initial values of soil moisture in lower layer wd0     initial values of soil moisture in deep layer eu     evaporation of the upper layer; it isn't used in this function el     evaporation of the lower layer ed     evaporation of the deep layer pe     net precipitation; it is able to be negative value in this function r     runoff</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.calculate_w_storage--returns","title":"Returns","text":"<p>torch.Tensor     wu,wl,wd -- soil moisture in upper, lower and deep layer</p> Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>def calculate_w_storage(um, lm, dm, wu0, wl0, wd0, eu, el, ed, pe, r):\n    \"\"\"\n    Update the soil moisture values of the three layers.\n\n    According to the runoff-generation equation 2.60 in the book \"SHUIWENYUBAO\", dW = dPE - dR\n\n    Parameters\n    ----------\n    um\n        average soil moisture storage capacity of the upper layer (mm)\n    lm\n        average soil moisture storage capacity of the lower layer (mm)\n    dm\n        average soil moisture storage capacity of the deep layer (mm)\n    wu0\n        initial values of soil moisture in upper layer\n    wl0\n        initial values of soil moisture in lower layer\n    wd0\n        initial values of soil moisture in deep layer\n    eu\n        evaporation of the upper layer; it isn't used in this function\n    el\n        evaporation of the lower layer\n    ed\n        evaporation of the deep layer\n    pe\n        net precipitation; it is able to be negative value in this function\n    r\n        runoff\n\n    Returns\n    -------\n    torch.Tensor\n        wu,wl,wd -- soil moisture in upper, lower and deep layer\n    \"\"\"\n    xaj_device = pe.device\n    tensor_zeros = torch.full(wu0.size(), 0.0).to(xaj_device)\n    # pe&gt;0: the upper soil moisture was added firstly, then lower layer, and the final is deep layer\n    # pe&lt;=0: no additional water, just remove evapotranspiration,\n    # but note the case: e &gt;= p &gt; 0\n    # (1) if wu0 + p &gt; e, then e = eu (2) else, wu must be zero\n    wu = torch.where(\n        pe &gt; 0.0,\n        torch.where(wu0 + pe - r &lt; um, wu0 + pe - r, um),\n        torch.where(wu0 + pe &gt; 0.0, wu0 + pe, tensor_zeros),\n    )\n    # calculate wd before wl because it is easier to cal using where statement\n    wd = torch.where(\n        pe &gt; 0.0,\n        torch.where(\n            wu0 + wl0 + pe - r &gt; um + lm, wu0 + wl0 + wd0 + pe - r - um - lm, wd0\n        ),\n        wd0 - ed,\n    )\n    # water balance (equation 2.2 in Page 13, also shown in Page 23)\n    # if wu0 + p &gt; e, then e = eu; else p must be used in upper layer,\n    # so no matter what the case is, el didn't include p, neither ed\n    wl = torch.where(pe &gt; 0.0, wu0 + wl0 + wd0 + pe - r - wu - wd, wl0 - el)\n    # the water storage should be in reasonable range\n    tensor_mins = torch.full(um.size(), 0.0).to(xaj_device)\n    wu_ = torch.clamp(wu, min=tensor_mins, max=um)\n    wl_ = torch.clamp(wl, min=tensor_mins, max=lm)\n    wd_ = torch.clamp(wd, min=tensor_mins, max=dm)\n    return wu_, wl_, wd_\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj.linear_reservoir","title":"<code>linear_reservoir(x, weight, last_y=None)</code>","text":"<p>Linear reservoir's release function</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.linear_reservoir--parameters","title":"Parameters","text":"<p>x     the input to the linear reservoir weight     the coefficient of linear reservoir last_y     the output of last period</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.linear_reservoir--returns","title":"Returns","text":"<p>torch.Tensor     one-step forward result</p> Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>def linear_reservoir(x, weight, last_y: Optional[Tensor] = None):\n    \"\"\"\n    Linear reservoir's release function\n\n    Parameters\n    ----------\n    x\n        the input to the linear reservoir\n    weight\n        the coefficient of linear reservoir\n    last_y\n        the output of last period\n\n    Returns\n    -------\n    torch.Tensor\n        one-step forward result\n    \"\"\"\n    weight1 = 1 - weight\n    if last_y is None:\n        last_y = torch.full(weight.size(), 0.001).to(x.device)\n    return weight * last_y + weight1 * x\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj.lstm_pbm","title":"<code>lstm_pbm(dl_model, pb_model, param_func, x, z)</code>","text":"<p>Differential parameter learning</p> <p>z (normalized input) -&gt; lstm -&gt; param -&gt; + x (not normalized) -&gt; pbm -&gt; q Parameters will be denormalized in pbm model</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.lstm_pbm--parameters","title":"Parameters","text":"<p>dl_model     lstm model pb_model     physics-based model param_func     function used to limit the range of params; now it is sigmoid or clamp function x     not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature] z     normalized data used for DL model; a sequence-first 3-dim tensor. [sequence, batch, feature]</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.lstm_pbm--returns","title":"Returns","text":"<p>torch.Tensor         one time forward result</p> Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>def lstm_pbm(dl_model, pb_model, param_func, x, z):\n    \"\"\"\n    Differential parameter learning\n\n    z (normalized input) -&gt; lstm -&gt; param -&gt; + x (not normalized) -&gt; pbm -&gt; q\n    Parameters will be denormalized in pbm model\n\n    Parameters\n    ----------\n    dl_model\n        lstm model\n    pb_model\n        physics-based model\n    param_func\n        function used to limit the range of params; now it is sigmoid or clamp function\n    x\n        not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n    z\n        normalized data used for DL model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n\n    Returns\n    -------\n    torch.Tensor\n            one time forward result\n    \"\"\"\n    gen = dl_model(z)\n    if torch.isnan(gen).any():\n        raise ValueError(\"Error: NaN values detected. Check your data firstly!!!\")\n    # we set all params' values in [0, 1] and will scale them when forwarding\n    if param_func == \"sigmoid\":\n        params_ = F.sigmoid(gen)\n    elif param_func == \"clamp\":\n        params_ = torch.clamp(gen, min=0.0, max=1.0)\n    else:\n        raise NotImplementedError(\n            \"We don't provide this way to limit parameters' range!! Please choose sigmoid or clamp\"\n        )\n    # just get one-period values, here we use the final period's values\n    params = params_[-1, :, :]\n    return pb_model(x[:, :, : pb_model.feature_size], params)\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj.xaj_generation","title":"<code>xaj_generation(p_and_e, k, b, im, um, lm, dm, c, wu0=None, wl0=None, wd0=None)</code>","text":"<p>Single-step runoff generation in XAJ.</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.xaj_generation--parameters","title":"Parameters","text":"<p>p_and_e     precipitation and potential evapotranspiration (mm/day) k     ratio of potential evapotranspiration to reference crop evaporation b     exponent parameter um     average soil moisture storage capacity of the upper layer (mm) lm     average soil moisture storage capacity of the lower layer (mm) dm     average soil moisture storage capacity of the deep layer (mm) im     impermeability coefficient c     coefficient of deep layer wu0     initial values of soil moisture in upper layer (mm) wl0     initial values of soil moisture in lower layer (mm) wd0     initial values of soil moisture in deep layer (mm)</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.xaj_generation--returns","title":"Returns","text":"<p>tuple[torch.Tensor]     (r, rim, e, pe), (wu, wl, wd)</p> Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>def xaj_generation(\n    p_and_e: Tensor,\n    k,\n    b,\n    im,\n    um,\n    lm,\n    dm,\n    c,\n    wu0: Tensor = None,\n    wl0: Tensor = None,\n    wd0: Tensor = None,\n) -&gt; tuple:\n    \"\"\"\n    Single-step runoff generation in XAJ.\n\n    Parameters\n    ----------\n    p_and_e\n        precipitation and potential evapotranspiration (mm/day)\n    k\n        ratio of potential evapotranspiration to reference crop evaporation\n    b\n        exponent parameter\n    um\n        average soil moisture storage capacity of the upper layer (mm)\n    lm\n        average soil moisture storage capacity of the lower layer (mm)\n    dm\n        average soil moisture storage capacity of the deep layer (mm)\n    im\n        impermeability coefficient\n    c\n        coefficient of deep layer\n    wu0\n        initial values of soil moisture in upper layer (mm)\n    wl0\n        initial values of soil moisture in lower layer (mm)\n    wd0\n        initial values of soil moisture in deep layer (mm)\n\n    Returns\n    -------\n    tuple[torch.Tensor]\n        (r, rim, e, pe), (wu, wl, wd)\n    \"\"\"\n    # make sure physical variables' value ranges are correct\n    prcp = torch.clamp(p_and_e[:, 0], min=0.0)\n    pet = torch.clamp(p_and_e[:, 1] * k, min=0.0)\n    # wm\n    wm = um + lm + dm\n    if wu0 is None:\n        # use detach func to make wu0 no_grad as it is an initial value\n        wu0 = 0.6 * (um.detach())\n    if wl0 is None:\n        wl0 = 0.6 * (lm.detach())\n    if wd0 is None:\n        wd0 = 0.6 * (dm.detach())\n    w0_ = wu0 + wl0 + wd0\n    # w0 need locate in correct range so that following calculation could be right\n    # To make sure the gradient is also not NaN (see case in xaj_sources),\n    # we'd better minus a precision (1e-5), although we've not met this situation (grad is NaN)\n    w0 = torch.clamp(w0_, max=wm - 1e-5)\n\n    # Calculate the amount of evaporation from storage\n    eu, el, ed = calculate_evap(lm, c, wu0, wl0, prcp, pet)\n    e = eu + el + ed\n\n    # Calculate the runoff generated by net precipitation\n    prcp_difference = prcp - e\n    pe = torch.clamp(prcp_difference, min=0.0)\n    r, rim = calculate_prcp_runoff(b, im, wm, w0, pe)\n    # Update wu, wl, wd;\n    # we use prcp_difference rather than pe, as when pe&lt;0 but prcp&gt;0, prcp should be considered\n    wu, wl, wd = calculate_w_storage(\n        um, lm, dm, wu0, wl0, wd0, eu, el, ed, prcp_difference, r\n    )\n\n    return (r, rim, e, pe), (wu, wl, wd)\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj.xaj_sources","title":"<code>xaj_sources(pe, r, sm, ex, ki, kg, s0=None, fr0=None, book='HF')</code>","text":"<p>Divide the runoff to different sources</p> <p>We use the initial version from the paper of the inventor of the XAJ model -- Prof. Renjun Zhao: \"Analysis of parameters of the XinAnJiang model\". Its Chinese name is &lt;&lt;\u65b0\u5b89\u6c5f\u6a21\u578b\u53c2\u6570\u7684\u5206\u6790&gt;&gt;, which could be found by searching in \"Baidu Xueshu\". The module's code can also be found in \"Watershed Hydrologic Simulation\" (WHS) Page 174. It is nearly same with that in \"Hydrologic Forecasting\" (HF) Page 148-149 We use the period average runoff as input and the unit period is day so we don't need to difference it as books show</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.xaj_sources--parameters","title":"Parameters","text":"<p>pe     net precipitation (mm/day) r     runoff from xaj_generation (mm/day) sm     areal mean free water capacity of the surface layer (mm) ex     exponent of the free water capacity curve ki     outflow coefficients of the free water storage to interflow relationships kg     outflow coefficients of the free water storage to groundwater relationships s0     initial free water capacity (mm) fr0     runoff area of last period</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.xaj_sources--return","title":"Return","text":"<p>torch.Tensor     rs -- surface runoff; ri-- interflow runoff; rg -- groundwater runoff</p> Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>def xaj_sources(\n    pe,\n    r,\n    sm,\n    ex,\n    ki,\n    kg,\n    s0: Optional[Tensor] = None,\n    fr0: Optional[Tensor] = None,\n    book=\"HF\",\n) -&gt; tuple:\n    \"\"\"\n    Divide the runoff to different sources\n\n    We use the initial version from the paper of the inventor of the XAJ model -- Prof. Renjun Zhao:\n    \"Analysis of parameters of the XinAnJiang model\". Its Chinese name is &lt;&lt;\u65b0\u5b89\u6c5f\u6a21\u578b\u53c2\u6570\u7684\u5206\u6790&gt;&gt;,\n    which could be found by searching in \"Baidu Xueshu\".\n    The module's code can also be found in \"Watershed Hydrologic Simulation\" (WHS) Page 174.\n    It is nearly same with that in \"Hydrologic Forecasting\" (HF) Page 148-149\n    We use the period average runoff as input and the unit period is day so we don't need to difference it as books show\n\n\n    Parameters\n    ------------\n    pe\n        net precipitation (mm/day)\n    r\n        runoff from xaj_generation (mm/day)\n    sm\n        areal mean free water capacity of the surface layer (mm)\n    ex\n        exponent of the free water capacity curve\n    ki\n        outflow coefficients of the free water storage to interflow relationships\n    kg\n        outflow coefficients of the free water storage to groundwater relationships\n    s0\n        initial free water capacity (mm)\n    fr0\n        runoff area of last period\n\n    Return\n    ------------\n    torch.Tensor\n        rs -- surface runoff; ri-- interflow runoff; rg -- groundwater runoff\n\n    \"\"\"\n    xaj_device = pe.device\n    # maximum free water storage capacity in a basin\n    ms = sm * (1 + ex)\n    if fr0 is None:\n        fr0 = torch.full(sm.shape[0], 0.1).to(xaj_device)\n    if s0 is None:\n        s0 = 0.5 * (sm.clone().detach())\n    # For free water storage, because s is related to fr and s0 and fr0 are both values of last period,\n    # we have to trans the initial value of s from last period to this one.\n    # both WHS\uff08\u6d41\u57df\u6c34\u6587\u6a21\u62df\uff09's sample code and HF\uff08\u6c34\u6587\u9884\u62a5\uff09 use s = fr0 * s0 / fr.\n    # I think they both think free water reservoir as a cubic tank. Its height is s and area of bottom rectangle is fr\n    # we will have a cubic tank with varying bottom and height,\n    # and fixed boundary (in HF sm is fixed) or none-fixed boundary (in EH smmf is not fixed)\n    # but notice r's list like\" [1,0] which 1 is the 1st period's runoff and 0 is the 2nd period's runoff\n    # after 1st period, the s1 could not be zero, but in the 2nd period, fr=0, then we cannot set s=0, because some water still in the tank\n    # fr's formula could be found in Eq. 9 in \"Analysis of parameters of the XinAnJiang model\",\n    # Here our r doesn't include rim, so there is no need to remove rim from r; this is also the method in HF\n    # Moreover, to make sure ss is not larger than sm, otherwise au will be nan value.\n    # It is worth to note that we have to use a precision here -- 1e-5, otherwise the gradient will be NaN;\n    # I guess maybe when calculating gradient -- \u0394y/\u0394x, \u0394 brings some precision problem when we need exponent function.\n\n    # NOTE: when r is 0, fr should be 0, however, s1 may not be zero and it still hold some water,\n    # then fr can not be 0, otherwise when fr is used as denominator it lead to error,\n    # so we have to deal with this case later, for example, when r=0, we cannot use pe * fr to replace r\n    # because fr get the value of last period, and it is not 0\n\n    # cannot use torch.where, because it will cause some error when calculating gradient\n    # fr = torch.where(r &gt; 0.0, r / pe, fr0)\n    # fr just use fr0, and it can be included in the computation graph, so we don't detach it\n    fr = torch.clone(fr0)\n    fr_mask = r &gt; 0.0\n    fr[fr_mask] = r[fr_mask] / pe[fr_mask]\n    if any(torch.isnan(fr)):\n        raise ValueError(\n            \"Error: NaN values detected. Try set clamp function or check your data!!!\"\n        )\n    if any(fr == 0.0):\n        raise ArithmeticError(\n            \"Please check fr's value, fr==0.0 will cause error in the next step!\"\n        )\n    ss = torch.clone(s0)\n    s = torch.clone(s0)\n\n    ss[fr_mask] = fr0[fr_mask] * s0[fr_mask] / fr[fr_mask]\n\n    if book == \"HF\":\n        ss = torch.clamp(ss, max=sm - PRECISION)\n        au = ms * (1.0 - (1.0 - ss / sm) ** (1.0 / (1.0 + ex)))\n        if any(torch.isnan(au)):\n            raise ValueError(\n                \"Error: NaN values detected. Try set clamp function or check your data!!!\"\n            )\n\n        rs = torch.full_like(r, 0.0, device=xaj_device)\n        rs[fr_mask] = torch.where(\n            pe[fr_mask] + au[fr_mask] &lt; ms[fr_mask],\n            # equation 2-85 in HF\n            # it's weird here, but we have to clamp so that the gradient could be not NaN;\n            # otherwise, even the forward calculation is correct, the gradient is still NaN;\n            # maybe when calculating gradient -- \u0394y/\u0394x, \u0394 brings some precision problem\n            # if we need exponent function.\n            fr[fr_mask]\n            * (\n                pe[fr_mask]\n                - sm[fr_mask]\n                + ss[fr_mask]\n                + sm[fr_mask]\n                * (\n                    (\n                        1\n                        - torch.clamp(pe[fr_mask] + au[fr_mask], max=ms[fr_mask])\n                        / ms[fr_mask]\n                    )\n                    ** (1 + ex[fr_mask])\n                )\n            ),\n            # equation 2-86 in HF\n            fr[fr_mask] * (pe[fr_mask] + ss[fr_mask] - sm[fr_mask]),\n        )\n        rs = torch.clamp(rs, max=r)\n        # ri's mask is not same as rs's, because last period's s may not be 0\n        # and in this time, ri and rg could be larger than 0\n        # we need firstly calculate the updated s, s's mask is same as fr_mask,\n        # when r==0, then s will be equal to last period's\n        # equation 2-87 in HF, some free water leave or save, so we update free water storage\n        s[fr_mask] = ss[fr_mask] + (r[fr_mask] - rs[fr_mask]) / fr[fr_mask]\n        s = torch.clamp(s, max=sm)\n    elif book == \"EH\":\n        smmf = ms * (1 - (1 - fr) ** (1 / ex))\n        smf = smmf / (1 + ex)\n        ss = torch.clamp(ss, max=smf - PRECISION)\n        au = smmf * (1 - (1 - ss / smf) ** (1 / (1 + ex)))\n        if torch.isnan(au).any():\n            raise ArithmeticError(\n                \"Error: NaN values detected. Try set clip function or check your data!!!\"\n            )\n        rs = torch.full_like(r, 0.0, device=xaj_device)\n        rs[fr_mask] = torch.where(\n            pe[fr_mask] + au[fr_mask] &lt; smmf[fr_mask],\n            (\n                pe[fr_mask]\n                - smf[fr_mask]\n                + ss[fr_mask]\n                + smf[fr_mask]\n                * (\n                    1\n                    - torch.clamp(\n                        pe[fr_mask] + au[fr_mask],\n                        max=smmf[fr_mask],\n                    )\n                    / smmf[fr_mask]\n                )\n                ** (ex[fr_mask] + 1)\n            )\n            * fr[fr_mask],\n            (pe[fr_mask] + ss[fr_mask] - smf[fr_mask]) * fr[fr_mask],\n        )\n        rs = torch.clamp(rs, max=r)\n        s[fr_mask] = ss[fr_mask] + (r[fr_mask] - rs[fr_mask]) / fr[fr_mask]\n        s[fr_mask] = torch.clamp(s[fr_mask], max=smf[fr_mask])\n        s = torch.clamp(s, max=smf)\n    else:\n        raise ValueError(\"Please set book as 'HF' or 'EH'!\")\n    # equation 2-88 in HF, next interflow and ground water will be released from the updated free water storage\n    # We use the period average runoff as input and the unit period is day.\n    # Hence, we directly use ki and kg rather than ki_{\u0394t} in books.\n    ri = ki * s * fr\n    rg = kg * s * fr\n    # equation 2-89 in HF; although it looks different with that in WHS, they are actually same\n    # Finally, calculate the final free water storage\n    s1 = s * (1 - ki - kg)\n    return (rs, ri, rg), (s1, fr)\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj.xaj_sources5mm","title":"<code>xaj_sources5mm(pe, runoff, sm, ex, ki, kg, s0=None, fr0=None, book='HF')</code>","text":"<p>Divide the runoff to different sources according to books -- \u300a\u6c34\u6587\u9884\u62a5\u300bHF 5th edition and \u300a\u5de5\u7a0b\u6c34\u6587\u5b66\u300bEH 3rd edition</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.xaj_sources5mm--parameters","title":"Parameters","text":"<p>pe     net precipitation runoff     runoff from xaj_generation sm     areal mean free water capacity of the surface layer ex     exponent of the free water capacity curve ki     outflow coefficients of the free water storage to interflow relationships kg     outflow coefficients of the free water storage to groundwater relationships s0     initial free water capacity fr0     initial area of generation time_interval_hours     \u7531\u4e8eKi\u3001Kg\u3001Ci\u3001Cg\u90fd\u662f\u4ee524\u5c0f\u65f6\u4e3a\u65f6\u6bb5\u957f\u5b9a\u4e49\u7684,\u9700\u6839\u636e\u65f6\u6bb5\u957f\u8f6c\u6362 book     the methods in \u300a\u6c34\u6587\u9884\u62a5\u300bHF 5th edition and \u300a\u5de5\u7a0b\u6c34\u6587\u5b66\u300bEH 3rd edition are different,     hence, both are provided, and the default is the former -- \"ShuiWenYuBao\";     the other one is \"GongChengShuiWenXue\"</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj.xaj_sources5mm--returns","title":"Returns","text":"<p>tuple[tuple, tuple]     rs_s -- surface runoff; rss_s-- interflow runoff; rg_s -- groundwater runoff;     (fr_ds[-1], s_ds[-1]): state variables' final value;     all variables are numpy array</p> Source code in <code>torchhydro/models/dpl4xaj.py</code> <pre><code>def xaj_sources5mm(\n    pe,\n    runoff,\n    sm,\n    ex,\n    ki,\n    kg,\n    s0=None,\n    fr0=None,\n    book=\"HF\",\n):\n    \"\"\"\n    Divide the runoff to different sources according to books -- \u300a\u6c34\u6587\u9884\u62a5\u300bHF 5th edition and \u300a\u5de5\u7a0b\u6c34\u6587\u5b66\u300bEH 3rd edition\n\n    Parameters\n    ----------\n    pe\n        net precipitation\n    runoff\n        runoff from xaj_generation\n    sm\n        areal mean free water capacity of the surface layer\n    ex\n        exponent of the free water capacity curve\n    ki\n        outflow coefficients of the free water storage to interflow relationships\n    kg\n        outflow coefficients of the free water storage to groundwater relationships\n    s0\n        initial free water capacity\n    fr0\n        initial area of generation\n    time_interval_hours\n        \u7531\u4e8eKi\u3001Kg\u3001Ci\u3001Cg\u90fd\u662f\u4ee524\u5c0f\u65f6\u4e3a\u65f6\u6bb5\u957f\u5b9a\u4e49\u7684,\u9700\u6839\u636e\u65f6\u6bb5\u957f\u8f6c\u6362\n    book\n        the methods in \u300a\u6c34\u6587\u9884\u62a5\u300bHF 5th edition and \u300a\u5de5\u7a0b\u6c34\u6587\u5b66\u300bEH 3rd edition are different,\n        hence, both are provided, and the default is the former -- \"ShuiWenYuBao\";\n        the other one is \"GongChengShuiWenXue\"\n\n    Returns\n    -------\n    tuple[tuple, tuple]\n        rs_s -- surface runoff; rss_s-- interflow runoff; rg_s -- groundwater runoff;\n        (fr_ds[-1], s_ds[-1]): state variables' final value;\n        all variables are numpy array\n    \"\"\"\n    xaj_device = pe.device\n    # \u6d41\u57df\u6700\u5927\u70b9\u81ea\u7531\u6c34\u84c4\u6c34\u5bb9\u91cf\u6df1\n    smm = sm * (1 + ex)\n    if fr0 is None:\n        fr0 = torch.full_like(sm, 0.1, device=xaj_device)\n    if s0 is None:\n        s0 = 0.5 * (sm.clone().detach())\n    fr = torch.clone(fr0)\n    fr_mask = runoff &gt; 0.0\n    fr[fr_mask] = runoff[fr_mask] / pe[fr_mask]\n    if torch.all(runoff &lt; 5):\n        n = 1\n    else:\n        r_max = torch.max(runoff).detach().cpu().numpy()\n        residue_temp = r_max % 5\n        if residue_temp != 0:\n            residue_temp = 1\n        n = int(r_max / 5) + residue_temp\n    rn = runoff / n\n    pen = pe / n\n    kss_d = (1 - (1 - (ki + kg)) ** (1 / n)) / (1 + kg / ki)\n    kg_d = kss_d * kg / ki\n    if torch.isnan(kss_d).any() or torch.isnan(kg_d).any():\n        raise ValueError(\"Error: NaN values detected. Check your parameters setting!!!\")\n    # kss_d = ki\n    # kg_d = kg\n\n    rs = torch.full_like(runoff, 0.0, device=xaj_device)\n    rss = torch.full_like(runoff, 0.0, device=xaj_device)\n    rg = torch.full_like(runoff, 0.0, device=xaj_device)\n\n    s_ds = []\n    fr_ds = []\n    s_ds.append(s0)\n    fr_ds.append(fr0)\n    for j in range(n):\n        fr0_d = fr_ds[j]\n        s0_d = s_ds[j]\n        # equation 5-32 in HF, but strange, cause each period, rn/pen is same\n        # fr_d = torch.full_like(fr0_d, PRECISION, device=xaj_device)\n        # fr_d_mask = fr &gt; PRECISION\n        # fr_d[fr_d_mask] = 1 - (1 - fr[fr_d_mask]) ** (1 / n)\n        fr_d = fr\n\n        ss_d = torch.clone(s0_d)\n        s_d = torch.clone(s0_d)\n\n        ss_d[fr_mask] = fr0_d[fr_mask] * s0_d[fr_mask] / fr_d[fr_mask]\n\n        if book == \"HF\":\n            # ms = smm\n            ss_d = torch.clamp(ss_d, max=sm - PRECISION)\n            au = smm * (1.0 - (1.0 - ss_d / sm) ** (1.0 / (1.0 + ex)))\n            if torch.isnan(au).any():\n                raise ValueError(\n                    \"Error: NaN values detected. Try set clip function or check your data!!!\"\n                )\n            rs_j = torch.full_like(rn, 0.0, device=xaj_device)\n            rs_j[fr_mask] = torch.where(\n                pen[fr_mask] + au[fr_mask] &lt; smm[fr_mask],\n                # equation 5-26 in HF\n                fr_d[fr_mask]\n                * (\n                    pen[fr_mask]\n                    - sm[fr_mask]\n                    + ss_d[fr_mask]\n                    + sm[fr_mask]\n                    * (\n                        (\n                            1\n                            - torch.clamp(pen[fr_mask] + au[fr_mask], max=smm[fr_mask])\n                            / smm[fr_mask]\n                        )\n                        ** (1 + ex[fr_mask])\n                    )\n                ),\n                # equation 5-27 in HF\n                fr_d[fr_mask] * (pen[fr_mask] + ss_d[fr_mask] - sm[fr_mask]),\n            )\n            rs_j = torch.clamp(rs_j, max=rn)\n            s_d[fr_mask] = ss_d[fr_mask] + (rn[fr_mask] - rs_j[fr_mask]) / fr_d[fr_mask]\n            s_d = torch.clamp(s_d, max=sm)\n\n        elif book == \"EH\":\n            smmf = smm * (1 - (1 - fr_d) ** (1 / ex))\n            smf = smmf / (1 + ex)\n            ss_d = torch.clamp(ss_d, max=smf - PRECISION)\n            au = smmf * (1 - (1 - ss_d / smf) ** (1 / (1 + ex)))\n            if torch.isnan(au).any():\n                raise ValueError(\n                    \"Error: NaN values detected. Try set clip function or check your data!!!\"\n                )\n            rs_j = torch.full(rn.size(), 0.0).to(xaj_device)\n            rs_j[fr_mask] = torch.where(\n                pen[fr_mask] + au[fr_mask] &lt; smmf[fr_mask],\n                (\n                    pen[fr_mask]\n                    - smf[fr_mask]\n                    + ss_d[fr_mask]\n                    + smf[fr_mask]\n                    * (\n                        1\n                        - torch.clamp(\n                            pen[fr_mask] + au[fr_mask],\n                            max=smmf[fr_mask],\n                        )\n                        / smmf[fr_mask]\n                    )\n                    ** (ex[fr_mask] + 1)\n                )\n                * fr_d[fr_mask],\n                (pen[fr_mask] + ss_d[fr_mask] - smf[fr_mask]) * fr_d[fr_mask],\n            )\n            rs_j = torch.clamp(rs_j, max=rn)\n            s_d[fr_mask] = ss_d[fr_mask] + (rn[fr_mask] - rs_j[fr_mask]) / fr_d[fr_mask]\n            s_d = torch.clamp(s_d, max=smf)\n        else:\n            raise NotImplementedError(\n                \"We don't have this implementation! Please chose 'HF' or 'EH'!!\"\n            )\n\n        rss_j = s_d * kss_d * fr_d\n        rg_j = s_d * kg_d * fr_d\n        s1_d = s_d * (1 - kss_d - kg_d)\n\n        rs = rs + rs_j\n        rss = rss + rss_j\n        rg = rg + rg_j\n        # \u8d4b\u503cs_d\u548cfr_d\u5230\u6570\u7ec4\u4e2d\uff0c\u4ee5\u7ed9\u4e0b\u4e00\u6bb5\u505a\u521d\u503c\n        s_ds.append(s1_d)\n        fr_ds.append(fr_d)\n\n    return (rs, rss, rg), (s_ds[-1], fr_ds[-1])\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et","title":"<code>dpl4xaj_nn4et</code>","text":"<p>The method is similar with dpl4xaj.py. The difference between dpl4xaj and dpl4xaj_nn4et is: in the former, the parameter of PBM is only one output of a DL model, while in the latter, time series output of a DL model can be as parameter of PBM and some modules could be replaced with neural networks</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.DplLstmNnModuleXaj","title":"<code> DplLstmNnModuleXaj            (Module)         </code>","text":"Source code in <code>torchhydro/models/dpl4xaj_nn4et.py</code> <pre><code>class DplLstmNnModuleXaj(nn.Module):\n    def __init__(\n        self,\n        n_input_features,\n        n_output_features,\n        n_hidden_states,\n        kernel_size,\n        warmup_length,\n        param_limit_func=\"clamp\",\n        param_test_way=\"final\",\n        param_var_index=None,\n        source_book=\"HF\",\n        source_type=\"sources\",\n        nn_hidden_size=None,\n        nn_dropout=0.2,\n        et_output=3,\n        return_et=True,\n    ):\n        \"\"\"\n        Differential Parameter learning model: LSTM -&gt; Param -&gt; XAJ\n\n        The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z\n\n        Parameters\n        ----------\n        n_input_features\n            the number of input features of LSTM\n        n_output_features\n            the number of output features of LSTM, and it should be equal to the number of learning parameters in XAJ\n        n_hidden_states\n            the number of hidden features of LSTM\n        kernel_size\n            the time length of unit hydrograph\n        warmup_length\n            the length of warmup periods;\n            hydrologic models need a warmup period to generate reasonable initial state values\n        param_limit_func\n            function used to limit the range of params; now it is sigmoid or clamp function\n        param_test_way\n            how we use parameters from dl model when testing;\n            now we have three ways:\n            1. \"final\" -- use the final period's parameter for each period\n            2. \"mean_time\" -- Mean values of all periods' parameters is used\n            3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used\n            but remember these ways are only for non-variable parameters\n        param_var_index\n            variable parameters' indices in all parameters\n        return_et\n            if True, return evapotranspiration\n        \"\"\"\n        if param_var_index is None:\n            param_var_index = [0, 1, 6]\n        if nn_hidden_size is None:\n            nn_hidden_size = [16, 8]\n        super(DplLstmNnModuleXaj, self).__init__()\n        self.dl_model = SimpleLSTM(n_input_features, n_output_features, n_hidden_states)\n        self.pb_model = Xaj4DplWithNnModule(\n            kernel_size,\n            warmup_length,\n            source_book=source_book,\n            source_type=source_type,\n            nn_hidden_size=nn_hidden_size,\n            nn_dropout=nn_dropout,\n            et_output=et_output,\n            param_var_index=param_var_index,\n            param_test_way=param_test_way,\n        )\n        self.param_func = param_limit_func\n        self.param_test_way = param_test_way\n        self.param_var_index = param_var_index\n        self.return_et = return_et\n\n    def forward(self, x, z):\n        \"\"\"\n        Differential parameter learning\n\n        z (normalized input) -&gt; lstm -&gt; param -&gt; + x (not normalized) -&gt; xaj -&gt; q\n        Parameters will be denormalized in xaj model\n\n        Parameters\n        ----------\n        x\n            not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n        z\n            normalized data used for DL model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n\n        Returns\n        -------\n        torch.Tensor\n            one time forward result\n        \"\"\"\n        gen = self.dl_model(z)\n        if torch.isnan(gen).any():\n            raise ValueError(\"Error: NaN values detected. Check your data firstly!!!\")\n        # we set all params' values in [0, 1] and will scale them when forwarding\n        if self.param_func == \"sigmoid\":\n            params = F.sigmoid(gen)\n        elif self.param_func == \"clamp\":\n            params = torch.clamp(gen, min=0.0, max=1.0)\n        else:\n            raise NotImplementedError(\n                \"We don't provide this way to limit parameters' range!! Please choose sigmoid or clamp\"\n            )\n        # just get one-period values, here we use the final period's values,\n        # when the MODEL_PARAM_TEST_WAY is not time_varing, we use the last period's values.\n        if self.param_test_way != MODEL_PARAM_TEST_WAY[\"time_varying\"]:\n            params = params[-1, :, :]\n        # Please put p in the first location and pet in the second\n        q, e = self.pb_model(x[:, :, : self.pb_model.feature_size], params)\n        return torch.cat([q, e], dim=-1) if self.return_et else q\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.DplLstmNnModuleXaj.__init__","title":"<code>__init__(self, n_input_features, n_output_features, n_hidden_states, kernel_size, warmup_length, param_limit_func='clamp', param_test_way='final', param_var_index=None, source_book='HF', source_type='sources', nn_hidden_size=None, nn_dropout=0.2, et_output=3, return_et=True)</code>  <code>special</code>","text":"<p>Differential Parameter learning model: LSTM -&gt; Param -&gt; XAJ</p> <p>The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.DplLstmNnModuleXaj.__init__--parameters","title":"Parameters","text":"<p>n_input_features     the number of input features of LSTM n_output_features     the number of output features of LSTM, and it should be equal to the number of learning parameters in XAJ n_hidden_states     the number of hidden features of LSTM kernel_size     the time length of unit hydrograph warmup_length     the length of warmup periods;     hydrologic models need a warmup period to generate reasonable initial state values param_limit_func     function used to limit the range of params; now it is sigmoid or clamp function param_test_way     how we use parameters from dl model when testing;     now we have three ways:     1. \"final\" -- use the final period's parameter for each period     2. \"mean_time\" -- Mean values of all periods' parameters is used     3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used     but remember these ways are only for non-variable parameters param_var_index     variable parameters' indices in all parameters return_et     if True, return evapotranspiration</p> Source code in <code>torchhydro/models/dpl4xaj_nn4et.py</code> <pre><code>def __init__(\n    self,\n    n_input_features,\n    n_output_features,\n    n_hidden_states,\n    kernel_size,\n    warmup_length,\n    param_limit_func=\"clamp\",\n    param_test_way=\"final\",\n    param_var_index=None,\n    source_book=\"HF\",\n    source_type=\"sources\",\n    nn_hidden_size=None,\n    nn_dropout=0.2,\n    et_output=3,\n    return_et=True,\n):\n    \"\"\"\n    Differential Parameter learning model: LSTM -&gt; Param -&gt; XAJ\n\n    The principle can be seen here: https://doi.org/10.1038/s41467-021-26107-z\n\n    Parameters\n    ----------\n    n_input_features\n        the number of input features of LSTM\n    n_output_features\n        the number of output features of LSTM, and it should be equal to the number of learning parameters in XAJ\n    n_hidden_states\n        the number of hidden features of LSTM\n    kernel_size\n        the time length of unit hydrograph\n    warmup_length\n        the length of warmup periods;\n        hydrologic models need a warmup period to generate reasonable initial state values\n    param_limit_func\n        function used to limit the range of params; now it is sigmoid or clamp function\n    param_test_way\n        how we use parameters from dl model when testing;\n        now we have three ways:\n        1. \"final\" -- use the final period's parameter for each period\n        2. \"mean_time\" -- Mean values of all periods' parameters is used\n        3. \"mean_basin\" -- Mean values of all basins' final periods' parameters is used\n        but remember these ways are only for non-variable parameters\n    param_var_index\n        variable parameters' indices in all parameters\n    return_et\n        if True, return evapotranspiration\n    \"\"\"\n    if param_var_index is None:\n        param_var_index = [0, 1, 6]\n    if nn_hidden_size is None:\n        nn_hidden_size = [16, 8]\n    super(DplLstmNnModuleXaj, self).__init__()\n    self.dl_model = SimpleLSTM(n_input_features, n_output_features, n_hidden_states)\n    self.pb_model = Xaj4DplWithNnModule(\n        kernel_size,\n        warmup_length,\n        source_book=source_book,\n        source_type=source_type,\n        nn_hidden_size=nn_hidden_size,\n        nn_dropout=nn_dropout,\n        et_output=et_output,\n        param_var_index=param_var_index,\n        param_test_way=param_test_way,\n    )\n    self.param_func = param_limit_func\n    self.param_test_way = param_test_way\n    self.param_var_index = param_var_index\n    self.return_et = return_et\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.DplLstmNnModuleXaj.forward","title":"<code>forward(self, x, z)</code>","text":"<p>Differential parameter learning</p> <p>z (normalized input) -&gt; lstm -&gt; param -&gt; + x (not normalized) -&gt; xaj -&gt; q Parameters will be denormalized in xaj model</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.DplLstmNnModuleXaj.forward--parameters","title":"Parameters","text":"<p>x     not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature] z     normalized data used for DL model; a sequence-first 3-dim tensor. [sequence, batch, feature]</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.DplLstmNnModuleXaj.forward--returns","title":"Returns","text":"<p>torch.Tensor     one time forward result</p> Source code in <code>torchhydro/models/dpl4xaj_nn4et.py</code> <pre><code>def forward(self, x, z):\n    \"\"\"\n    Differential parameter learning\n\n    z (normalized input) -&gt; lstm -&gt; param -&gt; + x (not normalized) -&gt; xaj -&gt; q\n    Parameters will be denormalized in xaj model\n\n    Parameters\n    ----------\n    x\n        not normalized data used for physical model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n    z\n        normalized data used for DL model; a sequence-first 3-dim tensor. [sequence, batch, feature]\n\n    Returns\n    -------\n    torch.Tensor\n        one time forward result\n    \"\"\"\n    gen = self.dl_model(z)\n    if torch.isnan(gen).any():\n        raise ValueError(\"Error: NaN values detected. Check your data firstly!!!\")\n    # we set all params' values in [0, 1] and will scale them when forwarding\n    if self.param_func == \"sigmoid\":\n        params = F.sigmoid(gen)\n    elif self.param_func == \"clamp\":\n        params = torch.clamp(gen, min=0.0, max=1.0)\n    else:\n        raise NotImplementedError(\n            \"We don't provide this way to limit parameters' range!! Please choose sigmoid or clamp\"\n        )\n    # just get one-period values, here we use the final period's values,\n    # when the MODEL_PARAM_TEST_WAY is not time_varing, we use the last period's values.\n    if self.param_test_way != MODEL_PARAM_TEST_WAY[\"time_varying\"]:\n        params = params[-1, :, :]\n    # Please put p in the first location and pet in the second\n    q, e = self.pb_model(x[:, :, : self.pb_model.feature_size], params)\n    return torch.cat([q, e], dim=-1) if self.return_et else q\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.NnModule4Hydro","title":"<code> NnModule4Hydro            (Module)         </code>","text":"<p>A NN module for Hydrological model. Generally, the difference between it and normal NN is: we need constrain its output to some specific value range</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.NnModule4Hydro--parameters","title":"Parameters","text":"<p>nn : type description</p> Source code in <code>torchhydro/models/dpl4xaj_nn4et.py</code> <pre><code>class NnModule4Hydro(nn.Module):\n    \"\"\"A NN module for Hydrological model.\n    Generally, the difference between it and normal NN is:\n    we need constrain its output to some specific value range\n\n    Parameters\n    ----------\n    nn : _type_\n        _description_\n    \"\"\"\n\n    def __init__(\n        self,\n        nx: int,\n        ny: int,\n        hidden_size: Union[int, tuple, list] = None,\n        dr: Union[float, tuple, list] = 0.0,\n    ):\n        \"\"\"\n        A simple multi-layer NN model with final linear layer\n\n        Parameters\n        ----------\n        nx\n            number of input neurons\n        ny\n            number of output neurons\n        hidden_size\n            a list/tuple which contains number of neurons in each hidden layer;\n            if int, only one hidden layer except for hidden_size=0\n        dr\n            dropout rate of layers, default is 0.0 which means no dropout;\n            here we set number of dropout layers to (number of nn layers - 1)\n        \"\"\"\n        super(NnModule4Hydro, self).__init__()\n        self.ann = SimpleAnn(nx, ny, hidden_size, dr)\n\n    def forward(self, x, w0, prcp, pet, k):\n        \"\"\"the forward function of the NN ET module\n\n        Parameters\n        ----------\n        x : _type_\n            _description_\n        w0 : _type_\n            water storage\n        p : _type_\n            precipitation\n        pet: tensor\n            potential evapotranspiration, used to be part of upper limit of ET\n        k: tensor\n            coefficient of PET in XAJ model, used to be part of upper limit of ET\n\n        Returns\n        -------\n        _type_\n            _description_\n        \"\"\"\n        zeros = torch.full_like(w0, 0.0, device=x.device)\n        et = torch.full_like(w0, 0.0, device=x.device)\n        w_mask = w0 + prcp &gt; PRECISION\n        y = self.ann(x)\n        z = y.flatten()\n        et[w_mask] = torch.clamp(\n            z[w_mask],\n            min=zeros[w_mask],\n            # torch.minimum computes the element-wise minimum: https://pytorch.org/docs/stable/generated/torch.minimum.html\n            # k * pet is real pet in XAJ model\n            max=torch.minimum(w0[w_mask] + prcp[w_mask], k[w_mask] * pet[w_mask]),\n        )\n        return et\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.NnModule4Hydro.__init__","title":"<code>__init__(self, nx, ny, hidden_size=None, dr=0.0)</code>  <code>special</code>","text":"<p>A simple multi-layer NN model with final linear layer</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.NnModule4Hydro.__init__--parameters","title":"Parameters","text":"<p>nx     number of input neurons ny     number of output neurons hidden_size     a list/tuple which contains number of neurons in each hidden layer;     if int, only one hidden layer except for hidden_size=0 dr     dropout rate of layers, default is 0.0 which means no dropout;     here we set number of dropout layers to (number of nn layers - 1)</p> Source code in <code>torchhydro/models/dpl4xaj_nn4et.py</code> <pre><code>def __init__(\n    self,\n    nx: int,\n    ny: int,\n    hidden_size: Union[int, tuple, list] = None,\n    dr: Union[float, tuple, list] = 0.0,\n):\n    \"\"\"\n    A simple multi-layer NN model with final linear layer\n\n    Parameters\n    ----------\n    nx\n        number of input neurons\n    ny\n        number of output neurons\n    hidden_size\n        a list/tuple which contains number of neurons in each hidden layer;\n        if int, only one hidden layer except for hidden_size=0\n    dr\n        dropout rate of layers, default is 0.0 which means no dropout;\n        here we set number of dropout layers to (number of nn layers - 1)\n    \"\"\"\n    super(NnModule4Hydro, self).__init__()\n    self.ann = SimpleAnn(nx, ny, hidden_size, dr)\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.NnModule4Hydro.forward","title":"<code>forward(self, x, w0, prcp, pet, k)</code>","text":"<p>the forward function of the NN ET module</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.NnModule4Hydro.forward--parameters","title":"Parameters","text":"<p>x : type description w0 : type     water storage p : type     precipitation !!! pet \"tensor\"     potential evapotranspiration, used to be part of upper limit of ET !!! k \"tensor\"     coefficient of PET in XAJ model, used to be part of upper limit of ET</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.NnModule4Hydro.forward--returns","title":"Returns","text":"<p>type description</p> Source code in <code>torchhydro/models/dpl4xaj_nn4et.py</code> <pre><code>def forward(self, x, w0, prcp, pet, k):\n    \"\"\"the forward function of the NN ET module\n\n    Parameters\n    ----------\n    x : _type_\n        _description_\n    w0 : _type_\n        water storage\n    p : _type_\n        precipitation\n    pet: tensor\n        potential evapotranspiration, used to be part of upper limit of ET\n    k: tensor\n        coefficient of PET in XAJ model, used to be part of upper limit of ET\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    zeros = torch.full_like(w0, 0.0, device=x.device)\n    et = torch.full_like(w0, 0.0, device=x.device)\n    w_mask = w0 + prcp &gt; PRECISION\n    y = self.ann(x)\n    z = y.flatten()\n    et[w_mask] = torch.clamp(\n        z[w_mask],\n        min=zeros[w_mask],\n        # torch.minimum computes the element-wise minimum: https://pytorch.org/docs/stable/generated/torch.minimum.html\n        # k * pet is real pet in XAJ model\n        max=torch.minimum(w0[w_mask] + prcp[w_mask], k[w_mask] * pet[w_mask]),\n    )\n    return et\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.Xaj4DplWithNnModule","title":"<code> Xaj4DplWithNnModule            (Module)         </code>","text":"<p>XAJ model for Differential Parameter learning with neural network as submodule</p> Source code in <code>torchhydro/models/dpl4xaj_nn4et.py</code> <pre><code>class Xaj4DplWithNnModule(nn.Module):\n    \"\"\"\n    XAJ model for Differential Parameter learning with neural network as submodule\n    \"\"\"\n\n    def __init__(\n        self,\n        kernel_size: int,\n        warmup_length: int,\n        nn_module=None,\n        param_var_index=None,\n        source_book=\"HF\",\n        source_type=\"sources\",\n        et_output=1,\n        nn_hidden_size: Union[int, tuple, list] = None,\n        nn_dropout=0.2,\n        param_test_way=MODEL_PARAM_TEST_WAY[\"time_varying\"],\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        kernel_size\n            the time length of unit hydrograph\n        warmup_length\n            the length of warmup periods;\n            XAJ needs a warmup period to generate reasonable initial state values\n        nn_module\n            We initialize the module when we firstly initialize Xaj4DplWithNnModule.\n            Then we will iterately call Xaj4DplWithNnModule module for warmup.\n            Hence, in warmup period, we don't need to initialize it again\n        param_var_index\n            the index of parameters which will be time-varying\n            NOTE: at the most, we support k, b, and c to be time-varying\n        et_output\n            we only support one-layer et now, because its water balance is not easy to handle with\n        \"\"\"\n        if param_var_index is None:\n            param_var_index = [0, 6]\n        if nn_hidden_size is None:\n            nn_hidden_size = [16, 8]\n        super(Xaj4DplWithNnModule, self).__init__()\n        self.params_names = MODEL_PARAM_DICT[\"xaj_mz\"][\"param_name\"]\n        param_range = MODEL_PARAM_DICT[\"xaj_mz\"][\"param_range\"]\n        self.k_scale = param_range[\"K\"]\n        self.b_scale = param_range[\"B\"]\n        self.im_sacle = param_range[\"IM\"]\n        self.um_scale = param_range[\"UM\"]\n        self.lm_scale = param_range[\"LM\"]\n        self.dm_scale = param_range[\"DM\"]\n        self.c_scale = param_range[\"C\"]\n        self.sm_scale = param_range[\"SM\"]\n        self.ex_scale = param_range[\"EX\"]\n        self.ki_scale = param_range[\"KI\"]\n        self.kg_scale = param_range[\"KG\"]\n        self.a_scale = param_range[\"A\"]\n        self.theta_scale = param_range[\"THETA\"]\n        self.ci_scale = param_range[\"CI\"]\n        self.cg_scale = param_range[\"CG\"]\n        self.kernel_size = kernel_size\n        self.warmup_length = warmup_length\n        # there are 2 input variables in XAJ: P and PET\n        self.feature_size = 2\n        if nn_module is None:\n            # 7: k, um, lm, dm, c, prcp, p_and_e[:, 1] + 1/3: w0 or wu0, wl0, wd0\n            self.evap_nn_module = NnModule4Hydro(\n                7 + et_output, et_output, nn_hidden_size, nn_dropout\n            )\n        else:\n            self.evap_nn_module = nn_module\n        self.source_book = source_book\n        self.source_type = source_type\n        self.et_output = et_output\n        self.param_var_index = param_var_index\n        self.nn_hidden_size = nn_hidden_size\n        self.nn_dropout = nn_dropout\n        self.param_test_way = param_test_way\n\n    def xaj_generation_with_new_module(\n        self,\n        p_and_e: Tensor,\n        k,\n        b,\n        im,\n        um,\n        lm,\n        dm,\n        c,\n        *args,\n        # wu0: Tensor = None,\n        # wl0: Tensor = None,\n        # wd0: Tensor = None,\n    ) -&gt; tuple:\n        # make sure physical variables' value ranges are correct\n        prcp = torch.clamp(p_and_e[:, 0], min=0.0)\n        pet = torch.clamp(p_and_e[:, 1], min=0.0)\n        # wm\n        wm = um + lm + dm\n        if self.et_output != 1:\n            raise NotImplementedError(\"We only support one-layer evaporation now\")\n        w0_ = args[0]\n        if w0_ is None:\n            w0_ = 0.6 * (um.detach() + lm.detach() + dm.detach())\n        w0 = torch.clamp(w0_, max=wm - PRECISION)\n        concat_input = torch.stack([k, um, lm, dm, c, w0, prcp, pet], dim=1)\n        e = self.evap_nn_module(concat_input, w0, prcp, pet, k)\n        # Calculate the runoff generated by net precipitation\n        prcp_difference = prcp - e\n        pe = torch.clamp(prcp_difference, min=0.0)\n        r, rim = calculate_prcp_runoff(b, im, wm, w0, pe)\n        if self.et_output == 1:\n            w = calculate_1layer_w_storage(\n                um,\n                lm,\n                dm,\n                w0,\n                prcp_difference,\n                r,\n            )\n            return (r, rim, e, pe), (w,)\n        else:\n            raise ValueError(\"et_output should be 1\")\n\n    def forward(self, p_and_e, parameters_ts, return_state=False):\n        \"\"\"\n        run XAJ model\n\n        Parameters\n        ----------\n        p_and_e\n            precipitation and potential evapotranspiration\n        parameters_ts\n            time series parameters of XAJ model;\n            some parameters may be time-varying specified by param_var_index\n        return_state\n            if True, return state values, mainly for warmup periods\n\n        Returns\n        -------\n        torch.Tensor\n            streamflow got by XAJ\n        \"\"\"\n        xaj_device = p_and_e.device\n        if self.param_test_way == MODEL_PARAM_TEST_WAY[\"time_varying\"]:\n            parameters = parameters_ts[-1, :, :]\n        else:\n            # parameters_ts must be a 2-d tensor: (basin, param)\n            parameters = parameters_ts\n        # denormalize the parameters to general range\n        # TODO: now the specific parameters are hard coded; 0 is k, 1 is b, 6 is c, same as in model_config.py\n        if 0 not in self.param_var_index or self.param_var_index is None:\n            ks = self.k_scale[0] + parameters[:, 0] * (\n                self.k_scale[1] - self.k_scale[0]\n            )\n        else:\n            ks = self.k_scale[0] + parameters_ts[:, :, 0] * (\n                self.k_scale[1] - self.k_scale[0]\n            )\n        if 1 not in self.param_var_index or self.param_var_index is None:\n            bs = self.b_scale[0] + parameters[:, 1] * (\n                self.b_scale[1] - self.b_scale[0]\n            )\n        else:\n            bs = self.b_scale[0] + parameters_ts[:, :, 1] * (\n                self.b_scale[1] - self.b_scale[0]\n            )\n        im = self.im_sacle[0] + parameters[:, 2] * (self.im_sacle[1] - self.im_sacle[0])\n        um = self.um_scale[0] + parameters[:, 3] * (self.um_scale[1] - self.um_scale[0])\n        lm = self.lm_scale[0] + parameters[:, 4] * (self.lm_scale[1] - self.lm_scale[0])\n        dm = self.dm_scale[0] + parameters[:, 5] * (self.dm_scale[1] - self.dm_scale[0])\n        if 6 not in self.param_var_index or self.param_var_index is None:\n            cs = self.c_scale[0] + parameters[:, 6] * (\n                self.c_scale[1] - self.c_scale[0]\n            )\n        else:\n            cs = self.c_scale[0] + parameters_ts[:, :, 6] * (\n                self.c_scale[1] - self.c_scale[0]\n            )\n        sm = self.sm_scale[0] + parameters[:, 7] * (self.sm_scale[1] - self.sm_scale[0])\n        ex = self.ex_scale[0] + parameters[:, 8] * (self.ex_scale[1] - self.ex_scale[0])\n        ki_ = self.ki_scale[0] + parameters[:, 9] * (\n            self.ki_scale[1] - self.ki_scale[0]\n        )\n        kg_ = self.kg_scale[0] + parameters[:, 10] * (\n            self.kg_scale[1] - self.kg_scale[0]\n        )\n        # ki+kg should be smaller than 1; if not, we scale them\n        ki = torch.where(\n            ki_ + kg_ &lt; 1.0,\n            ki_,\n            (1 - PRECISION) / (ki_ + kg_) * ki_,\n        )\n        kg = torch.where(\n            ki_ + kg_ &lt; 1.0,\n            kg_,\n            (1 - PRECISION) / (ki_ + kg_) * kg_,\n        )\n        a = self.a_scale[0] + parameters[:, 11] * (self.a_scale[1] - self.a_scale[0])\n        theta = self.theta_scale[0] + parameters[:, 12] * (\n            self.theta_scale[1] - self.theta_scale[0]\n        )\n        ci = self.ci_scale[0] + parameters[:, 13] * (\n            self.ci_scale[1] - self.ci_scale[0]\n        )\n        cg = self.cg_scale[0] + parameters[:, 14] * (\n            self.cg_scale[1] - self.cg_scale[0]\n        )\n\n        # initialize state values\n        warmup_length = self.warmup_length\n        if warmup_length &gt; 0:\n            # set no_grad for warmup periods\n            with torch.no_grad():\n                p_and_e_warmup = p_and_e[0:warmup_length, :, :]\n                if self.param_test_way == MODEL_PARAM_TEST_WAY[\"time_varying\"]:\n                    parameters_ts_warmup = parameters_ts[0:warmup_length, :, :]\n                else:\n                    parameters_ts_warmup = parameters_ts\n                cal_init_xaj4dpl = Xaj4DplWithNnModule(\n                    kernel_size=self.kernel_size,\n                    # warmup_length must be 0 here\n                    warmup_length=0,\n                    nn_module=self.evap_nn_module,\n                    param_var_index=self.param_var_index,\n                    source_book=self.source_book,\n                    source_type=self.source_type,\n                    et_output=self.et_output,\n                    nn_hidden_size=self.nn_hidden_size,\n                    nn_dropout=self.nn_dropout,\n                    param_test_way=self.param_test_way,\n                )\n                if cal_init_xaj4dpl.warmup_length &gt; 0:\n                    raise RuntimeError(\"Please set init model's warmup length to 0!!!\")\n                _, _, *w0, s0, fr0, qi0, qg0 = cal_init_xaj4dpl(\n                    p_and_e_warmup, parameters_ts_warmup, return_state=True\n                )\n        else:\n            # use detach func to make wu0 no_grad as it is an initial value\n            if self.et_output == 1:\n                # () and , must be added, otherwise, w0 will be a tensor, not a tuple\n                w0 = (0.5 * (um.detach() + lm.detach() + dm.detach()),)\n            else:\n                raise ValueError(\"et_output should be 1 or 3\")\n            s0 = 0.5 * (sm.detach())\n            fr0 = torch.full(ci.size(), 0.1).to(xaj_device)\n            qi0 = torch.full(ci.size(), 0.1).to(xaj_device)\n            qg0 = torch.full(cg.size(), 0.1).to(xaj_device)\n\n        inputs = p_and_e[warmup_length:, :, :]\n        runoff_ims_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n        rss_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n        ris_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n        rgs_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n        es_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n        for i in range(inputs.shape[0]):\n            if 0 in self.param_var_index or self.param_var_index is None:\n                k = ks[i]\n            else:\n                k = ks\n            if 1 in self.param_var_index or self.param_var_index is None:\n                b = bs[i]\n            else:\n                b = bs\n            if 6 in self.param_var_index or self.param_var_index is None:\n                c = cs[i]\n            else:\n                c = cs\n            if i == 0:\n                (r, rim, e, pe), w = self.xaj_generation_with_new_module(\n                    inputs[i, :, :], k, b, im, um, lm, dm, c, *w0\n                )\n                if self.source_type == \"sources\":\n                    (rs, ri, rg), (s, fr) = xaj_sources(\n                        pe, r, sm, ex, ki, kg, s0, fr0, book=self.source_book\n                    )\n                elif self.source_type == \"sources5mm\":\n                    (rs, ri, rg), (s, fr) = xaj_sources5mm(\n                        pe, r, sm, ex, ki, kg, s0, fr0, book=self.source_book\n                    )\n                else:\n                    raise NotImplementedError(\"No such divide-sources method\")\n            else:\n                (r, rim, e, pe), w = self.xaj_generation_with_new_module(\n                    inputs[i, :, :], k, b, im, um, lm, dm, c, *w\n                )\n                if self.source_type == \"sources\":\n                    (rs, ri, rg), (s, fr) = xaj_sources(\n                        pe, r, sm, ex, ki, kg, s, fr, book=self.source_book\n                    )\n                elif self.source_type == \"sources5mm\":\n                    (rs, ri, rg), (s, fr) = xaj_sources5mm(\n                        pe, r, sm, ex, ki, kg, s, fr, book=self.source_book\n                    )\n                else:\n                    raise NotImplementedError(\"No such divide-sources method\")\n            # impevious part is pe * im\n            runoff_ims_[i, :] = rim\n            # so for non-imprvious part, the result should be corrected\n            rss_[i, :] = rs * (1 - im)\n            ris_[i, :] = ri * (1 - im)\n            rgs_[i, :] = rg * (1 - im)\n            es_[i, :] = e\n        # seq, batch, feature\n        runoff_im = torch.unsqueeze(runoff_ims_, dim=2)\n        rss = torch.unsqueeze(rss_, dim=2)\n        es = torch.unsqueeze(es_, dim=2)\n\n        conv_uh = KernelConv(a, theta, self.kernel_size)\n        qs_ = conv_uh(runoff_im + rss)\n\n        qs = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n        for i in range(inputs.shape[0]):\n            if i == 0:\n                qi = linear_reservoir(ris_[i], ci, qi0)\n                qg = linear_reservoir(rgs_[i], cg, qg0)\n            else:\n                qi = linear_reservoir(ris_[i], ci, qi)\n                qg = linear_reservoir(rgs_[i], cg, qg)\n            qs[i, :] = qs_[i, :, 0] + qi + qg\n        # seq, batch, feature\n        q_sim = torch.unsqueeze(qs, dim=2)\n        if return_state:\n            return q_sim, es, *w, s, fr, qi, qg\n        return q_sim, es\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.Xaj4DplWithNnModule.__init__","title":"<code>__init__(self, kernel_size, warmup_length, nn_module=None, param_var_index=None, source_book='HF', source_type='sources', et_output=1, nn_hidden_size=None, nn_dropout=0.2, param_test_way='var')</code>  <code>special</code>","text":""},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.Xaj4DplWithNnModule.__init__--parameters","title":"Parameters","text":"<p>kernel_size     the time length of unit hydrograph warmup_length     the length of warmup periods;     XAJ needs a warmup period to generate reasonable initial state values nn_module     We initialize the module when we firstly initialize Xaj4DplWithNnModule.     Then we will iterately call Xaj4DplWithNnModule module for warmup.     Hence, in warmup period, we don't need to initialize it again param_var_index     the index of parameters which will be time-varying     NOTE: at the most, we support k, b, and c to be time-varying et_output     we only support one-layer et now, because its water balance is not easy to handle with</p> Source code in <code>torchhydro/models/dpl4xaj_nn4et.py</code> <pre><code>def __init__(\n    self,\n    kernel_size: int,\n    warmup_length: int,\n    nn_module=None,\n    param_var_index=None,\n    source_book=\"HF\",\n    source_type=\"sources\",\n    et_output=1,\n    nn_hidden_size: Union[int, tuple, list] = None,\n    nn_dropout=0.2,\n    param_test_way=MODEL_PARAM_TEST_WAY[\"time_varying\"],\n):\n    \"\"\"\n    Parameters\n    ----------\n    kernel_size\n        the time length of unit hydrograph\n    warmup_length\n        the length of warmup periods;\n        XAJ needs a warmup period to generate reasonable initial state values\n    nn_module\n        We initialize the module when we firstly initialize Xaj4DplWithNnModule.\n        Then we will iterately call Xaj4DplWithNnModule module for warmup.\n        Hence, in warmup period, we don't need to initialize it again\n    param_var_index\n        the index of parameters which will be time-varying\n        NOTE: at the most, we support k, b, and c to be time-varying\n    et_output\n        we only support one-layer et now, because its water balance is not easy to handle with\n    \"\"\"\n    if param_var_index is None:\n        param_var_index = [0, 6]\n    if nn_hidden_size is None:\n        nn_hidden_size = [16, 8]\n    super(Xaj4DplWithNnModule, self).__init__()\n    self.params_names = MODEL_PARAM_DICT[\"xaj_mz\"][\"param_name\"]\n    param_range = MODEL_PARAM_DICT[\"xaj_mz\"][\"param_range\"]\n    self.k_scale = param_range[\"K\"]\n    self.b_scale = param_range[\"B\"]\n    self.im_sacle = param_range[\"IM\"]\n    self.um_scale = param_range[\"UM\"]\n    self.lm_scale = param_range[\"LM\"]\n    self.dm_scale = param_range[\"DM\"]\n    self.c_scale = param_range[\"C\"]\n    self.sm_scale = param_range[\"SM\"]\n    self.ex_scale = param_range[\"EX\"]\n    self.ki_scale = param_range[\"KI\"]\n    self.kg_scale = param_range[\"KG\"]\n    self.a_scale = param_range[\"A\"]\n    self.theta_scale = param_range[\"THETA\"]\n    self.ci_scale = param_range[\"CI\"]\n    self.cg_scale = param_range[\"CG\"]\n    self.kernel_size = kernel_size\n    self.warmup_length = warmup_length\n    # there are 2 input variables in XAJ: P and PET\n    self.feature_size = 2\n    if nn_module is None:\n        # 7: k, um, lm, dm, c, prcp, p_and_e[:, 1] + 1/3: w0 or wu0, wl0, wd0\n        self.evap_nn_module = NnModule4Hydro(\n            7 + et_output, et_output, nn_hidden_size, nn_dropout\n        )\n    else:\n        self.evap_nn_module = nn_module\n    self.source_book = source_book\n    self.source_type = source_type\n    self.et_output = et_output\n    self.param_var_index = param_var_index\n    self.nn_hidden_size = nn_hidden_size\n    self.nn_dropout = nn_dropout\n    self.param_test_way = param_test_way\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.Xaj4DplWithNnModule.forward","title":"<code>forward(self, p_and_e, parameters_ts, return_state=False)</code>","text":"<p>run XAJ model</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.Xaj4DplWithNnModule.forward--parameters","title":"Parameters","text":"<p>p_and_e     precipitation and potential evapotranspiration parameters_ts     time series parameters of XAJ model;     some parameters may be time-varying specified by param_var_index return_state     if True, return state values, mainly for warmup periods</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.Xaj4DplWithNnModule.forward--returns","title":"Returns","text":"<p>torch.Tensor     streamflow got by XAJ</p> Source code in <code>torchhydro/models/dpl4xaj_nn4et.py</code> <pre><code>def forward(self, p_and_e, parameters_ts, return_state=False):\n    \"\"\"\n    run XAJ model\n\n    Parameters\n    ----------\n    p_and_e\n        precipitation and potential evapotranspiration\n    parameters_ts\n        time series parameters of XAJ model;\n        some parameters may be time-varying specified by param_var_index\n    return_state\n        if True, return state values, mainly for warmup periods\n\n    Returns\n    -------\n    torch.Tensor\n        streamflow got by XAJ\n    \"\"\"\n    xaj_device = p_and_e.device\n    if self.param_test_way == MODEL_PARAM_TEST_WAY[\"time_varying\"]:\n        parameters = parameters_ts[-1, :, :]\n    else:\n        # parameters_ts must be a 2-d tensor: (basin, param)\n        parameters = parameters_ts\n    # denormalize the parameters to general range\n    # TODO: now the specific parameters are hard coded; 0 is k, 1 is b, 6 is c, same as in model_config.py\n    if 0 not in self.param_var_index or self.param_var_index is None:\n        ks = self.k_scale[0] + parameters[:, 0] * (\n            self.k_scale[1] - self.k_scale[0]\n        )\n    else:\n        ks = self.k_scale[0] + parameters_ts[:, :, 0] * (\n            self.k_scale[1] - self.k_scale[0]\n        )\n    if 1 not in self.param_var_index or self.param_var_index is None:\n        bs = self.b_scale[0] + parameters[:, 1] * (\n            self.b_scale[1] - self.b_scale[0]\n        )\n    else:\n        bs = self.b_scale[0] + parameters_ts[:, :, 1] * (\n            self.b_scale[1] - self.b_scale[0]\n        )\n    im = self.im_sacle[0] + parameters[:, 2] * (self.im_sacle[1] - self.im_sacle[0])\n    um = self.um_scale[0] + parameters[:, 3] * (self.um_scale[1] - self.um_scale[0])\n    lm = self.lm_scale[0] + parameters[:, 4] * (self.lm_scale[1] - self.lm_scale[0])\n    dm = self.dm_scale[0] + parameters[:, 5] * (self.dm_scale[1] - self.dm_scale[0])\n    if 6 not in self.param_var_index or self.param_var_index is None:\n        cs = self.c_scale[0] + parameters[:, 6] * (\n            self.c_scale[1] - self.c_scale[0]\n        )\n    else:\n        cs = self.c_scale[0] + parameters_ts[:, :, 6] * (\n            self.c_scale[1] - self.c_scale[0]\n        )\n    sm = self.sm_scale[0] + parameters[:, 7] * (self.sm_scale[1] - self.sm_scale[0])\n    ex = self.ex_scale[0] + parameters[:, 8] * (self.ex_scale[1] - self.ex_scale[0])\n    ki_ = self.ki_scale[0] + parameters[:, 9] * (\n        self.ki_scale[1] - self.ki_scale[0]\n    )\n    kg_ = self.kg_scale[0] + parameters[:, 10] * (\n        self.kg_scale[1] - self.kg_scale[0]\n    )\n    # ki+kg should be smaller than 1; if not, we scale them\n    ki = torch.where(\n        ki_ + kg_ &lt; 1.0,\n        ki_,\n        (1 - PRECISION) / (ki_ + kg_) * ki_,\n    )\n    kg = torch.where(\n        ki_ + kg_ &lt; 1.0,\n        kg_,\n        (1 - PRECISION) / (ki_ + kg_) * kg_,\n    )\n    a = self.a_scale[0] + parameters[:, 11] * (self.a_scale[1] - self.a_scale[0])\n    theta = self.theta_scale[0] + parameters[:, 12] * (\n        self.theta_scale[1] - self.theta_scale[0]\n    )\n    ci = self.ci_scale[0] + parameters[:, 13] * (\n        self.ci_scale[1] - self.ci_scale[0]\n    )\n    cg = self.cg_scale[0] + parameters[:, 14] * (\n        self.cg_scale[1] - self.cg_scale[0]\n    )\n\n    # initialize state values\n    warmup_length = self.warmup_length\n    if warmup_length &gt; 0:\n        # set no_grad for warmup periods\n        with torch.no_grad():\n            p_and_e_warmup = p_and_e[0:warmup_length, :, :]\n            if self.param_test_way == MODEL_PARAM_TEST_WAY[\"time_varying\"]:\n                parameters_ts_warmup = parameters_ts[0:warmup_length, :, :]\n            else:\n                parameters_ts_warmup = parameters_ts\n            cal_init_xaj4dpl = Xaj4DplWithNnModule(\n                kernel_size=self.kernel_size,\n                # warmup_length must be 0 here\n                warmup_length=0,\n                nn_module=self.evap_nn_module,\n                param_var_index=self.param_var_index,\n                source_book=self.source_book,\n                source_type=self.source_type,\n                et_output=self.et_output,\n                nn_hidden_size=self.nn_hidden_size,\n                nn_dropout=self.nn_dropout,\n                param_test_way=self.param_test_way,\n            )\n            if cal_init_xaj4dpl.warmup_length &gt; 0:\n                raise RuntimeError(\"Please set init model's warmup length to 0!!!\")\n            _, _, *w0, s0, fr0, qi0, qg0 = cal_init_xaj4dpl(\n                p_and_e_warmup, parameters_ts_warmup, return_state=True\n            )\n    else:\n        # use detach func to make wu0 no_grad as it is an initial value\n        if self.et_output == 1:\n            # () and , must be added, otherwise, w0 will be a tensor, not a tuple\n            w0 = (0.5 * (um.detach() + lm.detach() + dm.detach()),)\n        else:\n            raise ValueError(\"et_output should be 1 or 3\")\n        s0 = 0.5 * (sm.detach())\n        fr0 = torch.full(ci.size(), 0.1).to(xaj_device)\n        qi0 = torch.full(ci.size(), 0.1).to(xaj_device)\n        qg0 = torch.full(cg.size(), 0.1).to(xaj_device)\n\n    inputs = p_and_e[warmup_length:, :, :]\n    runoff_ims_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n    rss_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n    ris_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n    rgs_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n    es_ = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n    for i in range(inputs.shape[0]):\n        if 0 in self.param_var_index or self.param_var_index is None:\n            k = ks[i]\n        else:\n            k = ks\n        if 1 in self.param_var_index or self.param_var_index is None:\n            b = bs[i]\n        else:\n            b = bs\n        if 6 in self.param_var_index or self.param_var_index is None:\n            c = cs[i]\n        else:\n            c = cs\n        if i == 0:\n            (r, rim, e, pe), w = self.xaj_generation_with_new_module(\n                inputs[i, :, :], k, b, im, um, lm, dm, c, *w0\n            )\n            if self.source_type == \"sources\":\n                (rs, ri, rg), (s, fr) = xaj_sources(\n                    pe, r, sm, ex, ki, kg, s0, fr0, book=self.source_book\n                )\n            elif self.source_type == \"sources5mm\":\n                (rs, ri, rg), (s, fr) = xaj_sources5mm(\n                    pe, r, sm, ex, ki, kg, s0, fr0, book=self.source_book\n                )\n            else:\n                raise NotImplementedError(\"No such divide-sources method\")\n        else:\n            (r, rim, e, pe), w = self.xaj_generation_with_new_module(\n                inputs[i, :, :], k, b, im, um, lm, dm, c, *w\n            )\n            if self.source_type == \"sources\":\n                (rs, ri, rg), (s, fr) = xaj_sources(\n                    pe, r, sm, ex, ki, kg, s, fr, book=self.source_book\n                )\n            elif self.source_type == \"sources5mm\":\n                (rs, ri, rg), (s, fr) = xaj_sources5mm(\n                    pe, r, sm, ex, ki, kg, s, fr, book=self.source_book\n                )\n            else:\n                raise NotImplementedError(\"No such divide-sources method\")\n        # impevious part is pe * im\n        runoff_ims_[i, :] = rim\n        # so for non-imprvious part, the result should be corrected\n        rss_[i, :] = rs * (1 - im)\n        ris_[i, :] = ri * (1 - im)\n        rgs_[i, :] = rg * (1 - im)\n        es_[i, :] = e\n    # seq, batch, feature\n    runoff_im = torch.unsqueeze(runoff_ims_, dim=2)\n    rss = torch.unsqueeze(rss_, dim=2)\n    es = torch.unsqueeze(es_, dim=2)\n\n    conv_uh = KernelConv(a, theta, self.kernel_size)\n    qs_ = conv_uh(runoff_im + rss)\n\n    qs = torch.full(inputs.shape[:2], 0.0).to(xaj_device)\n    for i in range(inputs.shape[0]):\n        if i == 0:\n            qi = linear_reservoir(ris_[i], ci, qi0)\n            qg = linear_reservoir(rgs_[i], cg, qg0)\n        else:\n            qi = linear_reservoir(ris_[i], ci, qi)\n            qg = linear_reservoir(rgs_[i], cg, qg)\n        qs[i, :] = qs_[i, :, 0] + qi + qg\n    # seq, batch, feature\n    q_sim = torch.unsqueeze(qs, dim=2)\n    if return_state:\n        return q_sim, es, *w, s, fr, qi, qg\n    return q_sim, es\n</code></pre>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.calculate_1layer_w_storage","title":"<code>calculate_1layer_w_storage(um, lm, dm, w0, pe, r)</code>","text":"<p>Update the soil moisture value.</p> <p>According to the runoff-generation equation 2.60 in the book \"SHUIWENYUBAO\", dW = dPE - dR</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.calculate_1layer_w_storage--parameters","title":"Parameters","text":"<p>um     average soil moisture storage capacity of the upper layer (mm) lm     average soil moisture storage capacity of the lower layer (mm) dm     average soil moisture storage capacity of the deep layer (mm) w0     initial values of soil moisture pe     net precipitation; it is able to be negative value in this function r     runoff</p>"},{"location":"api/models/#torchhydro.models.dpl4xaj_nn4et.calculate_1layer_w_storage--returns","title":"Returns","text":"<p>torch.Tensor     w -- soil moisture</p> Source code in <code>torchhydro/models/dpl4xaj_nn4et.py</code> <pre><code>def calculate_1layer_w_storage(um, lm, dm, w0, pe, r):\n    \"\"\"\n    Update the soil moisture value.\n\n    According to the runoff-generation equation 2.60 in the book \"SHUIWENYUBAO\", dW = dPE - dR\n\n    Parameters\n    ----------\n    um\n        average soil moisture storage capacity of the upper layer (mm)\n    lm\n        average soil moisture storage capacity of the lower layer (mm)\n    dm\n        average soil moisture storage capacity of the deep layer (mm)\n    w0\n        initial values of soil moisture\n    pe\n        net precipitation; it is able to be negative value in this function\n    r\n        runoff\n\n    Returns\n    -------\n    torch.Tensor\n        w -- soil moisture\n    \"\"\"\n    xaj_device = pe.device\n    tensor_zeros = torch.full_like(w0, 0.0, device=xaj_device)\n    # water balance (equation 2.2 in Page 13, also shown in Page 23)\n    w = w0 + pe - r\n    return torch.clamp(w, min=tensor_zeros, max=(um + lm + dm) - PRECISION)\n</code></pre>"},{"location":"api/models/#torchhydro.models.dropout","title":"<code>dropout</code>","text":"<p>Author: Wenyu Ouyang Date: 2023-07-11 17:39:09 LastEditTime: 2024-10-09 15:27:49 LastEditors: Wenyu Ouyang Description: Functions for dropout. Code is from Kuai Fang's repo: hydroDL FilePath:       orchhydro       orchhydro\\models\\dropout.py Copyright (c) 2023-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/models/#torchhydro.models.dropout.DropMask","title":"<code> DropMask            (InplaceFunction)         </code>","text":"Source code in <code>torchhydro/models/dropout.py</code> <pre><code>class DropMask(torch.autograd.function.InplaceFunction):\n    @classmethod\n    def forward(cls, ctx, input, mask, train=False, inplace=False):\n        \"\"\"_summary_\n\n        Parameters\n        ----------\n        ctx : autograd.Function\n            ctx is a context object that can be used to store information for backward computation\n        input : _type_\n            _description_\n        mask : _type_\n            _description_\n        train : bool, optional\n            if the model is in training mode, by default False\n        inplace : bool, optional\n            inplace operation, by default False\n\n        Returns\n        -------\n        _type_\n            _description_\n        \"\"\"\n        ctx.master_train = train\n        ctx.inplace = inplace\n        ctx.mask = mask\n\n        if not ctx.master_train:\n            # if not in training mode, just return the input\n            return input\n        if ctx.inplace:\n            # mark_dirty() is used to mark the input as dirty, meaning inplace operation is performed\n            # make it dirty so that the gradient is calculated correctly during backward\n            ctx.mark_dirty(input)\n            output = input\n        else:\n            # clone the input tensor so that avoid changing the input tensor\n            output = input.clone()\n        # inplace multiplication with the mask\n        output.mul_(ctx.mask)\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"\n        backward method for DropMask\n        staticmethod means that the method belongs to the class itself and not to the object of the class\n\n        Parameters\n        ----------\n        ctx : _type_\n            store information for backward computation\n        grad_output : _type_\n            gradient of the downstream layer\n\n        Returns\n        -------\n        _type_\n            _description_\n        \"\"\"\n        if ctx.master_train:\n            # if in training mode, return the gradient multiplied by the mask\n            return grad_output * ctx.mask, None, None, None\n        else:\n            # if not in training mode, return the gradient directly\n            return grad_output, None, None, None\n</code></pre>"},{"location":"api/models/#torchhydro.models.dropout.DropMask.backward","title":"<code>backward(ctx, grad_output)</code>  <code>staticmethod</code>","text":"<p>backward method for DropMask staticmethod means that the method belongs to the class itself and not to the object of the class</p>"},{"location":"api/models/#torchhydro.models.dropout.DropMask.backward--parameters","title":"Parameters","text":"<p>ctx : type     store information for backward computation grad_output : type     gradient of the downstream layer</p>"},{"location":"api/models/#torchhydro.models.dropout.DropMask.backward--returns","title":"Returns","text":"<p>type description</p> Source code in <code>torchhydro/models/dropout.py</code> <pre><code>@staticmethod\ndef backward(ctx, grad_output):\n    \"\"\"\n    backward method for DropMask\n    staticmethod means that the method belongs to the class itself and not to the object of the class\n\n    Parameters\n    ----------\n    ctx : _type_\n        store information for backward computation\n    grad_output : _type_\n        gradient of the downstream layer\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    if ctx.master_train:\n        # if in training mode, return the gradient multiplied by the mask\n        return grad_output * ctx.mask, None, None, None\n    else:\n        # if not in training mode, return the gradient directly\n        return grad_output, None, None, None\n</code></pre>"},{"location":"api/models/#torchhydro.models.dropout.DropMask.forward","title":"<code>forward(ctx, input, mask, train=False, inplace=False)</code>  <code>classmethod</code>","text":"<p>summary</p>"},{"location":"api/models/#torchhydro.models.dropout.DropMask.forward--parameters","title":"Parameters","text":"<p>ctx : autograd.Function     ctx is a context object that can be used to store information for backward computation input : type description mask : type description train : bool, optional     if the model is in training mode, by default False inplace : bool, optional     inplace operation, by default False</p>"},{"location":"api/models/#torchhydro.models.dropout.DropMask.forward--returns","title":"Returns","text":"<p>type description</p> Source code in <code>torchhydro/models/dropout.py</code> <pre><code>@classmethod\ndef forward(cls, ctx, input, mask, train=False, inplace=False):\n    \"\"\"_summary_\n\n    Parameters\n    ----------\n    ctx : autograd.Function\n        ctx is a context object that can be used to store information for backward computation\n    input : _type_\n        _description_\n    mask : _type_\n        _description_\n    train : bool, optional\n        if the model is in training mode, by default False\n    inplace : bool, optional\n        inplace operation, by default False\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    ctx.master_train = train\n    ctx.inplace = inplace\n    ctx.mask = mask\n\n    if not ctx.master_train:\n        # if not in training mode, just return the input\n        return input\n    if ctx.inplace:\n        # mark_dirty() is used to mark the input as dirty, meaning inplace operation is performed\n        # make it dirty so that the gradient is calculated correctly during backward\n        ctx.mark_dirty(input)\n        output = input\n    else:\n        # clone the input tensor so that avoid changing the input tensor\n        output = input.clone()\n    # inplace multiplication with the mask\n    output.mul_(ctx.mask)\n\n    return output\n</code></pre>"},{"location":"api/models/#torchhydro.models.dropout.create_mask","title":"<code>create_mask(x, dr)</code>","text":"<p>Dropout method in Gal &amp; Ghahramami: A Theoretically Grounded Application of Dropout in RNNs. http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.pdf</p>"},{"location":"api/models/#torchhydro.models.dropout.create_mask--parameters","title":"Parameters","text":"<p>!!! x \"torch.Tensor\"     input tensor !!! dr \"float\"     dropout rate</p>"},{"location":"api/models/#torchhydro.models.dropout.create_mask--returns","title":"Returns","text":"<p>torch.Tensor     mask tensor</p> Source code in <code>torchhydro/models/dropout.py</code> <pre><code>def create_mask(x, dr):\n    \"\"\"\n    Dropout method in Gal &amp; Ghahramami: A Theoretically Grounded Application of Dropout in RNNs.\n    http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.pdf\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        input tensor\n    dr: float\n        dropout rate\n\n    Returns\n    -------\n    torch.Tensor\n        mask tensor\n    \"\"\"\n    # x.new() creates a new tensor with the same data type as x\n    # bernoulli_(1-dr) creates a tensor with the same shape as x, filled with 0 or 1, where 1 has a probability of 1-dr\n    # div_(1-dr) divides the tensor by 1-dr, so that the expected value of the tensor is the same as x, for example, if dr=0.5, then the expected value of the tensor is 2*x\n    # detach_() can be used to detach the tensor from the computation graph so that the gradient is not calculated\n    # if dr=1, then the tensor is all zeros, the results are all NaNs if using the code with bernoulli_(1 - dr).div_(1 - dr).detach_()\n    # so we need to add a special case for dr=1\n    if dr == 1:\n        # add a warning message\n        print(\"Warning: dropout rate is 1, directly set 0.\")\n        return x.new().resize_as_(x).zero_().detach_()\n    return x.new().resize_as_(x).bernoulli_(1 - dr).div_(1 - dr).detach_()\n</code></pre>"},{"location":"api/models/#torchhydro.models.gnn","title":"<code>gnn</code>","text":""},{"location":"api/models/#torchhydro.models.gnn.GNNBaseModel","title":"<code> GNNBaseModel            (Module, ABC)         </code>","text":"Source code in <code>torchhydro/models/gnn.py</code> <pre><code>class GNNBaseModel(Module, ABC):\n    def __init__(\n        self,\n        in_channels: int,\n        hidden_channels: int,\n        num_hidden: int,\n        param_sharing: bool,\n        layerfun: Callable[[], Module],\n        edge_orientation: Optional[str],\n        edge_weights: Optional[torch.Tensor],\n        output_size: int = 1,\n        root_gauge_idx: Optional[int] = None,\n    ) -&gt; None:\n        super().__init__()\n        # \u4fee\u6539\uff1a\u652f\u6301\u591a\u65f6\u6bb5\u8f93\u51fa\n        self.output_size = output_size\n        self.root_gauge_idx = root_gauge_idx\n\n        self.encoder = Linear(\n            in_channels, hidden_channels, weight_initializer=\"kaiming_uniform\"\n        )\n        if param_sharing:\n            self.layers = ModuleList(num_hidden * [layerfun()])\n        else:\n            self.layers = ModuleList([layerfun() for _ in range(num_hidden)])\n        # \u4f20\u7edf\u7684decoder\uff08\u7528\u4e8e\u6240\u6709\u8282\u70b9\u8f93\u51fa\uff09\n        self.decoder = Linear(\n            hidden_channels, output_size, weight_initializer=\"kaiming_uniform\"\n        )\n        # \u805a\u5408\u5c42\uff1a\u5c06\u6240\u6709\u8282\u70b9\u7684\u4fe1\u606f\u805a\u5408\u5230\u6839\u8282\u70b9\n        if root_gauge_idx is not None:\n            # \u8fd9\u4e2a\u5c42\u5c06\u5728forward\u4e2d\u52a8\u6001\u521b\u5efa\uff0c\u9700\u8981\u77e5\u9053\u786e\u5207\u7684\u8282\u70b9\u6570\n            self.aggregation_layer: Optional[Linear] = None\n\n        self.edge_weights = edge_weights\n        self.edge_orientation = edge_orientation\n        # \u8bbe\u7f6e\u81ea\u73af\u586b\u5145\u503c\uff0c\u4f18\u5148\u4f7f\u7528\u9884\u8bbe\u7684edge_weights\uff0c\u5982\u679c\u6ca1\u6709\u5219\u5728forward\u4e2d\u52a8\u6001\u8bbe\u7f6e\n        if self.edge_weights is not None:\n            self.loop_fill_value: Union[float, str] = (\n                1.0 if (self.edge_weights == 0).all() else \"mean\"\n            )\n        else:\n            # \u5982\u679c\u6ca1\u6709\u9884\u8bbe\u6743\u91cd\uff0c\u4f7f\u7528\u9ed8\u8ba4\u503c\uff0c\u5728forward\u4e2d\u53ef\u80fd\u4f1a\u6839\u636e\u8f93\u5165\u7684edge_weight\u8c03\u6574\n            self.loop_fill_value: Union[float, str] = \"mean\"\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        edge_index: torch.Tensor,\n        edge_weight: torch.Tensor,\n        batch_vector: Optional[torch.Tensor] = None,\n        evo_tracking: bool = False,\n        **kwargs,\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]]]:\n        \"\"\"\n        \u901a\u7528GNN\u524d\u5411\u4f20\u64ad\uff0c\u517c\u5bb9\u4e24\u79cd\u8f93\u5165\u6a21\u5f0f\uff1a\n        1. batch_vector\u6a21\u5f0f\uff08\u53d8\u8282\u70b9\u6570batch\uff0cPyG\u98ce\u683c\uff0c\u9002\u5408\u5927\u89c4\u6a21\u5f02\u6784\u56fe/\u591a\u6d41\u57df\u62fc\u63a5\uff09\n        2. \u4f20\u7edfbatch\u7ef4\u5ea6\u6a21\u5f0f\uff08[batch, num_nodes, ...]\uff0c\u9002\u5408\u5b9a\u957f\u8282\u70b9\u6570\uff09\n\n        \u53c2\u6570:\n            x: \u8282\u70b9\u7279\u5f81\u5f20\u91cf\uff0cshape\u89c1\u4e0a\n            edge_index: \u8fb9\u7d22\u5f15\uff0cPyG\u683c\u5f0f\n            edge_weight: \u8fb9\u6743\u91cd\uff0c\u82e5\u4e3aNone\u5219\u81ea\u52a8\u88651\n            batch_vector: \u8282\u70b9\u5230batch\u7684\u6620\u5c04\uff08\u5982\u6709\uff09\n            evo_tracking: \u662f\u5426\u8bb0\u5f55\u6bcf\u5c42\u8f93\u51fa\n        \u8fd4\u56de:\n            \u9884\u6d4b\u7ed3\u679c\uff0c\u6216(\u9884\u6d4b, \u6f14\u5316\u5e8f\u5217)\n        \"\"\"\n        # \u4fdd\u9669\uff1a\u6240\u6709\u8f93\u5165 tensor \u5f3a\u5236\u540c\u6b65\u5230 encoder.device\uff0c\u9632\u6b62 device \u4e0d\u4e00\u81f4\n        device = self.encoder.weight.device\n        x = x.to(device)\n        edge_index = edge_index.to(device)\n        edge_weight = edge_weight.to(device)\n        if batch_vector is not None:\n            batch_vector = batch_vector.to(device)\n        if batch_vector is not None:\n            # \u652f\u6301 x \u4e3a [batch, num_nodes, window_size, num_features] \u6216 [total_nodes, window_size, num_features]\n            if x.dim() == 4:\n                x = x.view(-1, x.size(2), x.size(3))\n            elif x.dim() != 3:\n                raise ValueError(f\"Unsupported x shape for GNN batch_vector mode: {x.shape}\")\n            x = x.view(x.size(0), -1)\n            x_0 = self.encoder(x)\n            evolution = [x_0.detach()] if evo_tracking else None\n            x = x_0\n            for layer in self.layers:\n                x = self.apply_layer(layer, x, x_0, edge_index, edge_weight)\n                if evo_tracking:\n                    evolution.append(x.detach())\n            x = self.decoder(x)\n            if self.root_gauge_idx is not None:\n                if evo_tracking:\n                    evolution.append(x.detach())\n                batch_size = batch_vector.max().item() + 1\n                # PyTorch\u539f\u751f\u5b9e\u73b0batch mean\u805a\u5408\n                out_sum = torch.zeros(batch_size, x.size(-1), device=x.device)\n                out_sum = out_sum.index_add(0, batch_vector, x)\n                count = torch.zeros(batch_size, device=x.device)\n                count = count.index_add(0, batch_vector, torch.ones_like(batch_vector, dtype=x.dtype))\n                count = count.clamp_min(1).unsqueeze(-1)\n                x = out_sum / count\n                # \u4fdd\u8bc1\u8f93\u51fa shape \u4e3a [batch, time, feature]\uff0c\u5373 [batch, output_size, 1]\uff08\u5982\u679c output_size=\u65f6\u95f4\u6b65\uff0c\u7279\u5f81\u6570=1\uff09\n                if x.dim() == 2:\n                    x = x.unsqueeze(-1)\n                if evo_tracking:\n                    return x, evolution\n            else:\n                # \u4fdd\u8bc1\u8f93\u51fa shape \u4e3a [node, time, feature]\uff0c\u5373 [N, output_size, 1]\n                if x.dim() == 2:\n                    x = x.unsqueeze(-1)\n            return (x, evolution) if evo_tracking else x\n        else:\n            # \u6807\u51c6 batch \u6a21\u5f0f\uff0c\u8981\u6c42 edge_weight \u5fc5\u987b\u8f93\u5165\uff0c\u548c edge_index \u4e00\u81f4\n            batch_size, num_nodes, window_size, num_features = x.shape\n            x = x.view(batch_size * num_nodes, window_size * num_features)\n            # edge_index: [2, num_edges] \u6216 [batch, 2, num_edges]\n            if edge_index.dim() == 3:\n                node_offsets = torch.arange(batch_size, device=edge_index.device) * num_nodes\n                node_offsets = node_offsets.view(-1, 1, 1)\n                edge_index_offset = edge_index + node_offsets\n                edge_index = edge_index_offset.transpose(0, 1).contiguous().view(2, -1)\n            else:\n                if batch_size &gt; 1:\n                    edge_indices = []\n                    for b in range(batch_size):\n                        offset = b * num_nodes\n                        edge_indices.append(edge_index + offset)\n                    edge_index = torch.cat(edge_indices, dim=1)\n            # \u6dfb\u52a0\u81ea\u73af\uff08\u5982\u6709\u9700\u8981\uff0c\u53ef\u5728\u6570\u636e\u96c6\u9884\u5904\u7406\uff09\n            # edge_index, edge_weight = add_self_loops(edge_index, edge_weight, num_nodes=batch_size * num_nodes)\n            x_0 = self.encoder(x)\n            evolution: Optional[List[torch.Tensor]] = [x_0.detach()] if evo_tracking else None\n            x = x_0\n            for layer in self.layers:\n                x = self.apply_layer(layer, x, x_0, edge_index, edge_weight)\n                if evo_tracking:\n                    evolution.append(x.detach())\n            x = self.decoder(x)\n            if self.root_gauge_idx is not None:\n                if evo_tracking:\n                    evolution.append(x.detach())\n                x = x.view(batch_size, num_nodes, self.output_size)\n                if self.aggregation_layer is None:\n                    input_dim = num_nodes * self.output_size\n                    self.aggregation_layer = Linear(\n                        input_dim, self.output_size, weight_initializer=\"kaiming_uniform\"\n                    ).to(x.device)\n                x_flat = x.view(batch_size, -1)\n                x = self.aggregation_layer(x_flat)\n                if evo_tracking:\n                    return x, evolution\n            else:\n                x = x.view(batch_size, num_nodes, self.output_size)\n            return (x, evolution) if evo_tracking else x\n\n    @abstractmethod\n    def apply_layer(\n        self,\n        layer: Module,\n        x: torch.Tensor,\n        x_0: torch.Tensor,\n        edge_index: torch.Tensor,\n        edge_weights: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        pass\n</code></pre>"},{"location":"api/models/#torchhydro.models.gnn.GNNBaseModel.forward","title":"<code>forward(self, x, edge_index, edge_weight, batch_vector=None, evo_tracking=False, **kwargs)</code>","text":"<p>\u901a\u7528GNN\u524d\u5411\u4f20\u64ad\uff0c\u517c\u5bb9\u4e24\u79cd\u8f93\u5165\u6a21\u5f0f\uff1a 1. batch_vector\u6a21\u5f0f\uff08\u53d8\u8282\u70b9\u6570batch\uff0cPyG\u98ce\u683c\uff0c\u9002\u5408\u5927\u89c4\u6a21\u5f02\u6784\u56fe/\u591a\u6d41\u57df\u62fc\u63a5\uff09 2. \u4f20\u7edfbatch\u7ef4\u5ea6\u6a21\u5f0f\uff08[batch, num_nodes, ...]\uff0c\u9002\u5408\u5b9a\u957f\u8282\u70b9\u6570\uff09</p> <p>!!! \u53c2\u6570     x: \u8282\u70b9\u7279\u5f81\u5f20\u91cf\uff0cshape\u89c1\u4e0a     edge_index: \u8fb9\u7d22\u5f15\uff0cPyG\u683c\u5f0f     edge_weight: \u8fb9\u6743\u91cd\uff0c\u82e5\u4e3aNone\u5219\u81ea\u52a8\u88651     batch_vector: \u8282\u70b9\u5230batch\u7684\u6620\u5c04\uff08\u5982\u6709\uff09     evo_tracking: \u662f\u5426\u8bb0\u5f55\u6bcf\u5c42\u8f93\u51fa !!! \u8fd4\u56de     \u9884\u6d4b\u7ed3\u679c\uff0c\u6216(\u9884\u6d4b, \u6f14\u5316\u5e8f\u5217)</p> Source code in <code>torchhydro/models/gnn.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    edge_index: torch.Tensor,\n    edge_weight: torch.Tensor,\n    batch_vector: Optional[torch.Tensor] = None,\n    evo_tracking: bool = False,\n    **kwargs,\n) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]]]:\n    \"\"\"\n    \u901a\u7528GNN\u524d\u5411\u4f20\u64ad\uff0c\u517c\u5bb9\u4e24\u79cd\u8f93\u5165\u6a21\u5f0f\uff1a\n    1. batch_vector\u6a21\u5f0f\uff08\u53d8\u8282\u70b9\u6570batch\uff0cPyG\u98ce\u683c\uff0c\u9002\u5408\u5927\u89c4\u6a21\u5f02\u6784\u56fe/\u591a\u6d41\u57df\u62fc\u63a5\uff09\n    2. \u4f20\u7edfbatch\u7ef4\u5ea6\u6a21\u5f0f\uff08[batch, num_nodes, ...]\uff0c\u9002\u5408\u5b9a\u957f\u8282\u70b9\u6570\uff09\n\n    \u53c2\u6570:\n        x: \u8282\u70b9\u7279\u5f81\u5f20\u91cf\uff0cshape\u89c1\u4e0a\n        edge_index: \u8fb9\u7d22\u5f15\uff0cPyG\u683c\u5f0f\n        edge_weight: \u8fb9\u6743\u91cd\uff0c\u82e5\u4e3aNone\u5219\u81ea\u52a8\u88651\n        batch_vector: \u8282\u70b9\u5230batch\u7684\u6620\u5c04\uff08\u5982\u6709\uff09\n        evo_tracking: \u662f\u5426\u8bb0\u5f55\u6bcf\u5c42\u8f93\u51fa\n    \u8fd4\u56de:\n        \u9884\u6d4b\u7ed3\u679c\uff0c\u6216(\u9884\u6d4b, \u6f14\u5316\u5e8f\u5217)\n    \"\"\"\n    # \u4fdd\u9669\uff1a\u6240\u6709\u8f93\u5165 tensor \u5f3a\u5236\u540c\u6b65\u5230 encoder.device\uff0c\u9632\u6b62 device \u4e0d\u4e00\u81f4\n    device = self.encoder.weight.device\n    x = x.to(device)\n    edge_index = edge_index.to(device)\n    edge_weight = edge_weight.to(device)\n    if batch_vector is not None:\n        batch_vector = batch_vector.to(device)\n    if batch_vector is not None:\n        # \u652f\u6301 x \u4e3a [batch, num_nodes, window_size, num_features] \u6216 [total_nodes, window_size, num_features]\n        if x.dim() == 4:\n            x = x.view(-1, x.size(2), x.size(3))\n        elif x.dim() != 3:\n            raise ValueError(f\"Unsupported x shape for GNN batch_vector mode: {x.shape}\")\n        x = x.view(x.size(0), -1)\n        x_0 = self.encoder(x)\n        evolution = [x_0.detach()] if evo_tracking else None\n        x = x_0\n        for layer in self.layers:\n            x = self.apply_layer(layer, x, x_0, edge_index, edge_weight)\n            if evo_tracking:\n                evolution.append(x.detach())\n        x = self.decoder(x)\n        if self.root_gauge_idx is not None:\n            if evo_tracking:\n                evolution.append(x.detach())\n            batch_size = batch_vector.max().item() + 1\n            # PyTorch\u539f\u751f\u5b9e\u73b0batch mean\u805a\u5408\n            out_sum = torch.zeros(batch_size, x.size(-1), device=x.device)\n            out_sum = out_sum.index_add(0, batch_vector, x)\n            count = torch.zeros(batch_size, device=x.device)\n            count = count.index_add(0, batch_vector, torch.ones_like(batch_vector, dtype=x.dtype))\n            count = count.clamp_min(1).unsqueeze(-1)\n            x = out_sum / count\n            # \u4fdd\u8bc1\u8f93\u51fa shape \u4e3a [batch, time, feature]\uff0c\u5373 [batch, output_size, 1]\uff08\u5982\u679c output_size=\u65f6\u95f4\u6b65\uff0c\u7279\u5f81\u6570=1\uff09\n            if x.dim() == 2:\n                x = x.unsqueeze(-1)\n            if evo_tracking:\n                return x, evolution\n        else:\n            # \u4fdd\u8bc1\u8f93\u51fa shape \u4e3a [node, time, feature]\uff0c\u5373 [N, output_size, 1]\n            if x.dim() == 2:\n                x = x.unsqueeze(-1)\n        return (x, evolution) if evo_tracking else x\n    else:\n        # \u6807\u51c6 batch \u6a21\u5f0f\uff0c\u8981\u6c42 edge_weight \u5fc5\u987b\u8f93\u5165\uff0c\u548c edge_index \u4e00\u81f4\n        batch_size, num_nodes, window_size, num_features = x.shape\n        x = x.view(batch_size * num_nodes, window_size * num_features)\n        # edge_index: [2, num_edges] \u6216 [batch, 2, num_edges]\n        if edge_index.dim() == 3:\n            node_offsets = torch.arange(batch_size, device=edge_index.device) * num_nodes\n            node_offsets = node_offsets.view(-1, 1, 1)\n            edge_index_offset = edge_index + node_offsets\n            edge_index = edge_index_offset.transpose(0, 1).contiguous().view(2, -1)\n        else:\n            if batch_size &gt; 1:\n                edge_indices = []\n                for b in range(batch_size):\n                    offset = b * num_nodes\n                    edge_indices.append(edge_index + offset)\n                edge_index = torch.cat(edge_indices, dim=1)\n        # \u6dfb\u52a0\u81ea\u73af\uff08\u5982\u6709\u9700\u8981\uff0c\u53ef\u5728\u6570\u636e\u96c6\u9884\u5904\u7406\uff09\n        # edge_index, edge_weight = add_self_loops(edge_index, edge_weight, num_nodes=batch_size * num_nodes)\n        x_0 = self.encoder(x)\n        evolution: Optional[List[torch.Tensor]] = [x_0.detach()] if evo_tracking else None\n        x = x_0\n        for layer in self.layers:\n            x = self.apply_layer(layer, x, x_0, edge_index, edge_weight)\n            if evo_tracking:\n                evolution.append(x.detach())\n        x = self.decoder(x)\n        if self.root_gauge_idx is not None:\n            if evo_tracking:\n                evolution.append(x.detach())\n            x = x.view(batch_size, num_nodes, self.output_size)\n            if self.aggregation_layer is None:\n                input_dim = num_nodes * self.output_size\n                self.aggregation_layer = Linear(\n                    input_dim, self.output_size, weight_initializer=\"kaiming_uniform\"\n                ).to(x.device)\n            x_flat = x.view(batch_size, -1)\n            x = self.aggregation_layer(x_flat)\n            if evo_tracking:\n                return x, evolution\n        else:\n            x = x.view(batch_size, num_nodes, self.output_size)\n        return (x, evolution) if evo_tracking else x\n</code></pre>"},{"location":"api/models/#torchhydro.models.kernel_conv","title":"<code>kernel_conv</code>","text":""},{"location":"api/models/#torchhydro.models.kernel_conv.KernelConv","title":"<code> KernelConv            (Module)         </code>","text":"Source code in <code>torchhydro/models/kernel_conv.py</code> <pre><code>class KernelConv(nn.Module):\n    def __init__(self, a, theta, kernel_size):\n        \"\"\"\n        The convolution kernel for the convolution operation in routing module\n\n        We use two-parameter gamma distribution to determine the unit hydrograph,\n        which comes from [mizuRoute](http://www.geosci-model-dev.net/9/2223/2016/)\n\n        Parameters\n        ----------\n        a\n            shape parameter\n        theta\n            timescale parameter\n        kernel_size\n            the size of conv kernel\n        \"\"\"\n        super(KernelConv, self).__init__()\n        self.a = a\n        self.theta = theta\n        routa = self.a.repeat(kernel_size, 1).unsqueeze(-1)\n        routb = self.theta.repeat(kernel_size, 1).unsqueeze(-1)\n        self.uh_gamma = uh_gamma(routa, routb, len_uh=kernel_size)\n\n    def forward(self, x):\n        \"\"\"\n        1d-convolution calculation\n\n        Parameters\n        ----------\n        x\n            x is a sequence-first variable, so the dim of x is [seq, batch, feature]\n\n        Returns\n        -------\n        torch.Tensor\n            convolution\n        \"\"\"\n        # dim: permute from [len_uh, batch, feature] to [batch, feature, len_uh]\n        uh = self.uh_gamma.permute(1, 2, 0)\n        # the dim of conv kernel in F.conv1d is out_channels, in_channels (feature)/groups, width (seq)\n        # the dim of inputs in F.conv1d are batch, in_channels (feature) and width (seq),\n        # each element in a batch should has its own conv kernel,\n        # hence set groups = batch_size and permute input's batch-dim to channel-dim to make \"groups\" work\n        inputs = x.permute(2, 1, 0)\n        batch_size = x.shape[1]\n        # conv1d in NN is different from the general convolution: it is lack of a flip\n        outputs = F.conv1d(\n            inputs, torch.flip(uh, [2]), groups=batch_size, padding=uh.shape[-1] - 1\n        )\n        # permute from [feature, batch, seq] to [seq, batch, feature]\n        return outputs[:, :, : -(uh.shape[-1] - 1)].permute(2, 1, 0)\n</code></pre>"},{"location":"api/models/#torchhydro.models.kernel_conv.KernelConv.__init__","title":"<code>__init__(self, a, theta, kernel_size)</code>  <code>special</code>","text":"<p>The convolution kernel for the convolution operation in routing module</p> <p>We use two-parameter gamma distribution to determine the unit hydrograph, which comes from mizuRoute</p>"},{"location":"api/models/#torchhydro.models.kernel_conv.KernelConv.__init__--parameters","title":"Parameters","text":"<p>a     shape parameter theta     timescale parameter kernel_size     the size of conv kernel</p> Source code in <code>torchhydro/models/kernel_conv.py</code> <pre><code>def __init__(self, a, theta, kernel_size):\n    \"\"\"\n    The convolution kernel for the convolution operation in routing module\n\n    We use two-parameter gamma distribution to determine the unit hydrograph,\n    which comes from [mizuRoute](http://www.geosci-model-dev.net/9/2223/2016/)\n\n    Parameters\n    ----------\n    a\n        shape parameter\n    theta\n        timescale parameter\n    kernel_size\n        the size of conv kernel\n    \"\"\"\n    super(KernelConv, self).__init__()\n    self.a = a\n    self.theta = theta\n    routa = self.a.repeat(kernel_size, 1).unsqueeze(-1)\n    routb = self.theta.repeat(kernel_size, 1).unsqueeze(-1)\n    self.uh_gamma = uh_gamma(routa, routb, len_uh=kernel_size)\n</code></pre>"},{"location":"api/models/#torchhydro.models.kernel_conv.KernelConv.forward","title":"<code>forward(self, x)</code>","text":"<p>1d-convolution calculation</p>"},{"location":"api/models/#torchhydro.models.kernel_conv.KernelConv.forward--parameters","title":"Parameters","text":"<p>x     x is a sequence-first variable, so the dim of x is [seq, batch, feature]</p>"},{"location":"api/models/#torchhydro.models.kernel_conv.KernelConv.forward--returns","title":"Returns","text":"<p>torch.Tensor     convolution</p> Source code in <code>torchhydro/models/kernel_conv.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    1d-convolution calculation\n\n    Parameters\n    ----------\n    x\n        x is a sequence-first variable, so the dim of x is [seq, batch, feature]\n\n    Returns\n    -------\n    torch.Tensor\n        convolution\n    \"\"\"\n    # dim: permute from [len_uh, batch, feature] to [batch, feature, len_uh]\n    uh = self.uh_gamma.permute(1, 2, 0)\n    # the dim of conv kernel in F.conv1d is out_channels, in_channels (feature)/groups, width (seq)\n    # the dim of inputs in F.conv1d are batch, in_channels (feature) and width (seq),\n    # each element in a batch should has its own conv kernel,\n    # hence set groups = batch_size and permute input's batch-dim to channel-dim to make \"groups\" work\n    inputs = x.permute(2, 1, 0)\n    batch_size = x.shape[1]\n    # conv1d in NN is different from the general convolution: it is lack of a flip\n    outputs = F.conv1d(\n        inputs, torch.flip(uh, [2]), groups=batch_size, padding=uh.shape[-1] - 1\n    )\n    # permute from [feature, batch, seq] to [seq, batch, feature]\n    return outputs[:, :, : -(uh.shape[-1] - 1)].permute(2, 1, 0)\n</code></pre>"},{"location":"api/models/#torchhydro.models.kernel_conv.uh_conv","title":"<code>uh_conv(x, uh_made)</code>","text":"<p>Function for 1d-convolution calculation</p>"},{"location":"api/models/#torchhydro.models.kernel_conv.uh_conv--parameters","title":"Parameters","text":"<p>x     x is a sequence-first variable, so the dim of x is [seq, batch, feature] uh_made     unit hydrograph from uh_gamma or other unit-hydrograph method</p>"},{"location":"api/models/#torchhydro.models.kernel_conv.uh_conv--returns","title":"Returns","text":"<p>torch.Tensor     convolution, [seq, batch, feature]; the length of seq is same as x's</p> Source code in <code>torchhydro/models/kernel_conv.py</code> <pre><code>def uh_conv(x, uh_made) -&gt; torch.Tensor:\n    \"\"\"\n    Function for 1d-convolution calculation\n\n    Parameters\n    ----------\n    x\n        x is a sequence-first variable, so the dim of x is [seq, batch, feature]\n    uh_made\n        unit hydrograph from uh_gamma or other unit-hydrograph method\n\n    Returns\n    -------\n    torch.Tensor\n        convolution, [seq, batch, feature]; the length of seq is same as x's\n    \"\"\"\n    uh = uh_made.permute(1, 2, 0)\n    # the dim of conv kernel in F.conv1d is out_channels, in_channels (feature)/groups, width (seq)\n    # the dim of inputs in F.conv1d are batch, in_channels (feature) and width (seq),\n    # each element in a batch should has its own conv kernel,\n    # hence set groups = batch_size and permute input's batch-dim to channel-dim to make \"groups\" work\n    inputs = x.permute(2, 1, 0)\n    batch_size = x.shape[1]\n    # conv1d in NN is different from the general convolution: it is lack of a flip\n    outputs = F.conv1d(\n        inputs, torch.flip(uh, [2]), groups=batch_size, padding=uh.shape[-1] - 1\n    )\n    # cut to same shape with x and permute from [feature, batch, seq] to [seq, batch, feature]\n    return outputs[:, :, : x.shape[0]].permute(2, 1, 0)\n</code></pre>"},{"location":"api/models/#torchhydro.models.kernel_conv.uh_gamma","title":"<code>uh_gamma(a, theta, len_uh=10)</code>","text":"<p>A simple two-parameter Gamma distribution as a unit-hydrograph to route instantaneous runoff from a hydrologic model</p> <p>The method comes from mizuRoute -- http://www.geosci-model-dev.net/9/2223/2016/</p>"},{"location":"api/models/#torchhydro.models.kernel_conv.uh_gamma--parameters","title":"Parameters","text":"<p>a     shape parameter theta     timescale parameter len_uh     the time length of the unit hydrograph</p>"},{"location":"api/models/#torchhydro.models.kernel_conv.uh_gamma--returns","title":"Returns","text":"<p>torch.Tensor     the unit hydrograph, dim: [seq, batch, feature]</p> Source code in <code>torchhydro/models/kernel_conv.py</code> <pre><code>def uh_gamma(a, theta, len_uh=10):\n    \"\"\"\n    A simple two-parameter Gamma distribution as a unit-hydrograph to route instantaneous runoff from a hydrologic model\n\n    The method comes from mizuRoute -- http://www.geosci-model-dev.net/9/2223/2016/\n\n    Parameters\n    ----------\n    a\n        shape parameter\n    theta\n        timescale parameter\n    len_uh\n        the time length of the unit hydrograph\n\n    Returns\n    -------\n    torch.Tensor\n        the unit hydrograph, dim: [seq, batch, feature]\n\n    \"\"\"\n    # dims of a: time_seq (same all time steps), batch, feature=1\n    m = a.shape\n    assert len_uh &lt;= m[0]\n    # aa &gt; 0, here we set minimum 0.1 (min of a is 0, set when calling this func); First dimension of a is repeat\n    aa = F.relu(a[0:len_uh, :, :]) + 0.1\n    # theta &gt; 0, here set minimum 0.5\n    theta = F.relu(theta[0:len_uh, :, :]) + 0.5\n    # len_f, batch, feature\n    t = (\n        torch.arange(0.5, len_uh * 1.0)\n        .view([len_uh, 1, 1])\n        .repeat([1, m[1], m[2]])\n        .to(aa.device)\n    )\n    denominator = (aa.lgamma().exp()) * (theta**aa)\n    # [len_f, m[1], m[2]]\n    w = 1 / denominator * (t ** (aa - 1)) * (torch.exp(-t / theta))\n    w = w / w.sum(0)  # scale to 1 for each UH\n    return w\n</code></pre>"},{"location":"api/models/#torchhydro.models.model_dict_function","title":"<code>model_dict_function</code>","text":"<p>Author: Wenyu Ouyang Date: 2021-12-31 11:08:29 LastEditTime: 2025-07-13 18:17:48 LastEditors: Wenyu Ouyang Description: Dicts including models (which are seq-first), losses, and optims FilePath:       orchhydro       orchhydro\\models\\model_dict_function.py Copyright (c) 2021-2022 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/models/#torchhydro.models.model_utils","title":"<code>model_utils</code>","text":"<p>Author: Wenyu Ouyang Date: 2021-08-09 10:19:13 LastEditTime: 2025-04-15 12:59:35 LastEditors: Wenyu Ouyang Description: Some util functions for modeling FilePath: /torchhydro/torchhydro/models/model_utils.py Copyright (c) 2021-2022 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/models/#torchhydro.models.model_utils.get_the_device","title":"<code>get_the_device(device_num)</code>","text":"<p>Get device for torch according to its name</p>"},{"location":"api/models/#torchhydro.models.model_utils.get_the_device--parameters","title":"Parameters","text":"<p>device_num : Union[list, int]     number of the device -- -1 means \"cpu\" or 0, 1, ... means \"cuda:x\" or \"mps:x\"</p> Source code in <code>torchhydro/models/model_utils.py</code> <pre><code>def get_the_device(device_num: Union[list, int]):\n    \"\"\"\n    Get device for torch according to its name\n\n    Parameters\n    ----------\n    device_num : Union[list, int]\n        number of the device -- -1 means \"cpu\" or 0, 1, ... means \"cuda:x\" or \"mps:x\"\n    \"\"\"\n    if device_num in [[-1], -1, [\"-1\"]]:\n        return torch.device(\"cpu\")\n    if torch.cuda.is_available():\n        return (\n            torch.device(f\"cuda:{str(device_num)}\")\n            if type(device_num) is not list\n            else torch.device(f\"cuda:{str(device_num[0])}\")\n        )\n    # Check for MPS (MacOS)\n    mps_available = False\n    with contextlib.suppress(AttributeError):\n        mps_available = torch.backends.mps.is_available()\n    if mps_available:\n        if device_num != 0:\n            warnings.warn(\n                f\"MPS only supports device 0. Using 'mps:0' instead of {device_num}.\"\n            )\n        return torch.device(\"mps:0\")\n    if device_num not in [[-1], -1, [\"-1\"]]:\n        warnings.warn(\"You don't have GPU, so have to choose cpu for models\")\n    return torch.device(\"cpu\")\n</code></pre>"},{"location":"api/models/#torchhydro.models.mtslstm","title":"<code>mtslstm</code>","text":""},{"location":"api/models/#torchhydro.models.mtslstm.MTSLSTM","title":"<code> MTSLSTM            (Module)         </code>","text":"<p>Multi-Temporal-Scale LSTM (MTS-LSTM).</p> <p>This model processes multi-frequency time-series data (hour/day/week) by aggregating high-frequency (hourly) inputs into lower-frequency branches. It supports per-feature down-aggregation, optional state transfer between frequency branches, and loading pretrained weights for the daily branch.</p> <p>Example usage:     # Unified hourly input (recommended)     model = MTSLSTM(         hidden_sizes=[64, 64, 64],         output_size=1,         feature_buckets=[2, 2, 1, 0, ...],         frequency_factors=[7, 24],   # week-&gt;day \u00d77, day-&gt;hour \u00d724         seq_lengths=[T_week, T_day, T_hour],         slice_transfer=True,     )     y = model(x_hour)  # x_hour: (T_hour, B, D)</p> <pre><code># Legacy: explicitly provide each frequency branch\ny = model(x_week, x_day, x_hour)\n</code></pre> Source code in <code>torchhydro/models/mtslstm.py</code> <pre><code>class MTSLSTM(nn.Module):\n    \"\"\"Multi-Temporal-Scale LSTM (MTS-LSTM).\n\n    This model processes multi-frequency time-series data (hour/day/week) by\n    aggregating high-frequency (hourly) inputs into lower-frequency branches.\n    It supports per-feature down-aggregation, optional state transfer between\n    frequency branches, and loading pretrained weights for the daily branch.\n\n    Example usage:\n        # Unified hourly input (recommended)\n        model = MTSLSTM(\n            hidden_sizes=[64, 64, 64],\n            output_size=1,\n            feature_buckets=[2, 2, 1, 0, ...],\n            frequency_factors=[7, 24],   # week-&gt;day \u00d77, day-&gt;hour \u00d724\n            seq_lengths=[T_week, T_day, T_hour],\n            slice_transfer=True,\n        )\n        y = model(x_hour)  # x_hour: (T_hour, B, D)\n\n        # Legacy: explicitly provide each frequency branch\n        y = model(x_week, x_day, x_hour)\n    \"\"\"\n\n    def __init__(\n        self,\n        input_sizes: Union[int, List[int], None] = None,\n        hidden_sizes: Union[int, List[int]] = 128,\n        output_size: int = 1,\n        shared_mtslstm: bool = False,\n        transfer: Union[\n            None, str, Dict[str, Optional[Literal[\"identity\", \"linear\"]]]\n        ] = \"linear\",\n        dropout: float = 0.0,\n        return_all: bool = False,\n        add_freq_one_hot_if_shared: bool = True,\n        auto_build_lowfreq: bool = False,\n        build_factor: int = 7,\n        agg_reduce: Literal[\"mean\", \"sum\"] = \"mean\",\n        per_feature_aggs: Optional[List[Literal[\"mean\", \"sum\"]]] = None,\n        truncate_incomplete: bool = True,\n        slice_transfer: bool = True,\n        slice_use_ceil: bool = True,\n        seq_lengths: Optional[List[int]] = None,\n        frequency_factors: Optional[List[int]] = None,\n        feature_buckets: Optional[List[int]] = None,\n        per_feature_aggs_map: Optional[List[Literal[\"mean\", \"sum\"]]] = None,\n        down_aggregate_all_to_each_branch: bool = True,\n        pretrained_day_path: Optional[str] = None,\n        pretrained_lstm_prefix: Optional[str] = None,\n        pretrained_head_prefix: Optional[str] = None,\n        pretrained_flag: bool = False,\n        linear1_size: Optional[int] = None,\n        linear2_size: Optional[int] = None\n    ):\n        \"\"\"Initializes an MTSLSTM model.\n\n        Args:\n            input_sizes: Input feature dimension(s). Can be:\n                * int: shared across all frequency branches\n                * list: per-frequency input sizes\n                * None: inferred from `feature_buckets`\n            hidden_sizes: Hidden dimension(s) for each LSTM branch.\n            output_size: Output dimension per timestep.\n            shared_mtslstm: If True, all frequency branches share one LSTM.\n            transfer: Hidden state transfer mode between frequencies.\n                * None: no transfer\n                * \"identity\": copy states directly (same dim required)\n                * \"linear\": learn linear projection between dims\n            dropout: Dropout probability applied before heads.\n            return_all: If True, return all branch outputs (dict f0,f1,...).\n                If False, return only the highest-frequency output.\n            add_freq_one_hot_if_shared: If True and `shared_mtslstm=True`,\n                append frequency one-hot encoding to inputs.\n            auto_build_lowfreq: Legacy 2-frequency path (high-&gt;low).\n            build_factor: Aggregation factor for auto low-frequency.\n            agg_reduce: Aggregation method for downsampling (\"mean\" or \"sum\").\n            per_feature_aggs: Optional list of per-feature aggregation methods.\n            truncate_incomplete: Whether to drop remainder timesteps when\n                aggregating (vs. zero-padding).\n            slice_transfer: If True, transfer LSTM states at slice boundaries\n                computed by seq_lengths \u00d7 frequency_factors.\n            slice_use_ceil: If True, use ceil for slice length calculation.\n            seq_lengths: Per-frequency sequence lengths [low, ..., high].\n            frequency_factors: Multipliers between adjacent frequencies.\n                Example: [7,24] means week-&gt;day \u00d77, day-&gt;hour \u00d724.\n            feature_buckets: Per-feature frequency assignment (len = D).\n                0 = lowest (week), nf-1 = highest (hour).\n            per_feature_aggs_map: Per-feature aggregation method (\"mean\"/\"sum\").\n            down_aggregate_all_to_each_branch: If True, branch f includes all\n                features with bucket &gt;= f (down-aggregate); else only == f.\n            pretrained_day_path: Optional path to pretrained checkpoint. If set,\n                loads weights for the daily (f1) LSTM and head.\n\n        Raises:\n            AssertionError: If configuration is inconsistent.\n        \"\"\"\n        super().__init__()\n\n        # Store configuration parameters\n        self.pretrained_day_path = pretrained_day_path\n        self.pretrained_flag = pretrained_flag\n        self.linear1_size = linear1_size\n        self.linear2_size = linear2_size\n        self.output_size = output_size\n        self.shared = shared_mtslstm\n        self.return_all_default = return_all\n        self.feature_buckets = list(feature_buckets) if feature_buckets is not None else None\n        self.per_feature_aggs_map = list(per_feature_aggs_map) if per_feature_aggs_map is not None else None\n        self.down_agg_all = down_aggregate_all_to_each_branch\n        self.auto_build_lowfreq = auto_build_lowfreq\n\n        # Aggregation and slicing parameters\n        assert build_factor &gt;= 2, \"build_factor must be &gt;=2\"\n        self.build_factor = int(build_factor)\n        self.agg_reduce = agg_reduce\n        self.per_feature_aggs = per_feature_aggs\n        self.truncate_incomplete = truncate_incomplete\n        self.slice_transfer = slice_transfer\n        self.slice_use_ceil = slice_use_ceil\n        self.seq_lengths = list(seq_lengths) if seq_lengths is not None else None\n        self._warned_slice_fallback = False\n\n        # Setup frequency configuration\n        self._setup_frequency_config(feature_buckets, input_sizes, auto_build_lowfreq)\n\n        # Validate seq_lengths\n        if self.seq_lengths is not None:\n            assert len(self.seq_lengths) == self.nf, \"seq_lengths length must match nf\"\n\n        # Setup input sizes for each branch\n        self.base_input_sizes = self._setup_input_sizes(\n            self.feature_buckets, input_sizes, self.down_agg_all\n        )\n\n        # Setup hidden layer sizes\n        if isinstance(hidden_sizes, int):\n            self.hidden_sizes = [hidden_sizes] * self.nf\n        else:\n            assert len(hidden_sizes) == self.nf, \"hidden_sizes length mismatch\"\n            self.hidden_sizes = list(hidden_sizes)\n\n        # Setup frequency one-hot encoding\n        self.add_freq1hot = add_freq_one_hot_if_shared and self.shared\n\n        # Setup transfer configuration\n        self._setup_transfer_config(transfer)\n\n        # Setup frequency factors and slice timesteps\n        self._setup_frequency_factors(frequency_factors, auto_build_lowfreq, self.build_factor)\n\n        # Calculate effective input sizes (including one-hot if needed)\n        eff_input_sizes = self.base_input_sizes[:]\n        if self.add_freq1hot:\n            eff_input_sizes = [d + self.nf for d in eff_input_sizes]\n\n        if self.shared and len(set(eff_input_sizes)) != 1:\n            raise ValueError(\"shared_mtslstm=True requires equal input sizes.\")\n\n        # Create model layers\n        self._create_model_layers(eff_input_sizes)\n\n        # Create transfer layers\n        self._create_transfer_layers()\n\n        # Setup dropout\n        self.dropout = nn.Dropout(p=dropout)\n\n        # Set unified hourly aggregation flag\n        self.use_hourly_unified = self.feature_buckets is not None\n\n        # Load pretrained weights if specified\n        self._load_pretrained_weights(pretrained_lstm_prefix, pretrained_head_prefix)\n\n    def _setup_frequency_config(self, feature_buckets, input_sizes, auto_build_lowfreq):\n        \"\"\"Setup frequency configuration and compute number of frequencies.\"\"\"\n        if feature_buckets is not None:\n            assert len(feature_buckets) &gt; 0, \"feature_buckets cannot be empty\"\n            self.nf = max(feature_buckets) + 1\n        else:\n            if isinstance(input_sizes, int) or input_sizes is None:\n                self.nf = 2 if auto_build_lowfreq else 2\n            else:\n                assert len(input_sizes) &gt;= 2, \"At least 2 frequencies required\"\n                self.nf = len(input_sizes)\n\n    def _setup_input_sizes(self, feature_buckets, input_sizes, down_aggregate_all_to_each_branch):\n        \"\"\"Setup input sizes for each frequency branch.\"\"\"\n        if feature_buckets is not None:\n            D = len(feature_buckets)\n            if down_aggregate_all_to_each_branch:\n                base_input_sizes = [\n                    sum(1 for k in range(D) if feature_buckets[k] &gt;= f)\n                    for f in range(self.nf)\n                ]\n            else:\n                base_input_sizes = [\n                    sum(1 for k in range(D) if feature_buckets[k] == f)\n                    for f in range(self.nf)\n                ]\n        else:\n            if isinstance(input_sizes, int):\n                base_input_sizes = [input_sizes] * self.nf\n            else:\n                base_input_sizes = list(input_sizes)\n\n        assert len(base_input_sizes) == self.nf, \"input_sizes mismatch with nf\"\n        return base_input_sizes\n\n    def _setup_transfer_config(self, transfer):\n        \"\"\"Setup transfer configuration for hidden and cell states.\"\"\"\n        if transfer is None or isinstance(transfer, str):\n            transfer = {\"h\": transfer, \"c\": transfer}\n        self.transfer_mode: Dict[str, Optional[str]] = {\n            \"h\": transfer.get(\"h\", None),\n            \"c\": transfer.get(\"c\", None),\n        }\n        for k in (\"h\", \"c\"):\n            assert self.transfer_mode[k] in (\n                None, \"identity\", \"linear\"\n            ), \"transfer must be None/'identity'/'linear'\"\n\n    def _setup_frequency_factors(self, frequency_factors, auto_build_lowfreq, build_factor):\n        \"\"\"Setup frequency factors and slice timesteps.\"\"\"\n        if frequency_factors is not None:\n            assert (\n                len(frequency_factors) == self.nf - 1\n            ), \"frequency_factors length must be nf-1\"\n            self.frequency_factors = list(map(int, frequency_factors))\n        elif self.nf == 2 and auto_build_lowfreq:\n            self.frequency_factors = [int(build_factor)]\n        else:\n            self.frequency_factors = None\n\n        # Pre-compute slice positions if seq_lengths and frequency_factors are provided\n        self.slice_timesteps: Optional[List[int]] = None\n        if self.seq_lengths is not None and self.frequency_factors is not None:\n            self.slice_timesteps = []\n            for i in range(self.nf - 1):\n                fac = int(self.frequency_factors[i])\n                next_len = int(self.seq_lengths[i + 1])\n                st = int(next_len / fac)  # floor\n                self.slice_timesteps.append(max(0, st))\n\n    def _create_model_layers(self, eff_input_sizes):\n        \"\"\"Create LSTM and linear layers based on configuration.\"\"\"\n        if self.pretrained_flag:\n            # Use pretrained model specific layers\n            self.linear1 = nn.ModuleList([\n                nn.Linear(eff_input_sizes[i], self.linear1_size) for i in range(self.nf)\n            ])\n            self.linear2 = nn.ModuleList([\n                nn.Linear(self.linear1_size, self.linear2_size) for i in range(self.nf)\n            ])\n            self.lstms = nn.ModuleList()\n            if self.shared:\n                self.lstms.append(nn.LSTM(self.hidden_sizes[0], self.hidden_sizes[0]))\n            else:\n                for i in range(self.nf):\n                    self.lstms.append(nn.LSTM(self.linear2_size, self.hidden_sizes[i]))\n        else:\n            # Use default layers when no pretrained model is loaded\n            self.input_linears = nn.ModuleList([\n                nn.Linear(eff_input_sizes[i], self.hidden_sizes[i]) for i in range(self.nf)\n            ])\n            self.lstms = nn.ModuleList()\n            if self.shared:\n                self.lstms.append(nn.LSTM(self.hidden_sizes[0], self.hidden_sizes[0]))\n            else:\n                for i in range(self.nf):\n                    self.lstms.append(nn.LSTM(self.hidden_sizes[i], self.hidden_sizes[i]))\n\n        # Create head layers\n        self.heads = nn.ModuleList()\n        if self.shared:\n            self.heads.append(nn.Linear(self.hidden_sizes[0], self.output_size))\n        else:\n            for i in range(self.nf):\n                self.heads.append(nn.Linear(self.hidden_sizes[i], self.output_size))\n\n    def _create_transfer_layers(self):\n        \"\"\"Create transfer projection layers between frequency branches.\"\"\"\n        self.transfer_h = nn.ModuleList()\n        self.transfer_c = nn.ModuleList()\n        for i in range(self.nf - 1):\n            hs_i = self.hidden_sizes[i]\n            hs_j = self.hidden_sizes[i + 1]\n\n            if self.transfer_mode[\"h\"] == \"linear\":\n                self.transfer_h.append(nn.Linear(hs_i, hs_j))\n            elif self.transfer_mode[\"h\"] == \"identity\":\n                assert hs_i == hs_j, \"identity requires same hidden size\"\n                self.transfer_h.append(nn.Identity())\n            else:\n                self.transfer_h.append(None)\n\n            if self.transfer_mode[\"c\"] == \"linear\":\n                self.transfer_c.append(nn.Linear(hs_i, hs_j))\n            elif self.transfer_mode[\"c\"] == \"identity\":\n                assert hs_i == hs_j, \"identity requires same hidden size\"\n                self.transfer_c.append(nn.Identity())\n            else:\n                self.transfer_c.append(None)\n\n    def _load_pretrained_weights(self, pretrained_lstm_prefix, pretrained_head_prefix):\n        \"\"\"Load pretrained weights for the daily branch if specified.\"\"\"\n        if self.pretrained_day_path is None:\n            return\n\n        if not os.path.isfile(self.pretrained_day_path):\n            warnings.warn(\n                f\"[MTSLSTM] Pretrained file not found: {self.pretrained_day_path}\"\n            )\n            return\n\n        if self.shared:\n            warnings.warn(\n                \"[MTSLSTM] shared_mtslstm=True: skip daily-only pretrained load.\"\n            )\n            return\n\n        if self.nf &lt; 2:\n            warnings.warn(\"[MTSLSTM] nf&lt;2: no daily branch, skip pretrained load.\")\n            return\n\n        try:\n            state = torch.load(self.pretrained_day_path, map_location=\"cpu\")\n            if isinstance(state, dict):\n                if \"state_dict\" in state:\n                    state = state[\"state_dict\"]\n                elif \"model\" in state:\n                    state = state[\"model\"]\n\n            self._load_daily_branch_weights(state, pretrained_lstm_prefix, pretrained_head_prefix)\n\n        except Exception as e:\n            warnings.warn(f\"[MTSLSTM] Failed to load daily pretrained: {e}\")\n\n    def _load_daily_branch_weights(self, state, pretrained_lstm_prefix, pretrained_head_prefix):\n        \"\"\"Load weights for the daily LSTM and head from pretrained state.\"\"\"\n        day_lstm = self.lstms[1]\n        day_head = self.heads[1]\n        lstm_state = day_lstm.state_dict()\n        head_state = day_head.state_dict()\n\n        matched = skipped = shape_mismatch = 0\n\n        def try_load(prefix: str, target_state: Dict[str, torch.Tensor]) -&gt; None:\n            nonlocal matched, skipped, shape_mismatch\n            for k_pre, v in state.items():\n                if not k_pre.startswith(prefix):\n                    continue\n                k = k_pre[len(prefix):]\n                if k in target_state:\n                    if target_state[k].shape == v.shape:\n                        target_state[k].copy_(v)\n                        matched += 1\n                    else:\n                        shape_mismatch += 1\n                else:\n                    skipped += 1\n\n        try_load(pretrained_lstm_prefix, lstm_state)\n        try_load(pretrained_head_prefix, head_state)\n\n        # Load pretrained linearIn into linear2[1]\n        self._load_linear_weights(state)\n\n        # Fallback: try raw state_dict without prefix\n        if matched == 0:\n            for k, v in state.items():\n                if k in lstm_state and lstm_state[k].shape == v.shape:\n                    lstm_state[k].copy_(v)\n                    matched += 1\n                elif k in head_state and head_state[k].shape == v.shape:\n                    head_state[k].copy_(v)\n                    matched += 1\n                else:\n                    shape_mismatch += 1\n\n        self._print_loading_debug_info(state, day_lstm, day_head)\n\n        day_lstm.load_state_dict(lstm_state)\n        day_head.load_state_dict(head_state)\n        print(\n            f\"[MTSLSTM] Daily pretrained loaded: matched={matched}, \"\n            f\"shape_mismatch={shape_mismatch}, skipped={skipped}\"\n        )\n\n    def _load_linear_weights(self, state):\n        \"\"\"Load pretrained linear layer weights.\"\"\"\n        if \"linearIn.weight\" in state and \"linearIn.bias\" in state:\n            linear_weight = state[\"linearIn.weight\"]\n            linear_bias = state[\"linearIn.bias\"]\n\n            target_linear = self.linear2[1]\n            if (\n                target_linear.weight.shape == linear_weight.shape\n                and target_linear.bias.shape == linear_bias.shape\n            ):\n                target_linear.weight.data.copy_(linear_weight)\n                target_linear.bias.data.copy_(linear_bias)\n                print(\"[MTSLSTM] Pretrained linearIn loaded into linear2[1]\")\n            else:\n                warnings.warn(\n                    f\"[MTSLSTM] linearIn shape mismatch: pretrained {linear_weight.shape}, \"\n                    f\"current {target_linear.weight.shape}\"\n                )\n        else:\n            warnings.warn(\"[MTSLSTM] linearIn keys not found in pretrained state\")\n\n    def _print_loading_debug_info(self, state, day_lstm, day_head):\n        \"\"\"Print debug information about pretrained weight loading.\"\"\"\n        print(\"=== Pretrained keys ===\")\n        for k in list(state.keys())[:10]:\n            print(k)\n\n        print(\"\\n=== Current LSTM keys ===\")\n        for k in list(day_lstm.state_dict().keys())[:10]:\n            print(k)\n\n        print(\"\\n=== Current Head keys ===\")\n        for k in list(day_head.state_dict().keys())[:10]:\n            print(k)\n\n        print(\"Head.bias pretrained:\", state[\"linearOut.bias\"].shape)\n        print(\"Head.bias current:\", day_head.state_dict()[\"bias\"].shape)\n\n    def _append_one_hot(self, x: torch.Tensor, freq_idx: int) -&gt; torch.Tensor:\n        \"\"\"Appends a one-hot frequency indicator to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (T, B, D).\n            freq_idx (int): Frequency index to mark as 1 in the one-hot vector.\n\n        Returns:\n            torch.Tensor: Tensor of shape (T, B, D + nf), where `nf` is the\n            number of frequency branches. The appended one-hot encodes the\n            branch identity.\n        \"\"\"\n        T, B, _ = x.shape\n        oh = x.new_zeros((T, B, self.nf))\n        oh[:, :, freq_idx] = 1\n        return torch.cat([x, oh], dim=-1)\n\n    def _run_lstm(\n        self,\n        x: torch.Tensor,\n        lstm: nn.LSTM,\n        head: nn.Linear,\n        h0: Optional[torch.Tensor],\n        c0: Optional[torch.Tensor],\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Runs an LSTM followed by a linear head with optional initial states.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (T, B, D).\n            lstm (nn.LSTM): LSTM module for this branch.\n            head (nn.Linear): Linear output layer.\n            h0 (torch.Tensor): Optional initial hidden state (1, B, H).\n            c0 (torch.Tensor): Optional initial cell state (1, B, H).\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n                - y (torch.Tensor): Output sequence (T, B, O).\n                - h_n (torch.Tensor): Final hidden state (1, B, H).\n                - c_n (torch.Tensor): Final cell state (1, B, H).\n        \"\"\"\n        out, (h_n, c_n) = (\n            lstm(x, (h0, c0)) if (h0 is not None and c0 is not None) else lstm(x)\n        )\n        y = head(self.dropout(out))  # Project to output size\n        return y, h_n, c_n\n\n    def _aggregate_lowfreq(\n        self,\n        x_high: torch.Tensor,\n        factor: int,\n        agg_reduce: Literal[\"mean\", \"sum\"],\n        per_feature_aggs: Optional[List[Literal[\"mean\", \"sum\"]]],\n        truncate_incomplete: bool,\n    ) -&gt; torch.Tensor:\n        \"\"\"Aggregates high-frequency input into lower-frequency sequences.\n\n        Args:\n            x_high (torch.Tensor): Input tensor (T, B, D) at high frequency.\n            factor (int): Aggregation factor (e.g., 24 for daily from hourly).\n            agg_reduce (str): Default aggregation method, \"mean\" or \"sum\".\n            per_feature_aggs (List[str] | None):\n                Optional per-feature aggregation strategies (\"mean\"/\"sum\").\n            truncate_incomplete (bool): If True, drop incomplete groups;\n                if False, pad to make groups complete.\n\n        Returns:\n            torch.Tensor: Aggregated tensor of shape (T_low, B, D),\n            where T_low = floor(T / factor) if truncate_incomplete,\n            else ceil(T / factor).\n\n        Raises:\n            ValueError: If `agg_reduce` is not \"mean\" or \"sum\".\n            AssertionError: If `per_feature_aggs` length mismatches feature dim.\n        \"\"\"\n        T, B, D = x_high.shape\n        if factor &lt;= 1:\n            return x_high\n        if truncate_incomplete:\n            T_trim = (T // factor) * factor\n            xh = x_high[:T_trim]\n            groups = xh.view(T_trim // factor, factor, B, D)\n        else:\n            pad = (factor - (T % factor)) % factor\n            if pad &gt; 0:\n                pad_tensor = x_high.new_zeros((pad, B, D))\n                xh = torch.cat([x_high, pad_tensor], dim=0)\n            else:\n                xh = x_high\n            groups = xh.view(xh.shape[0] // factor, factor, B, D)\n\n        if per_feature_aggs is None:\n            if agg_reduce == \"mean\":\n                return groups.mean(dim=1)\n            elif agg_reduce == \"sum\":\n                return groups.sum(dim=1)\n            else:\n                raise ValueError(\"agg_reduce must be 'mean' or 'sum'\")\n\n        assert len(per_feature_aggs) == D, \"per_feature_aggs length must match D\"\n        mean_agg = groups.mean(dim=1)\n        sum_agg = groups.sum(dim=1)\n        mask_sum = x_high.new_tensor(\n            [1.0 if a == \"sum\" else 0.0 for a in per_feature_aggs]\n        ).view(1, 1, D)\n        mask_mean = 1.0 - mask_sum\n        return mean_agg * mask_mean + sum_agg * mask_sum\n\n    def _multi_factor_to_high(self, f: int) -&gt; int:\n        \"\"\"Computes cumulative factor from branch f to the highest frequency.\n\n        Args:\n            f (int): Branch index (0 = lowest frequency).\n\n        Returns:\n            int: Product of frequency factors from branch f to highest branch.\n\n        Raises:\n            RuntimeError: If `frequency_factors` is not defined.\n        \"\"\"\n        if self.frequency_factors is None:\n            raise RuntimeError(\"frequency_factors must be provided.\")\n        fac = 1\n        for k in range(f, self.nf - 1):\n            fac *= int(self.frequency_factors[k])\n        return fac\n\n    def _build_from_hourly(self, x_hour: torch.Tensor) -&gt; List[torch.Tensor]:\n        \"\"\"Builds multi-frequency inputs from raw hourly features.\n\n        Each branch selects features based on bucket assignment and aggregates\n        them down to its frequency using frequency factors.\n\n        Args:\n            x_hour (torch.Tensor): Hourly input tensor (T_h, B, D).\n                - T_h: number of hourly timesteps\n                - B: batch size\n                - D: feature dimension (must match `feature_buckets`)\n\n        Returns:\n            List[torch.Tensor]: List of tensors, one per branch,\n            with shapes (T_f, B, D_f).\n\n        Raises:\n            AssertionError: If feature dimension mismatches `feature_buckets`.\n        \"\"\"\n        assert self.feature_buckets is not None, \"feature_buckets must be provided\"\n        T_h, B, D = x_hour.shape\n        assert D == len(self.feature_buckets), \"Mismatch between features and buckets\"\n\n        xs = []\n        for f in range(self.nf):\n            if self.down_agg_all:\n                cols = [i for i in range(D) if self.feature_buckets[i] &gt;= f]\n            else:\n                cols = [i for i in range(D) if self.feature_buckets[i] == f]\n\n            if len(cols) == 0:\n                # Insert placeholder if branch has no features\n                x_sub = x_hour.new_zeros((T_h, B, 1))\n                per_aggs = [\"mean\"]\n            else:\n                x_sub = x_hour[:, :, cols]\n                per_aggs = None\n                if self.per_feature_aggs_map is not None:\n                    per_aggs = [self.per_feature_aggs_map[i] for i in cols]\n\n            factor = self._multi_factor_to_high(f) if (self.nf &gt;= 2) else 1\n            x_f = self._aggregate_lowfreq(\n                x_high=x_sub,\n                factor=factor,\n                agg_reduce=self.agg_reduce,\n                per_feature_aggs=per_aggs,\n                truncate_incomplete=self.truncate_incomplete,\n            )\n            xs.append(x_f)\n        return xs\n\n    def _get_slice_len_low(self, i: int, T_low: int, T_high: int) -&gt; int:\n        \"\"\"Computes the slice length for a low-frequency branch.\n\n        This method determines how many timesteps from the low-frequency input\n        should be aligned with the high-frequency branch during state transfer.\n\n        Priority:\n            1. If `slice_timesteps` is precomputed, return the clamped value.\n            2. Otherwise, fall back to heuristic estimation using ceil/floor.\n\n        Args:\n            i (int): Index of the low-frequency branch.\n            T_low (int): Sequence length of the low-frequency input.\n            T_high (int): Sequence length of the high-frequency input.\n\n        Returns:\n            int: Number of timesteps to slice from the low-frequency input,\n            clamped between [0, T_low].\n\n        Raises:\n            RuntimeWarning: If `seq_lengths` and `frequency_factors` are not\n            provided, a warning is issued since the fallback may not perfectly\n            match NeuralHydrology's fixed slicing definition.\n        \"\"\"\n        if self.slice_timesteps is not None:\n            return max(0, min(self.slice_timesteps[i], T_low))\n        if (not self._warned_slice_fallback) and self.slice_transfer:\n            warnings.warn(\n                \"[MTSLSTM] \u672a\u63d0\u4f9b seq_lengths/frequency_factors\uff0c\u5207\u7247\u4f4d\u7f6e\u91c7\u7528\u542f\u53d1\u5f0f\u4f30\u8ba1\uff08ceil/floor\uff09\uff0c\"\n                \"\u4e0e NeuralHydrology \u7684\u56fa\u5b9a\u5207\u7247\u5b9a\u4e49\u4e0d\u5b8c\u5168\u4e00\u81f4\u3002\u5efa\u8bae\u63d0\u4f9b\u8fd9\u4e24\u4e2a\u8d85\u53c2\u4ee5\u5b8c\u5168\u5bf9\u9f50\u3002\",\n                RuntimeWarning,\n            )\n            self._warned_slice_fallback = True\n        factor = (\n            self.build_factor\n            if (\n                self.nf == 2\n                and self.feature_buckets is None\n                and self.auto_build_lowfreq\n            )\n            else max(int(round(T_high / max(1, T_low))), 1)\n        )\n        if self.slice_use_ceil:\n            slice_len_low = int((T_high + factor - 1) // factor)\n        else:\n            slice_len_low = int(T_high // factor)\n        return max(0, min(slice_len_low, T_low))\n\n    def _prepare_inputs(self, xs: Tuple[torch.Tensor, ...]) -&gt; Tuple[torch.Tensor, ...]:\n        \"\"\"Prepare and validate input tensors for multi-frequency processing.\n\n        Args:\n            xs: Input tensors from forward method\n\n        Returns:\n            Tuple of validated input tensors for each frequency branch\n        \"\"\"\n        # \u65b0\u8def\u5f84\uff1a\u5355\u4e00\u5c0f\u65f6\u8f93\u5165 + feature_buckets\n        if len(xs) == 1 and self.use_hourly_unified:\n            return tuple(self._build_from_hourly(xs[0]))\n\n        # \u65e7\u8def\u5f84\uff1a\u5355\u4e00\u9ad8\u9891\u8f93\u5165 + \u81ea\u52a8\u6784\u9020\u4f4e\u9891\uff08\u4ec5\u4e24\u9891\uff09\n        elif (\n            len(xs) == 1\n            and self.auto_build_lowfreq\n            and self.nf == 2\n            and self.feature_buckets is None\n        ):\n            x_high = xs[0]\n            assert x_high.dim() == 3, \"\u8f93\u5165\u5fc5\u987b\u662f (time,batch,features)\"\n            x_low = self._aggregate_lowfreq(\n                x_high=x_high,\n                factor=self.build_factor,\n                agg_reduce=self.agg_reduce,\n                per_feature_aggs=self.per_feature_aggs,\n                truncate_incomplete=self.truncate_incomplete,\n            )\n            return (x_low, x_high)\n\n        return xs\n\n    def _validate_inputs(self, xs: Tuple[torch.Tensor, ...]) -&gt; None:\n        \"\"\"Validate input tensor dimensions and shapes.\n\n        Args:\n            xs: Input tensors to validate\n\n        Raises:\n            AssertionError: If inputs don't match expected configuration\n        \"\"\"\n        assert len(xs) == self.nf, f\"\u6536\u5230 {len(xs)} \u4e2a\u9891\u7387\uff0c\u4f46\u6a21\u578b\u671f\u671b {self.nf}\"\n        for i, x in enumerate(xs):\n            assert x.dim() == 3, f\"\u7b2c {i} \u4e2a\u8f93\u5165\u5fc5\u987b\u662f (time,batch,features)\"\n            exp_d = self.base_input_sizes[i]\n            assert (\n                x.shape[-1] == exp_d\n            ), f\"\u7b2c {i} \u4e2a\u8f93\u5165\u7279\u5f81\u7ef4 {x.shape[-1]} \u4e0e\u671f\u671b {exp_d} \u4e0d\u4e00\u81f4\"\n\n    def _preprocess_branch_input(self, x: torch.Tensor, branch_idx: int) -&gt; torch.Tensor:\n        \"\"\"Preprocess input for a specific frequency branch.\n\n        Args:\n            x: Input tensor for the branch\n            branch_idx: Index of the frequency branch\n\n        Returns:\n            Preprocessed tensor ready for LSTM processing\n        \"\"\"\n        # Add frequency one-hot encoding first if needed\n        if self.add_freq1hot:\n            x = self._append_one_hot(x, branch_idx)\n\n        if self.pretrained_flag:\n            x = self.linear1[branch_idx](x)\n            x = F.relu(x)\n            x = self.linear2[branch_idx](x)\n            x = F.relu(x)\n        else:\n            # For non-pretrained mode, apply input linear layer\n            x = self.input_linears[branch_idx](x)\n            x = F.relu(x)\n\n        return x\n\n    def _get_branch_modules(self, branch_idx: int) -&gt; Tuple[nn.LSTM, nn.Linear]:\n        \"\"\"Get LSTM and head modules for a specific branch.\n\n        Args:\n            branch_idx: Index of the frequency branch\n\n        Returns:\n            Tuple of (lstm_module, head_module) for the branch\n        \"\"\"\n        lstm = self.lstms[0] if self.shared else self.lstms[branch_idx]\n        head = self.heads[0] if self.shared else self.heads[branch_idx]\n        return lstm, head\n\n    def _initialize_transfer_states(self, device: torch.device, batch_size: int) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Initialize hidden and cell states for transfer between branches.\n\n        Args:\n            device: Device to create tensors on\n            batch_size: Batch size for state tensors\n\n        Returns:\n            Tuple of (h_transfer, c_transfer) initial states\n        \"\"\"\n        H0 = self.hidden_sizes[0]\n        h_transfer = torch.zeros(1, batch_size, H0, device=device)\n        c_transfer = torch.zeros(1, batch_size, H0, device=device)\n        return h_transfer, c_transfer\n\n    def _update_transfer_states(\n        self, \n        branch_idx: int, \n        h_state: torch.Tensor, \n        c_state: torch.Tensor,\n        next_batch_size: int,\n        device: torch.device\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Update transfer states for the next branch.\n\n        Args:\n            branch_idx: Current branch index\n            h_state: Current hidden state\n            c_state: Current cell state  \n            next_batch_size: Batch size for next branch\n            device: Device for tensor creation\n\n        Returns:\n            Updated (h_transfer, c_transfer) for next branch\n        \"\"\"\n        if branch_idx &gt;= self.nf - 1:\n            return h_state, c_state\n\n        Hn = self.hidden_sizes[branch_idx + 1]\n        h_transfer = torch.zeros(1, next_batch_size, Hn, device=device)\n        c_transfer = torch.zeros(1, next_batch_size, Hn, device=device)\n\n        if self.transfer_h[branch_idx] is not None:\n            h_transfer = self.transfer_h[branch_idx](h_state[0]).unsqueeze(0)\n        if self.transfer_c[branch_idx] is not None:\n            c_transfer = self.transfer_c[branch_idx](c_state[0]).unsqueeze(0)\n\n        return h_transfer, c_transfer\n\n    def _process_branch_with_slice_transfer(\n        self,\n        x_i: torch.Tensor,\n        branch_idx: int,\n        xs: Tuple[torch.Tensor, ...],\n        lstm: nn.LSTM,\n        head: nn.Linear,\n        h_transfer: torch.Tensor,\n        c_transfer: torch.Tensor,\n        device: torch.device\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Process a branch with slice transfer enabled.\n\n        Args:\n            x_i: Input tensor for current branch\n            branch_idx: Index of current branch\n            xs: All input tensors\n            lstm: LSTM module for current branch\n            head: Head module for current branch\n            h_transfer: Transfer hidden state\n            c_transfer: Transfer cell state\n            device: Device for tensor operations\n\n        Returns:\n            Tuple of (output, final_h_state, final_c_state)\n        \"\"\"\n        T_low = x_i.shape[0]\n        T_high = xs[branch_idx + 1].shape[0]\n        slice_len_low = self._get_slice_len_low(branch_idx, T_low=T_low, T_high=T_high)\n\n        if slice_len_low == 0:\n            return self._run_lstm(x_i, lstm, head, h_transfer, c_transfer)\n        else:\n            # Process first part\n            x_part1 = (\n                x_i[:-slice_len_low] if slice_len_low &lt; x_i.shape[0] else x_i[:0]\n            )\n            if x_part1.shape[0] &gt; 0:\n                y1, h1, c1 = self._run_lstm(x_part1, lstm, head, h_transfer, c_transfer)\n            else:\n                y1 = x_i.new_zeros((0, x_i.shape[1], self.output_size))\n                h1, c1 = h_transfer, c_transfer\n\n            # Process second part\n            x_part2 = x_i[-slice_len_low:] if slice_len_low &gt; 0 else x_i[:0]\n            if x_part2.shape[0] &gt; 0:\n                y2, h_final, c_final = self._run_lstm(x_part2, lstm, head, h1, c1)\n                y_all = torch.cat([y1, y2], dim=0)\n            else:\n                y_all = y1\n                h_final, c_final = h1, c1\n\n            return y_all, h_final, c_final\n\n    def _process_single_branch(\n        self,\n        branch_idx: int,\n        xs: Tuple[torch.Tensor, ...],\n        h_transfer: torch.Tensor,\n        c_transfer: torch.Tensor,\n        device: torch.device\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Process a single frequency branch.\n\n        Args:\n            branch_idx: Index of the branch to process\n            xs: All input tensors\n            h_transfer: Transfer hidden state\n            c_transfer: Transfer cell state\n            device: Device for tensor operations\n\n        Returns:\n            Tuple of (branch_output, final_h_state, final_c_state)\n        \"\"\"\n        # Preprocess input\n        x_i = self._preprocess_branch_input(xs[branch_idx], branch_idx)\n\n        # Get branch modules\n        lstm_i, head_i = self._get_branch_modules(branch_idx)\n\n        # Process with or without slice transfer\n        if (branch_idx &lt; self.nf - 1) and self.slice_transfer:\n            return self._process_branch_with_slice_transfer(\n                x_i, branch_idx, xs, lstm_i, head_i, h_transfer, c_transfer, device\n            )\n        else:\n            return self._run_lstm(x_i, lstm_i, head_i, h_transfer, c_transfer)\n\n    def forward(\n        self, *xs: torch.Tensor, return_all: Optional[bool] = None, **kwargs: Any\n    ) -&gt; Union[Dict[str, torch.Tensor], torch.Tensor]:\n        \"\"\"Forward pass of the Multi-Time-Scale LSTM (MTSLSTM).\n\n        This method supports three types of input pipelines:\n\n        1. New unified hourly input (recommended):\n            - Pass a single hourly tensor (T, B, D).\n            - Requires `feature_buckets` set in the constructor.\n            - Internally calls `_build_from_hourly()` to build multi-scale inputs.\n\n        2. Legacy path A (two-frequency auto build):\n            - Pass a single high-frequency tensor (daily).\n            - Requires `auto_build_lowfreq=True` and `nf=2`.\n            - Automatically constructs the low-frequency branch.\n\n        3. Legacy path B (manual multi-frequency input):\n            - Pass a tuple of tensors: (x_f0, x_f1, ..., x_f{nf-1}).\n\n        Args:\n            *xs (torch.Tensor): Input tensors. Can be:\n                - One hourly tensor of shape (T, B, D).\n                - One daily tensor (legacy auto-build).\n                - A tuple of nf tensors, each shaped (T_f, B, D_f).\n            return_all (Optional[bool], default=None): Whether to return outputs\n                from all frequency branches. If None, uses the class default.\n            **kwargs: Additional unused keyword arguments.\n\n        Returns:\n            Dict[str, torch.Tensor] | torch.Tensor:\n                - If `return_all=True`: Dictionary mapping branch names to outputs,\n                  e.g. {\"f0\": y_low, \"f1\": y_mid, \"f2\": y_high}.\n                - If `return_all=False`: Only returns the highest-frequency output\n                  tensor of shape (T_high, B, output_size).\n\n        Raises:\n            AssertionError: If the number of provided inputs does not match `nf`,\n                or if input feature dimensions do not match the expected\n                configuration.\n        \"\"\"\n        if return_all is None:\n            return_all = self.return_all_default\n\n        # Prepare and validate inputs\n        xs = self._prepare_inputs(xs)\n        self._validate_inputs(xs)\n\n        # Initialize processing state\n        device = xs[0].device\n        batch_size = xs[0].shape[1]\n        h_transfer, c_transfer = self._initialize_transfer_states(device, batch_size)\n\n        outputs: Dict[str, torch.Tensor] = {}\n\n        # Process each frequency branch\n        for i in range(self.nf):\n            y_i, h_i, c_i = self._process_single_branch(i, xs, h_transfer, c_transfer, device)\n            outputs[f\"f{i}\"] = y_i\n\n            # Update transfer states for next branch\n            if i &lt; self.nf - 1:\n                next_batch_size = xs[i + 1].shape[1]\n                h_transfer, c_transfer = self._update_transfer_states(\n                    i, h_i, c_i, next_batch_size, device\n                )\n\n        return outputs if return_all else outputs[f\"f{self.nf - 1}\"]\n</code></pre>"},{"location":"api/models/#torchhydro.models.mtslstm.MTSLSTM.__init__","title":"<code>__init__(self, input_sizes=None, hidden_sizes=128, output_size=1, shared_mtslstm=False, transfer='linear', dropout=0.0, return_all=False, add_freq_one_hot_if_shared=True, auto_build_lowfreq=False, build_factor=7, agg_reduce='mean', per_feature_aggs=None, truncate_incomplete=True, slice_transfer=True, slice_use_ceil=True, seq_lengths=None, frequency_factors=None, feature_buckets=None, per_feature_aggs_map=None, down_aggregate_all_to_each_branch=True, pretrained_day_path=None, pretrained_lstm_prefix=None, pretrained_head_prefix=None, pretrained_flag=False, linear1_size=None, linear2_size=None)</code>  <code>special</code>","text":"<p>Initializes an MTSLSTM model.</p> <p>Parameters:</p> Name Type Description Default <code>input_sizes</code> <code>Union[int, List[int]]</code> <p>Input feature dimension(s). Can be: * int: shared across all frequency branches * list: per-frequency input sizes * None: inferred from <code>feature_buckets</code></p> <code>None</code> <code>hidden_sizes</code> <code>Union[int, List[int]]</code> <p>Hidden dimension(s) for each LSTM branch.</p> <code>128</code> <code>output_size</code> <code>int</code> <p>Output dimension per timestep.</p> <code>1</code> <code>shared_mtslstm</code> <code>bool</code> <p>If True, all frequency branches share one LSTM.</p> <code>False</code> <code>transfer</code> <code>Union[NoneType, str, Dict[str, Optional[Literal['identity', 'linear']]]]</code> <p>Hidden state transfer mode between frequencies. * None: no transfer * \"identity\": copy states directly (same dim required) * \"linear\": learn linear projection between dims</p> <code>'linear'</code> <code>dropout</code> <code>float</code> <p>Dropout probability applied before heads.</p> <code>0.0</code> <code>return_all</code> <code>bool</code> <p>If True, return all branch outputs (dict f0,f1,...). If False, return only the highest-frequency output.</p> <code>False</code> <code>add_freq_one_hot_if_shared</code> <code>bool</code> <p>If True and <code>shared_mtslstm=True</code>, append frequency one-hot encoding to inputs.</p> <code>True</code> <code>auto_build_lowfreq</code> <code>bool</code> <p>Legacy 2-frequency path (high-&gt;low).</p> <code>False</code> <code>build_factor</code> <code>int</code> <p>Aggregation factor for auto low-frequency.</p> <code>7</code> <code>agg_reduce</code> <code>Literal['mean', 'sum']</code> <p>Aggregation method for downsampling (\"mean\" or \"sum\").</p> <code>'mean'</code> <code>per_feature_aggs</code> <code>Optional[List[Literal['mean', 'sum']]]</code> <p>Optional list of per-feature aggregation methods.</p> <code>None</code> <code>truncate_incomplete</code> <code>bool</code> <p>Whether to drop remainder timesteps when aggregating (vs. zero-padding).</p> <code>True</code> <code>slice_transfer</code> <code>bool</code> <p>If True, transfer LSTM states at slice boundaries computed by seq_lengths \u00d7 frequency_factors.</p> <code>True</code> <code>slice_use_ceil</code> <code>bool</code> <p>If True, use ceil for slice length calculation.</p> <code>True</code> <code>seq_lengths</code> <code>Optional[List[int]]</code> <p>Per-frequency sequence lengths [low, ..., high].</p> <code>None</code> <code>frequency_factors</code> <code>Optional[List[int]]</code> <p>Multipliers between adjacent frequencies. Example: [7,24] means week-&gt;day \u00d77, day-&gt;hour \u00d724.</p> <code>None</code> <code>feature_buckets</code> <code>Optional[List[int]]</code> <p>Per-feature frequency assignment (len = D). 0 = lowest (week), nf-1 = highest (hour).</p> <code>None</code> <code>per_feature_aggs_map</code> <code>Optional[List[Literal['mean', 'sum']]]</code> <p>Per-feature aggregation method (\"mean\"/\"sum\").</p> <code>None</code> <code>down_aggregate_all_to_each_branch</code> <code>bool</code> <p>If True, branch f includes all features with bucket &gt;= f (down-aggregate); else only == f.</p> <code>True</code> <code>pretrained_day_path</code> <code>Optional[str]</code> <p>Optional path to pretrained checkpoint. If set, loads weights for the daily (f1) LSTM and head.</p> <code>None</code> <p>Exceptions:</p> Type Description <code>AssertionError</code> <p>If configuration is inconsistent.</p> Source code in <code>torchhydro/models/mtslstm.py</code> <pre><code>def __init__(\n    self,\n    input_sizes: Union[int, List[int], None] = None,\n    hidden_sizes: Union[int, List[int]] = 128,\n    output_size: int = 1,\n    shared_mtslstm: bool = False,\n    transfer: Union[\n        None, str, Dict[str, Optional[Literal[\"identity\", \"linear\"]]]\n    ] = \"linear\",\n    dropout: float = 0.0,\n    return_all: bool = False,\n    add_freq_one_hot_if_shared: bool = True,\n    auto_build_lowfreq: bool = False,\n    build_factor: int = 7,\n    agg_reduce: Literal[\"mean\", \"sum\"] = \"mean\",\n    per_feature_aggs: Optional[List[Literal[\"mean\", \"sum\"]]] = None,\n    truncate_incomplete: bool = True,\n    slice_transfer: bool = True,\n    slice_use_ceil: bool = True,\n    seq_lengths: Optional[List[int]] = None,\n    frequency_factors: Optional[List[int]] = None,\n    feature_buckets: Optional[List[int]] = None,\n    per_feature_aggs_map: Optional[List[Literal[\"mean\", \"sum\"]]] = None,\n    down_aggregate_all_to_each_branch: bool = True,\n    pretrained_day_path: Optional[str] = None,\n    pretrained_lstm_prefix: Optional[str] = None,\n    pretrained_head_prefix: Optional[str] = None,\n    pretrained_flag: bool = False,\n    linear1_size: Optional[int] = None,\n    linear2_size: Optional[int] = None\n):\n    \"\"\"Initializes an MTSLSTM model.\n\n    Args:\n        input_sizes: Input feature dimension(s). Can be:\n            * int: shared across all frequency branches\n            * list: per-frequency input sizes\n            * None: inferred from `feature_buckets`\n        hidden_sizes: Hidden dimension(s) for each LSTM branch.\n        output_size: Output dimension per timestep.\n        shared_mtslstm: If True, all frequency branches share one LSTM.\n        transfer: Hidden state transfer mode between frequencies.\n            * None: no transfer\n            * \"identity\": copy states directly (same dim required)\n            * \"linear\": learn linear projection between dims\n        dropout: Dropout probability applied before heads.\n        return_all: If True, return all branch outputs (dict f0,f1,...).\n            If False, return only the highest-frequency output.\n        add_freq_one_hot_if_shared: If True and `shared_mtslstm=True`,\n            append frequency one-hot encoding to inputs.\n        auto_build_lowfreq: Legacy 2-frequency path (high-&gt;low).\n        build_factor: Aggregation factor for auto low-frequency.\n        agg_reduce: Aggregation method for downsampling (\"mean\" or \"sum\").\n        per_feature_aggs: Optional list of per-feature aggregation methods.\n        truncate_incomplete: Whether to drop remainder timesteps when\n            aggregating (vs. zero-padding).\n        slice_transfer: If True, transfer LSTM states at slice boundaries\n            computed by seq_lengths \u00d7 frequency_factors.\n        slice_use_ceil: If True, use ceil for slice length calculation.\n        seq_lengths: Per-frequency sequence lengths [low, ..., high].\n        frequency_factors: Multipliers between adjacent frequencies.\n            Example: [7,24] means week-&gt;day \u00d77, day-&gt;hour \u00d724.\n        feature_buckets: Per-feature frequency assignment (len = D).\n            0 = lowest (week), nf-1 = highest (hour).\n        per_feature_aggs_map: Per-feature aggregation method (\"mean\"/\"sum\").\n        down_aggregate_all_to_each_branch: If True, branch f includes all\n            features with bucket &gt;= f (down-aggregate); else only == f.\n        pretrained_day_path: Optional path to pretrained checkpoint. If set,\n            loads weights for the daily (f1) LSTM and head.\n\n    Raises:\n        AssertionError: If configuration is inconsistent.\n    \"\"\"\n    super().__init__()\n\n    # Store configuration parameters\n    self.pretrained_day_path = pretrained_day_path\n    self.pretrained_flag = pretrained_flag\n    self.linear1_size = linear1_size\n    self.linear2_size = linear2_size\n    self.output_size = output_size\n    self.shared = shared_mtslstm\n    self.return_all_default = return_all\n    self.feature_buckets = list(feature_buckets) if feature_buckets is not None else None\n    self.per_feature_aggs_map = list(per_feature_aggs_map) if per_feature_aggs_map is not None else None\n    self.down_agg_all = down_aggregate_all_to_each_branch\n    self.auto_build_lowfreq = auto_build_lowfreq\n\n    # Aggregation and slicing parameters\n    assert build_factor &gt;= 2, \"build_factor must be &gt;=2\"\n    self.build_factor = int(build_factor)\n    self.agg_reduce = agg_reduce\n    self.per_feature_aggs = per_feature_aggs\n    self.truncate_incomplete = truncate_incomplete\n    self.slice_transfer = slice_transfer\n    self.slice_use_ceil = slice_use_ceil\n    self.seq_lengths = list(seq_lengths) if seq_lengths is not None else None\n    self._warned_slice_fallback = False\n\n    # Setup frequency configuration\n    self._setup_frequency_config(feature_buckets, input_sizes, auto_build_lowfreq)\n\n    # Validate seq_lengths\n    if self.seq_lengths is not None:\n        assert len(self.seq_lengths) == self.nf, \"seq_lengths length must match nf\"\n\n    # Setup input sizes for each branch\n    self.base_input_sizes = self._setup_input_sizes(\n        self.feature_buckets, input_sizes, self.down_agg_all\n    )\n\n    # Setup hidden layer sizes\n    if isinstance(hidden_sizes, int):\n        self.hidden_sizes = [hidden_sizes] * self.nf\n    else:\n        assert len(hidden_sizes) == self.nf, \"hidden_sizes length mismatch\"\n        self.hidden_sizes = list(hidden_sizes)\n\n    # Setup frequency one-hot encoding\n    self.add_freq1hot = add_freq_one_hot_if_shared and self.shared\n\n    # Setup transfer configuration\n    self._setup_transfer_config(transfer)\n\n    # Setup frequency factors and slice timesteps\n    self._setup_frequency_factors(frequency_factors, auto_build_lowfreq, self.build_factor)\n\n    # Calculate effective input sizes (including one-hot if needed)\n    eff_input_sizes = self.base_input_sizes[:]\n    if self.add_freq1hot:\n        eff_input_sizes = [d + self.nf for d in eff_input_sizes]\n\n    if self.shared and len(set(eff_input_sizes)) != 1:\n        raise ValueError(\"shared_mtslstm=True requires equal input sizes.\")\n\n    # Create model layers\n    self._create_model_layers(eff_input_sizes)\n\n    # Create transfer layers\n    self._create_transfer_layers()\n\n    # Setup dropout\n    self.dropout = nn.Dropout(p=dropout)\n\n    # Set unified hourly aggregation flag\n    self.use_hourly_unified = self.feature_buckets is not None\n\n    # Load pretrained weights if specified\n    self._load_pretrained_weights(pretrained_lstm_prefix, pretrained_head_prefix)\n</code></pre>"},{"location":"api/models/#torchhydro.models.mtslstm.MTSLSTM.forward","title":"<code>forward(self, *xs, *, return_all=None, **kwargs)</code>","text":"<p>Forward pass of the Multi-Time-Scale LSTM (MTSLSTM).</p> <p>This method supports three types of input pipelines:</p> <ol> <li> <p>New unified hourly input (recommended):</p> <ul> <li>Pass a single hourly tensor (T, B, D).</li> <li>Requires <code>feature_buckets</code> set in the constructor.</li> <li>Internally calls <code>_build_from_hourly()</code> to build multi-scale inputs.</li> </ul> </li> <li> <p>Legacy path A (two-frequency auto build):</p> <ul> <li>Pass a single high-frequency tensor (daily).</li> <li>Requires <code>auto_build_lowfreq=True</code> and <code>nf=2</code>.</li> <li>Automatically constructs the low-frequency branch.</li> </ul> </li> <li> <p>Legacy path B (manual multi-frequency input):</p> <ul> <li>Pass a tuple of tensors: (x_f0, x_f1, ..., x_f{nf-1}).</li> </ul> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>*xs</code> <code>torch.Tensor</code> <p>Input tensors. Can be: - One hourly tensor of shape (T, B, D). - One daily tensor (legacy auto-build). - A tuple of nf tensors, each shaped (T_f, B, D_f).</p> <code>()</code> <code>return_all</code> <code>Optional[bool], default=None</code> <p>Whether to return outputs from all frequency branches. If None, uses the class default.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional unused keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, torch.Tensor] | torch.Tensor</code> <ul> <li>If <code>return_all=True</code>: Dictionary mapping branch names to outputs,       e.g. {\"f0\": y_low, \"f1\": y_mid, \"f2\": y_high}.<ul> <li>If <code>return_all=False</code>: Only returns the highest-frequency output   tensor of shape (T_high, B, output_size).</li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>AssertionError</code> <p>If the number of provided inputs does not match <code>nf</code>, or if input feature dimensions do not match the expected configuration.</p> Source code in <code>torchhydro/models/mtslstm.py</code> <pre><code>def forward(\n    self, *xs: torch.Tensor, return_all: Optional[bool] = None, **kwargs: Any\n) -&gt; Union[Dict[str, torch.Tensor], torch.Tensor]:\n    \"\"\"Forward pass of the Multi-Time-Scale LSTM (MTSLSTM).\n\n    This method supports three types of input pipelines:\n\n    1. New unified hourly input (recommended):\n        - Pass a single hourly tensor (T, B, D).\n        - Requires `feature_buckets` set in the constructor.\n        - Internally calls `_build_from_hourly()` to build multi-scale inputs.\n\n    2. Legacy path A (two-frequency auto build):\n        - Pass a single high-frequency tensor (daily).\n        - Requires `auto_build_lowfreq=True` and `nf=2`.\n        - Automatically constructs the low-frequency branch.\n\n    3. Legacy path B (manual multi-frequency input):\n        - Pass a tuple of tensors: (x_f0, x_f1, ..., x_f{nf-1}).\n\n    Args:\n        *xs (torch.Tensor): Input tensors. Can be:\n            - One hourly tensor of shape (T, B, D).\n            - One daily tensor (legacy auto-build).\n            - A tuple of nf tensors, each shaped (T_f, B, D_f).\n        return_all (Optional[bool], default=None): Whether to return outputs\n            from all frequency branches. If None, uses the class default.\n        **kwargs: Additional unused keyword arguments.\n\n    Returns:\n        Dict[str, torch.Tensor] | torch.Tensor:\n            - If `return_all=True`: Dictionary mapping branch names to outputs,\n              e.g. {\"f0\": y_low, \"f1\": y_mid, \"f2\": y_high}.\n            - If `return_all=False`: Only returns the highest-frequency output\n              tensor of shape (T_high, B, output_size).\n\n    Raises:\n        AssertionError: If the number of provided inputs does not match `nf`,\n            or if input feature dimensions do not match the expected\n            configuration.\n    \"\"\"\n    if return_all is None:\n        return_all = self.return_all_default\n\n    # Prepare and validate inputs\n    xs = self._prepare_inputs(xs)\n    self._validate_inputs(xs)\n\n    # Initialize processing state\n    device = xs[0].device\n    batch_size = xs[0].shape[1]\n    h_transfer, c_transfer = self._initialize_transfer_states(device, batch_size)\n\n    outputs: Dict[str, torch.Tensor] = {}\n\n    # Process each frequency branch\n    for i in range(self.nf):\n        y_i, h_i, c_i = self._process_single_branch(i, xs, h_transfer, c_transfer, device)\n        outputs[f\"f{i}\"] = y_i\n\n        # Update transfer states for next branch\n        if i &lt; self.nf - 1:\n            next_batch_size = xs[i + 1].shape[1]\n            h_transfer, c_transfer = self._update_transfer_states(\n                i, h_i, c_i, next_batch_size, device\n            )\n\n    return outputs if return_all else outputs[f\"f{self.nf - 1}\"]\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq","title":"<code>seq2seq</code>","text":"<p>Author: Wenyu Ouyang Date: 2024-04-17 12:32:26 LastEditTime: 2024-11-05 19:10:02 LastEditors: Wenyu Ouyang Description: FilePath:       orchhydro       orchhydro\\models\\seq2seq.py Copyright (c) 2021-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/models/#torchhydro.models.seq2seq.AdditiveAttention","title":"<code> AdditiveAttention            (Module)         </code>","text":"Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>class AdditiveAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(AdditiveAttention, self).__init__()\n        self.W_q = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.W_k = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.v = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, encoder_outputs, hidden):\n        seq_len = encoder_outputs.shape[1]\n        hidden_transformed = self.W_q(hidden).repeat(seq_len, 1, 1).transpose(0, 1)\n        encoder_outputs_transformed = self.W_k(encoder_outputs)\n        combined = torch.tanh(hidden_transformed + encoder_outputs_transformed)\n        scores = self.v(combined).squeeze(2)\n        return F.softmax(scores, dim=1)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.AdditiveAttention.forward","title":"<code>forward(self, encoder_outputs, hidden)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>def forward(self, encoder_outputs, hidden):\n    seq_len = encoder_outputs.shape[1]\n    hidden_transformed = self.W_q(hidden).repeat(seq_len, 1, 1).transpose(0, 1)\n    encoder_outputs_transformed = self.W_k(encoder_outputs)\n    combined = torch.tanh(hidden_transformed + encoder_outputs_transformed)\n    scores = self.v(combined).squeeze(2)\n    return F.softmax(scores, dim=1)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.Attention","title":"<code> Attention            (Module)         </code>","text":"Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>class Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(Attention, self).__init__()\n        self.attn = nn.Linear(hidden_dim * 2, 1, bias=False)\n\n    def forward(self, encoder_outputs, hidden):\n        seq_len = encoder_outputs.shape[1]\n        hidden = hidden.repeat(seq_len, 1, 1).transpose(0, 1)\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n        return F.softmax(energy.squeeze(2), dim=1)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.Attention.forward","title":"<code>forward(self, encoder_outputs, hidden)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>def forward(self, encoder_outputs, hidden):\n    seq_len = encoder_outputs.shape[1]\n    hidden = hidden.repeat(seq_len, 1, 1).transpose(0, 1)\n    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n    return F.softmax(energy.squeeze(2), dim=1)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.DataEnhancedModel","title":"<code> DataEnhancedModel            (GeneralSeq2Seq)         </code>","text":"Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>class DataEnhancedModel(GeneralSeq2Seq):\n    def __init__(self, hidden_length, **kwargs):\n        super(DataEnhancedModel, self).__init__(**kwargs)\n        self.lstm = nn.LSTM(1, hidden_length, num_layers=1, batch_first=True)\n        self.fc = nn.Linear(hidden_length, 6)\n\n    def forward(self, *src):\n        src1, src2, token = src\n        processed_src1 = torch.unsqueeze(src1[:, :, 0], dim=2)\n        out_src1, _ = self.lstm(processed_src1)\n        out_src1 = self.fc(out_src1)\n        combined_input = torch.cat((out_src1, src1[:, :, 1:]), dim=2)\n        return super(DataEnhancedModel, self).forward(combined_input, src2, token)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.DataEnhancedModel.forward","title":"<code>forward(self, *src)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>def forward(self, *src):\n    src1, src2, token = src\n    processed_src1 = torch.unsqueeze(src1[:, :, 0], dim=2)\n    out_src1, _ = self.lstm(processed_src1)\n    out_src1 = self.fc(out_src1)\n    combined_input = torch.cat((out_src1, src1[:, :, 1:]), dim=2)\n    return super(DataEnhancedModel, self).forward(combined_input, src2, token)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.DataFusionModel","title":"<code> DataFusionModel            (DataEnhancedModel)         </code>","text":"Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>class DataFusionModel(DataEnhancedModel):\n    def __init__(self, input_dim, **kwargs):\n        super(DataFusionModel, self).__init__(**kwargs)\n        self.input_dim = input_dim\n\n        self.fusion_layer = nn.Conv1d(\n            in_channels=input_dim, out_channels=1, kernel_size=1\n        )\n\n    def forward(self, *src):\n        src1, src2, token = src\n        if self.input_dim == 3:\n            processed_src1 = self.fusion_layer(\n                src1[:, :, 0:3].permute(0, 2, 1)\n            ).permute(0, 2, 1)\n            combined_input = torch.cat((processed_src1, src1[:, :, 3:]), dim=2)\n        else:\n            processed_src1 = self.fusion_layer(\n                src1[:, :, 0:2].permute(0, 2, 1)\n            ).permute(0, 2, 1)\n            combined_input = torch.cat((processed_src1, src1[:, :, 2:]), dim=2)\n\n        return super(DataFusionModel, self).forward(combined_input, src2, token)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.DataFusionModel.forward","title":"<code>forward(self, *src)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>def forward(self, *src):\n    src1, src2, token = src\n    if self.input_dim == 3:\n        processed_src1 = self.fusion_layer(\n            src1[:, :, 0:3].permute(0, 2, 1)\n        ).permute(0, 2, 1)\n        combined_input = torch.cat((processed_src1, src1[:, :, 3:]), dim=2)\n    else:\n        processed_src1 = self.fusion_layer(\n            src1[:, :, 0:2].permute(0, 2, 1)\n        ).permute(0, 2, 1)\n        combined_input = torch.cat((processed_src1, src1[:, :, 2:]), dim=2)\n\n    return super(DataFusionModel, self).forward(combined_input, src2, token)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.Decoder","title":"<code> Decoder            (Module)         </code>","text":"Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>class Decoder(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim, num_layers=1, dropout=0.3):\n        super(Decoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.pre_fc = nn.Linear(input_dim, hidden_dim)\n        self.pre_relu = nn.ReLU()\n        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers)\n        self.dropout = nn.Dropout(dropout)\n        self.fc_out = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, input, hidden, cell):\n        x0 = self.pre_fc(input)\n        x1 = self.pre_relu(x0)\n        output_, (hidden_, cell_) = self.lstm(x1, (hidden, cell))\n        output_dr = self.dropout(output_)\n        output = self.fc_out(output_dr)\n        return output, hidden_, cell_\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.Decoder.forward","title":"<code>forward(self, input, hidden, cell)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>def forward(self, input, hidden, cell):\n    x0 = self.pre_fc(input)\n    x1 = self.pre_relu(x0)\n    output_, (hidden_, cell_) = self.lstm(x1, (hidden, cell))\n    output_dr = self.dropout(output_)\n    output = self.fc_out(output_dr)\n    return output, hidden_, cell_\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.DotProductAttention","title":"<code> DotProductAttention            (Module)         </code>","text":"Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>class DotProductAttention(nn.Module):\n    def __init__(self, dropout=0.1):\n        super(DotProductAttention, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, encoder_outputs, hidden):\n        hidden_dim = encoder_outputs.shape[2]\n        hidden_expanded = hidden.unsqueeze(1)\n        scores = torch.bmm(\n            hidden_expanded, encoder_outputs.transpose(1, 2)\n        ) / math.sqrt(hidden_dim)\n        attention_weights = F.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n        return attention_weights.squeeze(1)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.DotProductAttention.forward","title":"<code>forward(self, encoder_outputs, hidden)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>def forward(self, encoder_outputs, hidden):\n    hidden_dim = encoder_outputs.shape[2]\n    hidden_expanded = hidden.unsqueeze(1)\n    scores = torch.bmm(\n        hidden_expanded, encoder_outputs.transpose(1, 2)\n    ) / math.sqrt(hidden_dim)\n    attention_weights = F.softmax(scores, dim=-1)\n    attention_weights = self.dropout(attention_weights)\n    return attention_weights.squeeze(1)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.Encoder","title":"<code> Encoder            (Module)         </code>","text":"Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>class Encoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.3):\n        super(Encoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.pre_fc = nn.Linear(input_dim, hidden_dim)\n        self.pre_relu = nn.ReLU()\n        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # a nonlinear layer to transform the input\n        x0 = self.pre_fc(x)\n        x1 = self.pre_relu(x0)\n        # the LSTM layer\n        outputs_, (hidden, cell) = self.lstm(x1)\n        # a dropout layer\n        dr_outputs = self.dropout(outputs_)\n        # final linear layer\n        outputs = self.fc(dr_outputs)\n        return outputs, hidden, cell\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.Encoder.forward","title":"<code>forward(self, x)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>def forward(self, x):\n    # a nonlinear layer to transform the input\n    x0 = self.pre_fc(x)\n    x1 = self.pre_relu(x0)\n    # the LSTM layer\n    outputs_, (hidden, cell) = self.lstm(x1)\n    # a dropout layer\n    dr_outputs = self.dropout(outputs_)\n    # final linear layer\n    outputs = self.fc(dr_outputs)\n    return outputs, hidden, cell\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.GeneralSeq2Seq","title":"<code> GeneralSeq2Seq            (Module)         </code>","text":"Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>class GeneralSeq2Seq(nn.Module):\n    def __init__(\n        self,\n        en_input_size,\n        de_input_size,\n        output_size,\n        hidden_size,\n        forecast_length,\n        hindcast_output_window=0,\n        teacher_forcing_ratio=0.5,\n    ):\n        \"\"\"General Seq2Seq model\n\n        Parameters\n        ----------\n        en_input_size : _type_\n            the size of the input of the encoder\n        de_input_size : _type_\n            the size of the input of the decoder\n        output_size : _type_\n            the size of the output, same for encoder and decoder\n        hidden_size : _type_\n            the size of the hidden state of LSTMs\n        forecast_length : _type_\n            the length of the forecast, i.e., the periods of decoder outputs\n        hindcast_output_window : int, optional\n            the encoder's final several outputs in the final output;\n            default is 0 which means no encoder output is included in the final output;\n        teacher_forcing_ratio : float, optional\n            the probability of using teacher forcing\n        \"\"\"\n        super(GeneralSeq2Seq, self).__init__()\n        self.trg_len = forecast_length\n        self.hindcast_output_window = hindcast_output_window\n        self.teacher_forcing_ratio = teacher_forcing_ratio\n        self.output_size = output_size\n        self.encoder = Encoder(\n            input_dim=en_input_size, hidden_dim=hidden_size, output_dim=output_size\n        )\n        self.decoder = Decoder(\n            input_dim=de_input_size, hidden_dim=hidden_size, output_dim=output_size\n        )\n        self.transfer = StateTransferNetwork(hidden_dim=hidden_size)\n\n    def _teacher_forcing_preparation(self, trgs):\n        # teacher forcing preparation\n        valid_mask = ~torch.isnan(trgs)\n        random_vals = torch.rand_like(valid_mask, dtype=torch.float)\n        return (random_vals &lt; self.teacher_forcing_ratio) * valid_mask\n\n    def forward(self, *src):\n        if len(src) == 3:\n            encoder_input, decoder_input, trgs = src\n        else:\n            encoder_input, decoder_input = src\n            device = decoder_input.device\n            trgs = torch.full(\n                (\n                    self.hindcast_output_window + self.trg_len,  # seq\n                    decoder_input.shape[1],  # batch_size\n                    self.output_size,  # features\n                ),\n                float(\"nan\"),\n            ).to(device)\n        trgs_q = trgs[:, :, :1]\n        trgs_s = trgs[:, :, 1:]\n        trgs = torch.cat((trgs_s, trgs_q), dim=2)  # sq\n        encoder_outputs, hidden_, cell_ = self.encoder(encoder_input)  # sq\n        hidden, cell = self.transfer(hidden_, cell_)\n        outputs = []\n        prev_output = encoder_outputs[-1, :, :].unsqueeze(0)  # sq\n        _, batch_size, _ = decoder_input.size()\n\n        outputs = torch.zeros(self.trg_len, batch_size, self.output_size).to(\n            decoder_input.device\n        )\n        use_teacher_forcing = self._teacher_forcing_preparation(trgs)\n        for t in range(self.trg_len):\n            pc = decoder_input[t : t + 1, :, :]  # sq\n            obs = trgs[self.hindcast_output_window + t, :, :].unsqueeze(0)  # sq\n            safe_obs = torch.where(torch.isnan(obs), torch.zeros_like(obs), obs)\n            prev_output = torch.where(  # sq\n                use_teacher_forcing[t : t + 1, :, :],\n                safe_obs,\n                prev_output,\n            )\n            current_input = torch.cat((pc, prev_output), dim=2)  # pcsq\n            output, hidden, cell = self.decoder(current_input, hidden, cell)\n            outputs[t, :, :] = output.squeeze(0)  # sq\n        if self.hindcast_output_window &gt; 0:\n            prec_outputs = encoder_outputs[-self.hindcast_output_window :, :, :]\n            outputs = torch.cat((prec_outputs, outputs), dim=0)\n        outputs_s = outputs[:, :, :1]\n        outputs_q = outputs[:, :, 1:]\n        outputs = torch.cat((outputs_q, outputs_s), dim=2)  # qs\n        return outputs\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.GeneralSeq2Seq.__init__","title":"<code>__init__(self, en_input_size, de_input_size, output_size, hidden_size, forecast_length, hindcast_output_window=0, teacher_forcing_ratio=0.5)</code>  <code>special</code>","text":"<p>General Seq2Seq model</p>"},{"location":"api/models/#torchhydro.models.seq2seq.GeneralSeq2Seq.__init__--parameters","title":"Parameters","text":"<p>en_input_size : type     the size of the input of the encoder de_input_size : type     the size of the input of the decoder output_size : type     the size of the output, same for encoder and decoder hidden_size : type     the size of the hidden state of LSTMs forecast_length : type     the length of the forecast, i.e., the periods of decoder outputs hindcast_output_window : int, optional     the encoder's final several outputs in the final output;     default is 0 which means no encoder output is included in the final output; teacher_forcing_ratio : float, optional     the probability of using teacher forcing</p> Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>def __init__(\n    self,\n    en_input_size,\n    de_input_size,\n    output_size,\n    hidden_size,\n    forecast_length,\n    hindcast_output_window=0,\n    teacher_forcing_ratio=0.5,\n):\n    \"\"\"General Seq2Seq model\n\n    Parameters\n    ----------\n    en_input_size : _type_\n        the size of the input of the encoder\n    de_input_size : _type_\n        the size of the input of the decoder\n    output_size : _type_\n        the size of the output, same for encoder and decoder\n    hidden_size : _type_\n        the size of the hidden state of LSTMs\n    forecast_length : _type_\n        the length of the forecast, i.e., the periods of decoder outputs\n    hindcast_output_window : int, optional\n        the encoder's final several outputs in the final output;\n        default is 0 which means no encoder output is included in the final output;\n    teacher_forcing_ratio : float, optional\n        the probability of using teacher forcing\n    \"\"\"\n    super(GeneralSeq2Seq, self).__init__()\n    self.trg_len = forecast_length\n    self.hindcast_output_window = hindcast_output_window\n    self.teacher_forcing_ratio = teacher_forcing_ratio\n    self.output_size = output_size\n    self.encoder = Encoder(\n        input_dim=en_input_size, hidden_dim=hidden_size, output_dim=output_size\n    )\n    self.decoder = Decoder(\n        input_dim=de_input_size, hidden_dim=hidden_size, output_dim=output_size\n    )\n    self.transfer = StateTransferNetwork(hidden_dim=hidden_size)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.GeneralSeq2Seq.forward","title":"<code>forward(self, *src)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>def forward(self, *src):\n    if len(src) == 3:\n        encoder_input, decoder_input, trgs = src\n    else:\n        encoder_input, decoder_input = src\n        device = decoder_input.device\n        trgs = torch.full(\n            (\n                self.hindcast_output_window + self.trg_len,  # seq\n                decoder_input.shape[1],  # batch_size\n                self.output_size,  # features\n            ),\n            float(\"nan\"),\n        ).to(device)\n    trgs_q = trgs[:, :, :1]\n    trgs_s = trgs[:, :, 1:]\n    trgs = torch.cat((trgs_s, trgs_q), dim=2)  # sq\n    encoder_outputs, hidden_, cell_ = self.encoder(encoder_input)  # sq\n    hidden, cell = self.transfer(hidden_, cell_)\n    outputs = []\n    prev_output = encoder_outputs[-1, :, :].unsqueeze(0)  # sq\n    _, batch_size, _ = decoder_input.size()\n\n    outputs = torch.zeros(self.trg_len, batch_size, self.output_size).to(\n        decoder_input.device\n    )\n    use_teacher_forcing = self._teacher_forcing_preparation(trgs)\n    for t in range(self.trg_len):\n        pc = decoder_input[t : t + 1, :, :]  # sq\n        obs = trgs[self.hindcast_output_window + t, :, :].unsqueeze(0)  # sq\n        safe_obs = torch.where(torch.isnan(obs), torch.zeros_like(obs), obs)\n        prev_output = torch.where(  # sq\n            use_teacher_forcing[t : t + 1, :, :],\n            safe_obs,\n            prev_output,\n        )\n        current_input = torch.cat((pc, prev_output), dim=2)  # pcsq\n        output, hidden, cell = self.decoder(current_input, hidden, cell)\n        outputs[t, :, :] = output.squeeze(0)  # sq\n    if self.hindcast_output_window &gt; 0:\n        prec_outputs = encoder_outputs[-self.hindcast_output_window :, :, :]\n        outputs = torch.cat((prec_outputs, outputs), dim=0)\n    outputs_s = outputs[:, :, :1]\n    outputs_q = outputs[:, :, 1:]\n    outputs = torch.cat((outputs_q, outputs_s), dim=2)  # qs\n    return outputs\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.StateTransferNetwork","title":"<code> StateTransferNetwork            (Module)         </code>","text":"Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>class StateTransferNetwork(nn.Module):\n    def __init__(self, hidden_dim):\n        super(StateTransferNetwork, self).__init__()\n        self.fc_hidden = nn.Linear(hidden_dim, hidden_dim)\n        self.fc_cell = nn.Linear(hidden_dim, hidden_dim)\n\n    def forward(self, hidden, cell):\n        transfer_hidden = torch.tanh(self.fc_hidden(hidden))\n        transfer_cell = self.fc_cell(cell)\n        return transfer_hidden, transfer_cell\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.StateTransferNetwork.forward","title":"<code>forward(self, hidden, cell)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>def forward(self, hidden, cell):\n    transfer_hidden = torch.tanh(self.fc_hidden(hidden))\n    transfer_cell = self.fc_cell(cell)\n    return transfer_hidden, transfer_cell\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.Transformer","title":"<code> Transformer            (Module)         </code>","text":"Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>class Transformer(nn.Module):\n    def __init__(\n        self,\n        n_encoder_inputs,\n        n_decoder_inputs,\n        n_decoder_output,\n        channels=256,\n        num_embeddings=512,\n        nhead=8,\n        num_layers=8,\n        dropout=0.1,\n        hindcast_output_window=0,\n    ):\n        \"\"\"TODO: hindcast_output_window seems not used\n\n        Parameters\n        ----------\n        n_encoder_inputs : _type_\n            _description_\n        n_decoder_inputs : _type_\n            _description_\n        n_decoder_output : _type_\n            _description_\n        channels : int, optional\n            _description_, by default 256\n        num_embeddings : int, optional\n            _description_, by default 512\n        nhead : int, optional\n            _description_, by default 8\n        num_layers : int, optional\n            _description_, by default 8\n        dropout : float, optional\n            _description_, by default 0.1\n        hindcast_output_window : int, optional\n            _description_, by default 0\n        \"\"\"\n        super().__init__()\n\n        self.input_pos_embedding = torch.nn.Embedding(num_embeddings, channels)\n        self.target_pos_embedding = torch.nn.Embedding(num_embeddings, channels)\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=channels,\n            nhead=nhead,\n            dropout=dropout,\n            dim_feedforward=4 * channels,\n        )\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=channels,\n            nhead=nhead,\n            dropout=dropout,\n            dim_feedforward=4 * channels,\n        )\n\n        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers)\n        self.decoder = torch.nn.TransformerDecoder(decoder_layer, num_layers)\n\n        self.input_projection = nn.Linear(n_encoder_inputs, channels)\n        self.output_projection = nn.Linear(n_decoder_inputs, channels)\n\n        self.linear = nn.Linear(channels, n_decoder_output)\n\n        self.do = nn.Dropout(p=dropout)\n\n    def encode_src(self, src):\n        src_start = self.input_projection(src)\n\n        in_sequence_len, batch_size = src_start.size(0), src_start.size(1)\n        pos_encoder = (\n            torch.arange(0, in_sequence_len, device=src.device)\n            .unsqueeze(0)\n            .repeat(batch_size, 1)\n        )\n        pos_encoder = self.input_pos_embedding(pos_encoder).permute(1, 0, 2)\n\n        src = src_start + pos_encoder\n        src = self.encoder(src) + src_start\n        src = self.do(src)\n        return src\n\n    def decode_trg(self, trg, memory):\n        trg_start = self.output_projection(trg)\n\n        out_sequence_len, batch_size = trg_start.size(0), trg_start.size(1)\n        pos_decoder = (\n            torch.arange(0, out_sequence_len, device=trg.device)\n            .unsqueeze(0)\n            .repeat(batch_size, 1)\n        )\n        pos_decoder = self.target_pos_embedding(pos_decoder).permute(1, 0, 2)\n\n        trg = pos_decoder + trg_start\n        trg_mask = gen_trg_mask(out_sequence_len, trg.device)\n        out = self.decoder(tgt=trg, memory=memory, tgt_mask=trg_mask) + trg_start\n        out = self.do(out)\n        out = self.linear(out)\n        return out\n\n    def forward(self, *x):\n        src, trg = x\n        src = self.encode_src(src)\n        return self.decode_trg(trg=trg, memory=src)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.Transformer.__init__","title":"<code>__init__(self, n_encoder_inputs, n_decoder_inputs, n_decoder_output, channels=256, num_embeddings=512, nhead=8, num_layers=8, dropout=0.1, hindcast_output_window=0)</code>  <code>special</code>","text":"<p>TODO: hindcast_output_window seems not used</p>"},{"location":"api/models/#torchhydro.models.seq2seq.Transformer.__init__--parameters","title":"Parameters","text":"<p>n_encoder_inputs : type description n_decoder_inputs : type description n_decoder_output : type description channels : int, optional     description, by default 256 num_embeddings : int, optional     description, by default 512 nhead : int, optional     description, by default 8 num_layers : int, optional     description, by default 8 dropout : float, optional     description, by default 0.1 hindcast_output_window : int, optional     description, by default 0</p> Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>def __init__(\n    self,\n    n_encoder_inputs,\n    n_decoder_inputs,\n    n_decoder_output,\n    channels=256,\n    num_embeddings=512,\n    nhead=8,\n    num_layers=8,\n    dropout=0.1,\n    hindcast_output_window=0,\n):\n    \"\"\"TODO: hindcast_output_window seems not used\n\n    Parameters\n    ----------\n    n_encoder_inputs : _type_\n        _description_\n    n_decoder_inputs : _type_\n        _description_\n    n_decoder_output : _type_\n        _description_\n    channels : int, optional\n        _description_, by default 256\n    num_embeddings : int, optional\n        _description_, by default 512\n    nhead : int, optional\n        _description_, by default 8\n    num_layers : int, optional\n        _description_, by default 8\n    dropout : float, optional\n        _description_, by default 0.1\n    hindcast_output_window : int, optional\n        _description_, by default 0\n    \"\"\"\n    super().__init__()\n\n    self.input_pos_embedding = torch.nn.Embedding(num_embeddings, channels)\n    self.target_pos_embedding = torch.nn.Embedding(num_embeddings, channels)\n\n    encoder_layer = nn.TransformerEncoderLayer(\n        d_model=channels,\n        nhead=nhead,\n        dropout=dropout,\n        dim_feedforward=4 * channels,\n    )\n    decoder_layer = nn.TransformerDecoderLayer(\n        d_model=channels,\n        nhead=nhead,\n        dropout=dropout,\n        dim_feedforward=4 * channels,\n    )\n\n    self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers)\n    self.decoder = torch.nn.TransformerDecoder(decoder_layer, num_layers)\n\n    self.input_projection = nn.Linear(n_encoder_inputs, channels)\n    self.output_projection = nn.Linear(n_decoder_inputs, channels)\n\n    self.linear = nn.Linear(channels, n_decoder_output)\n\n    self.do = nn.Dropout(p=dropout)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seq2seq.Transformer.forward","title":"<code>forward(self, *x)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/seq2seq.py</code> <pre><code>def forward(self, *x):\n    src, trg = x\n    src = self.encode_src(src)\n    return self.decode_trg(trg=trg, memory=src)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seqforecast","title":"<code>seqforecast</code>","text":""},{"location":"api/models/#torchhydro.models.seqforecast.FeatureEmbedding","title":"<code> FeatureEmbedding            (Module)         </code>","text":"Source code in <code>torchhydro/models/seqforecast.py</code> <pre><code>class FeatureEmbedding(nn.Module):\n    def __init__(\n        self, input_dim, embedding_dim, hidden_size=0, dropout=0.0, activation=\"relu\"\n    ):\n        super(FeatureEmbedding, self).__init__()\n        self.embedding = Mlp(\n            input_dim,\n            embedding_dim,\n            hidden_size=hidden_size,\n            dr=dropout,\n            activation=activation,\n        )\n\n    def forward(self, static_features):\n        return self.embedding(static_features)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seqforecast.FeatureEmbedding.forward","title":"<code>forward(self, static_features)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/seqforecast.py</code> <pre><code>def forward(self, static_features):\n    return self.embedding(static_features)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seqforecast.ForecastLSTM","title":"<code> ForecastLSTM            (Module)         </code>","text":"Source code in <code>torchhydro/models/seqforecast.py</code> <pre><code>class ForecastLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, dropout=0):\n        super(ForecastLSTM, self).__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, dropout=dropout)\n\n    def forward(self, x, h, c):\n        output, _ = self.lstm(x, (h, c))\n        return output\n</code></pre>"},{"location":"api/models/#torchhydro.models.seqforecast.ForecastLSTM.forward","title":"<code>forward(self, x, h, c)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/seqforecast.py</code> <pre><code>def forward(self, x, h, c):\n    output, _ = self.lstm(x, (h, c))\n    return output\n</code></pre>"},{"location":"api/models/#torchhydro.models.seqforecast.HiddenStateTransferNet","title":"<code> HiddenStateTransferNet            (Module)         </code>","text":"Source code in <code>torchhydro/models/seqforecast.py</code> <pre><code>class HiddenStateTransferNet(nn.Module):\n    def __init__(\n        self, hindcast_hidden_dim, forecast_hidden_dim, dropout=0.0, activation=\"relu\"\n    ):\n        super(HiddenStateTransferNet, self).__init__()\n        self.linear_transfer = nn.Linear(hindcast_hidden_dim, forecast_hidden_dim)\n        self.nonlinear_transfer = Mlp(\n            hindcast_hidden_dim,\n            forecast_hidden_dim,\n            hidden_size=0,\n            dr=dropout,\n            activation=activation,\n        )\n\n    def forward(self, hidden, cell):\n        transfer_hidden = self.nonlinear_transfer(hidden)\n        transfer_cell = self.linear_transfer(cell)\n        return transfer_hidden, transfer_cell\n</code></pre>"},{"location":"api/models/#torchhydro.models.seqforecast.HiddenStateTransferNet.forward","title":"<code>forward(self, hidden, cell)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/seqforecast.py</code> <pre><code>def forward(self, hidden, cell):\n    transfer_hidden = self.nonlinear_transfer(hidden)\n    transfer_cell = self.linear_transfer(cell)\n    return transfer_hidden, transfer_cell\n</code></pre>"},{"location":"api/models/#torchhydro.models.seqforecast.HindcastLSTM","title":"<code> HindcastLSTM            (Module)         </code>","text":"Source code in <code>torchhydro/models/seqforecast.py</code> <pre><code>class HindcastLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, dropout=0):\n        super(HindcastLSTM, self).__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, dropout=dropout)\n\n    def forward(self, x):\n        output, (h, c) = self.lstm(x)\n        return output, h, c\n</code></pre>"},{"location":"api/models/#torchhydro.models.seqforecast.HindcastLSTM.forward","title":"<code>forward(self, x)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/seqforecast.py</code> <pre><code>def forward(self, x):\n    output, (h, c) = self.lstm(x)\n    return output, h, c\n</code></pre>"},{"location":"api/models/#torchhydro.models.seqforecast.ModelOutputHead","title":"<code> ModelOutputHead            (Module)         </code>","text":"Source code in <code>torchhydro/models/seqforecast.py</code> <pre><code>class ModelOutputHead(nn.Module):\n    def __init__(self, hidden_dim, output_dim):\n        super(ModelOutputHead, self).__init__()\n        self.head = nn.Sequential(nn.Linear(hidden_dim, output_dim))\n\n    def forward(self, x):\n        return self.head(x)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seqforecast.ModelOutputHead.forward","title":"<code>forward(self, x)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/seqforecast.py</code> <pre><code>def forward(self, x):\n    return self.head(x)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seqforecast.SequentialForecastLSTM","title":"<code> SequentialForecastLSTM            (Module)         </code>","text":"Source code in <code>torchhydro/models/seqforecast.py</code> <pre><code>class SequentialForecastLSTM(nn.Module):\n    def __init__(\n        self,\n        static_input_dim,\n        dynamic_input_dim,\n        static_embedding_dim,\n        sta_embed_hidden_dim,\n        dynamic_embedding_dim,\n        dyn_embed_hidden_dim,\n        hindcast_hidden_dim,\n        forecast_hidden_dim,\n        output_dim,\n        hindcast_output_window,\n        embedding_dropout,\n        handoff_dropout,\n        lstm_dropout,\n        activation=\"relu\",\n    ):\n        \"\"\"_summary_\n\n        Parameters\n        ----------\n        static_input_dim : int\n            _description_\n        dynamic_input_dim : int\n            _description_\n        static_embedding_dim : int\n            output size of static embedding\n        sta_embed_hidden_dim: int\n            hidden size of static embedding\n        dynamic_embedding_dim : int\n            output size of dynamic embedding\n        dyn_embed_hidden_dim: int\n            hidden size of dynamic embedding\n        hidden_dim : _type_\n            _description_\n        output_dim : _type_\n            _description_\n        hindcast_output_window : int\n            length of hindcast output to calculate loss\n        \"\"\"\n        super(SequentialForecastLSTM, self).__init__()\n        self.output_dim = output_dim\n        self.hindcast_output_window = hindcast_output_window\n        self.dynamic_embedding_dim = dynamic_embedding_dim\n        if static_embedding_dim &gt; 0:\n            self.static_embedding = FeatureEmbedding(\n                static_input_dim,\n                static_embedding_dim,\n                sta_embed_hidden_dim,\n                embedding_dropout,\n                activation,\n            )\n        if dynamic_embedding_dim &gt; 0:\n            self.dynamic_embedding = FeatureEmbedding(\n                dynamic_input_dim,\n                dynamic_embedding_dim,\n                dyn_embed_hidden_dim,\n                embedding_dropout,\n                activation,\n            )\n        self.static_embedding_dim = static_embedding_dim\n        self.dynamic_embedding_dim = dynamic_embedding_dim\n        hindcast_input_dim = (\n            dynamic_embedding_dim if dynamic_embedding_dim != 0 else dynamic_input_dim\n        ) + (static_embedding_dim if static_embedding_dim != 0 else static_input_dim)\n        forecast_input_dim = (\n            dynamic_embedding_dim if dynamic_embedding_dim != 0 else dynamic_input_dim\n        ) + (static_embedding_dim if static_embedding_dim != 0 else static_input_dim)\n        self.hindcast_lstm = HindcastLSTM(\n            hindcast_input_dim, hindcast_hidden_dim, lstm_dropout\n        )\n        self.forecast_lstm = ForecastLSTM(\n            forecast_input_dim, forecast_hidden_dim, lstm_dropout\n        )\n        self.hiddenstatetransfer = HiddenStateTransferNet(\n            hindcast_hidden_dim,\n            forecast_hidden_dim,\n            dropout=handoff_dropout,\n            activation=activation,\n        )\n        self.hindcast_output_head = ModelOutputHead(hindcast_hidden_dim, output_dim)\n        self.forecast_output_head = ModelOutputHead(forecast_hidden_dim, output_dim)\n\n    def _perform_embedding(self, static_features, dynamic_features):\n        if self.dynamic_embedding_dim &gt; 0:\n            dynamic_embedded = self.dynamic_embedding(dynamic_features)\n        else:\n            dynamic_embedded = dynamic_features\n        if self.static_embedding_dim &gt; 0:\n            static_embedded = self.static_embedding(static_features)\n        else:\n            static_embedded = static_features\n        static_embedded = static_embedded.unsqueeze(1).expand(\n            -1, dynamic_embedded.size(1), -1\n        )\n        return torch.cat([dynamic_embedded, static_embedded], dim=-1)\n\n    def forward(self, *src):\n        (\n            hindcast_features,\n            forecast_features,\n            static_features,\n        ) = src\n\n        # Hindcast LSTM\n        hindcast_input = self._perform_embedding(static_features, hindcast_features)\n        hincast_output, h, c = self.hindcast_lstm(hindcast_input)\n\n        if self.hindcast_output_window &gt; 0:\n            hincast_output = self.hindcast_output_head(\n                hincast_output[:, -self.hindcast_output_window :, :]\n            )\n\n        h, c = self.hiddenstatetransfer(h, c)\n\n        # Forecast LSTM\n        forecast_input = self._perform_embedding(static_features, forecast_features)\n        forecast_output = self.forecast_lstm(forecast_input, h, c)\n        forecast_output = self.forecast_output_head(forecast_output)\n        if self.hindcast_output_window &gt; 0:\n            forecast_output = torch.cat([hincast_output, forecast_output], dim=1)\n        return forecast_output\n</code></pre>"},{"location":"api/models/#torchhydro.models.seqforecast.SequentialForecastLSTM.__init__","title":"<code>__init__(self, static_input_dim, dynamic_input_dim, static_embedding_dim, sta_embed_hidden_dim, dynamic_embedding_dim, dyn_embed_hidden_dim, hindcast_hidden_dim, forecast_hidden_dim, output_dim, hindcast_output_window, embedding_dropout, handoff_dropout, lstm_dropout, activation='relu')</code>  <code>special</code>","text":"<p>summary</p>"},{"location":"api/models/#torchhydro.models.seqforecast.SequentialForecastLSTM.__init__--parameters","title":"Parameters","text":"<p>static_input_dim : int     description dynamic_input_dim : int     description static_embedding_dim : int     output size of static embedding !!! sta_embed_hidden_dim \"int\"     hidden size of static embedding dynamic_embedding_dim : int     output size of dynamic embedding !!! dyn_embed_hidden_dim \"int\"     hidden size of dynamic embedding hidden_dim : type description output_dim : type description hindcast_output_window : int     length of hindcast output to calculate loss</p> Source code in <code>torchhydro/models/seqforecast.py</code> <pre><code>def __init__(\n    self,\n    static_input_dim,\n    dynamic_input_dim,\n    static_embedding_dim,\n    sta_embed_hidden_dim,\n    dynamic_embedding_dim,\n    dyn_embed_hidden_dim,\n    hindcast_hidden_dim,\n    forecast_hidden_dim,\n    output_dim,\n    hindcast_output_window,\n    embedding_dropout,\n    handoff_dropout,\n    lstm_dropout,\n    activation=\"relu\",\n):\n    \"\"\"_summary_\n\n    Parameters\n    ----------\n    static_input_dim : int\n        _description_\n    dynamic_input_dim : int\n        _description_\n    static_embedding_dim : int\n        output size of static embedding\n    sta_embed_hidden_dim: int\n        hidden size of static embedding\n    dynamic_embedding_dim : int\n        output size of dynamic embedding\n    dyn_embed_hidden_dim: int\n        hidden size of dynamic embedding\n    hidden_dim : _type_\n        _description_\n    output_dim : _type_\n        _description_\n    hindcast_output_window : int\n        length of hindcast output to calculate loss\n    \"\"\"\n    super(SequentialForecastLSTM, self).__init__()\n    self.output_dim = output_dim\n    self.hindcast_output_window = hindcast_output_window\n    self.dynamic_embedding_dim = dynamic_embedding_dim\n    if static_embedding_dim &gt; 0:\n        self.static_embedding = FeatureEmbedding(\n            static_input_dim,\n            static_embedding_dim,\n            sta_embed_hidden_dim,\n            embedding_dropout,\n            activation,\n        )\n    if dynamic_embedding_dim &gt; 0:\n        self.dynamic_embedding = FeatureEmbedding(\n            dynamic_input_dim,\n            dynamic_embedding_dim,\n            dyn_embed_hidden_dim,\n            embedding_dropout,\n            activation,\n        )\n    self.static_embedding_dim = static_embedding_dim\n    self.dynamic_embedding_dim = dynamic_embedding_dim\n    hindcast_input_dim = (\n        dynamic_embedding_dim if dynamic_embedding_dim != 0 else dynamic_input_dim\n    ) + (static_embedding_dim if static_embedding_dim != 0 else static_input_dim)\n    forecast_input_dim = (\n        dynamic_embedding_dim if dynamic_embedding_dim != 0 else dynamic_input_dim\n    ) + (static_embedding_dim if static_embedding_dim != 0 else static_input_dim)\n    self.hindcast_lstm = HindcastLSTM(\n        hindcast_input_dim, hindcast_hidden_dim, lstm_dropout\n    )\n    self.forecast_lstm = ForecastLSTM(\n        forecast_input_dim, forecast_hidden_dim, lstm_dropout\n    )\n    self.hiddenstatetransfer = HiddenStateTransferNet(\n        hindcast_hidden_dim,\n        forecast_hidden_dim,\n        dropout=handoff_dropout,\n        activation=activation,\n    )\n    self.hindcast_output_head = ModelOutputHead(hindcast_hidden_dim, output_dim)\n    self.forecast_output_head = ModelOutputHead(forecast_hidden_dim, output_dim)\n</code></pre>"},{"location":"api/models/#torchhydro.models.seqforecast.SequentialForecastLSTM.forward","title":"<code>forward(self, *src)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/seqforecast.py</code> <pre><code>def forward(self, *src):\n    (\n        hindcast_features,\n        forecast_features,\n        static_features,\n    ) = src\n\n    # Hindcast LSTM\n    hindcast_input = self._perform_embedding(static_features, hindcast_features)\n    hincast_output, h, c = self.hindcast_lstm(hindcast_input)\n\n    if self.hindcast_output_window &gt; 0:\n        hincast_output = self.hindcast_output_head(\n            hincast_output[:, -self.hindcast_output_window :, :]\n        )\n\n    h, c = self.hiddenstatetransfer(h, c)\n\n    # Forecast LSTM\n    forecast_input = self._perform_embedding(static_features, forecast_features)\n    forecast_output = self.forecast_lstm(forecast_input, h, c)\n    forecast_output = self.forecast_output_head(forecast_output)\n    if self.hindcast_output_window &gt; 0:\n        forecast_output = torch.cat([hincast_output, forecast_output], dim=1)\n    return forecast_output\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm","title":"<code>simple_lstm</code>","text":"<p>Author: Wenyu Ouyang Date: 2023-09-19 09:36:25 LastEditTime: 2025-11-08 15:55:14 LastEditors: Wenyu Ouyang Description: Some self-made LSTMs FilePath:       orchhydro       orchhydro\\models\\simple_lstm.py Copyright (c) 2023-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/models/#torchhydro.models.simple_lstm.HFLSTM","title":"<code> HFLSTM            (Module)         </code>","text":"Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>class HFLSTM(nn.Module):\n    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        hidden_size: int,\n        dr: float = 0.0,\n        teacher_forcing_ratio: float = 0,\n        hindcast_with_output: bool = True,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        input_size : int\n            without streamflow\n        output_size : int\n            streamflow\n        hidden_size : int\n        dr : float, optional\n            dropout, by default 0.0\n        teacher_forcing_ratio : float, optional\n            by default 0\n        hindcast_with_output : bool, optional\n            whether to use the output of the model as input for the next time step, by default True\n        \"\"\"\n        super(HFLSTM, self).__init__()\n        self.linearIn = nn.Linear(input_size, hidden_size)\n        self.lstm = nn.LSTM(\n            hidden_size,\n            hidden_size,\n        )\n        self.dropout = nn.Dropout(p=dr)\n        self.linearOut = nn.Linear(hidden_size, output_size)\n        self.teacher_forcing_ratio = teacher_forcing_ratio\n        self.hindcast_with_output = hindcast_with_output\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n\n    def _teacher_forcing_preparation(self, xq_hor: torch.Tensor) -&gt; torch.Tensor:\n        # teacher forcing preparation\n        valid_mask = ~torch.isnan(xq_hor)\n        random_vals = torch.rand_like(valid_mask, dtype=torch.float)\n        return (random_vals &lt; self.teacher_forcing_ratio) * valid_mask\n\n    def _rho_forward(\n        self, x_rho: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        x0_rho = F.relu(self.linearIn(x_rho))\n        out_lstm_rho, (hn_rho, cn_rho) = self.lstm(x0_rho)\n        out_lstm_rho_dr = self.dropout(out_lstm_rho)\n        out_lstm_rho_lnout = self.linearOut(out_lstm_rho_dr)\n        prev_output = out_lstm_rho_lnout[-1:, :, :]\n        return out_lstm_rho_lnout, hn_rho, cn_rho, prev_output\n\n    def forward(self, *x: Tuple[torch.Tensor, ...]) -&gt; torch.Tensor:\n        xfc_rho, xfc_hor, xq_rho, xq_hor = x\n\n        x_rho = torch.cat((xfc_rho, xq_rho), dim=-1)\n        hor_len, batch_size, _ = xfc_hor.size()\n\n        # hindcast-forecast, we do not have forecast-hindcast situation\n        # do rho forward first, prev_output is the last output of rho (seq_length = 1, batch_size, feature = output_size)\n        if self.hindcast_with_output:\n            _, h_n, c_n, prev_output = self._rho_forward(x_rho)\n            seq_len = hor_len\n        else:\n            # TODO: need more test\n            seq_len = xfc_rho.shape[0] + hor_len\n            xfc_hor = torch.cat((xfc_rho, xfc_hor), dim=0)\n            xq_hor = torch.cat((xq_rho, xq_hor), dim=0)\n            h_n = torch.randn(1, batch_size, self.hidden_size).to(xfc_rho.device) * 0.1\n            c_n = torch.randn(1, batch_size, self.hidden_size).to(xfc_rho.device) * 0.1\n            prev_output = (\n                torch.randn(1, batch_size, self.output_size).to(xfc_rho.device) * 0.1\n            )\n\n        use_teacher_forcing = self._teacher_forcing_preparation(xq_hor)\n\n        # do hor forward\n        outputs = torch.zeros(seq_len, batch_size, self.output_size).to(xfc_rho.device)\n        # TODO: too slow here when seq_len is large, need to optimize\n        for t in range(seq_len):\n            real_streamflow_input = xq_hor[t : t + 1, :, :]\n            prev_output = torch.where(\n                use_teacher_forcing[t : t + 1, :, :],\n                real_streamflow_input,\n                prev_output,\n            )\n            input_concat = torch.cat((xfc_hor[t : t + 1, :, :], prev_output), dim=-1)\n\n            # Pass through the initial linear layer\n            x0 = F.relu(self.linearIn(input_concat))\n\n            # LSTM step\n            out_lstm, (h_n, c_n) = self.lstm(x0, (h_n, c_n))\n\n            # Generate the current output\n            prev_output = self.linearOut(out_lstm)\n            outputs[t, :, :] = prev_output.squeeze(0)\n        # Return the outputs\n        return outputs[-hor_len:, :, :]\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.HFLSTM.__init__","title":"<code>__init__(self, input_size, output_size, hidden_size, dr=0.0, teacher_forcing_ratio=0, hindcast_with_output=True)</code>  <code>special</code>","text":""},{"location":"api/models/#torchhydro.models.simple_lstm.HFLSTM.__init__--parameters","title":"Parameters","text":"<p>input_size : int     without streamflow output_size : int     streamflow hidden_size : int dr : float, optional     dropout, by default 0.0 teacher_forcing_ratio : float, optional     by default 0 hindcast_with_output : bool, optional     whether to use the output of the model as input for the next time step, by default True</p> Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    output_size: int,\n    hidden_size: int,\n    dr: float = 0.0,\n    teacher_forcing_ratio: float = 0,\n    hindcast_with_output: bool = True,\n):\n    \"\"\"\n\n    Parameters\n    ----------\n    input_size : int\n        without streamflow\n    output_size : int\n        streamflow\n    hidden_size : int\n    dr : float, optional\n        dropout, by default 0.0\n    teacher_forcing_ratio : float, optional\n        by default 0\n    hindcast_with_output : bool, optional\n        whether to use the output of the model as input for the next time step, by default True\n    \"\"\"\n    super(HFLSTM, self).__init__()\n    self.linearIn = nn.Linear(input_size, hidden_size)\n    self.lstm = nn.LSTM(\n        hidden_size,\n        hidden_size,\n    )\n    self.dropout = nn.Dropout(p=dr)\n    self.linearOut = nn.Linear(hidden_size, output_size)\n    self.teacher_forcing_ratio = teacher_forcing_ratio\n    self.hindcast_with_output = hindcast_with_output\n    self.hidden_size = hidden_size\n    self.output_size = output_size\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.HFLSTM.forward","title":"<code>forward(self, *x)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>def forward(self, *x: Tuple[torch.Tensor, ...]) -&gt; torch.Tensor:\n    xfc_rho, xfc_hor, xq_rho, xq_hor = x\n\n    x_rho = torch.cat((xfc_rho, xq_rho), dim=-1)\n    hor_len, batch_size, _ = xfc_hor.size()\n\n    # hindcast-forecast, we do not have forecast-hindcast situation\n    # do rho forward first, prev_output is the last output of rho (seq_length = 1, batch_size, feature = output_size)\n    if self.hindcast_with_output:\n        _, h_n, c_n, prev_output = self._rho_forward(x_rho)\n        seq_len = hor_len\n    else:\n        # TODO: need more test\n        seq_len = xfc_rho.shape[0] + hor_len\n        xfc_hor = torch.cat((xfc_rho, xfc_hor), dim=0)\n        xq_hor = torch.cat((xq_rho, xq_hor), dim=0)\n        h_n = torch.randn(1, batch_size, self.hidden_size).to(xfc_rho.device) * 0.1\n        c_n = torch.randn(1, batch_size, self.hidden_size).to(xfc_rho.device) * 0.1\n        prev_output = (\n            torch.randn(1, batch_size, self.output_size).to(xfc_rho.device) * 0.1\n        )\n\n    use_teacher_forcing = self._teacher_forcing_preparation(xq_hor)\n\n    # do hor forward\n    outputs = torch.zeros(seq_len, batch_size, self.output_size).to(xfc_rho.device)\n    # TODO: too slow here when seq_len is large, need to optimize\n    for t in range(seq_len):\n        real_streamflow_input = xq_hor[t : t + 1, :, :]\n        prev_output = torch.where(\n            use_teacher_forcing[t : t + 1, :, :],\n            real_streamflow_input,\n            prev_output,\n        )\n        input_concat = torch.cat((xfc_hor[t : t + 1, :, :], prev_output), dim=-1)\n\n        # Pass through the initial linear layer\n        x0 = F.relu(self.linearIn(input_concat))\n\n        # LSTM step\n        out_lstm, (h_n, c_n) = self.lstm(x0, (h_n, c_n))\n\n        # Generate the current output\n        prev_output = self.linearOut(out_lstm)\n        outputs[t, :, :] = prev_output.squeeze(0)\n    # Return the outputs\n    return outputs[-hor_len:, :, :]\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.LinearMultiLayerLSTMModel","title":"<code> LinearMultiLayerLSTMModel            (MultiLayerLSTM)         </code>","text":"<p>This model is nonlinear layer + MultiLayerLSTM.</p> Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>class LinearMultiLayerLSTMModel(MultiLayerLSTM):\n    \"\"\"\n    This model is nonlinear layer + MultiLayerLSTM.\n    \"\"\"\n\n    def __init__(self, linear_size: int, **kwargs: Any):\n        \"\"\"\n\n        Parameters\n        ----------\n        linear_size\n            the number of input features for the first input linear layer\n        \"\"\"\n        super(LinearMultiLayerLSTMModel, self).__init__(**kwargs)\n        self.former_linear = nn.Linear(linear_size, kwargs[\"input_size\"])\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x0 = F.relu(self.former_linear(x))\n        return super(LinearMultiLayerLSTMModel, self).forward(x0)\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.LinearMultiLayerLSTMModel.__init__","title":"<code>__init__(self, linear_size, **kwargs)</code>  <code>special</code>","text":""},{"location":"api/models/#torchhydro.models.simple_lstm.LinearMultiLayerLSTMModel.__init__--parameters","title":"Parameters","text":"<p>linear_size     the number of input features for the first input linear layer</p> Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>def __init__(self, linear_size: int, **kwargs: Any):\n    \"\"\"\n\n    Parameters\n    ----------\n    linear_size\n        the number of input features for the first input linear layer\n    \"\"\"\n    super(LinearMultiLayerLSTMModel, self).__init__(**kwargs)\n    self.former_linear = nn.Linear(linear_size, kwargs[\"input_size\"])\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.LinearMultiLayerLSTMModel.forward","title":"<code>forward(self, x)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    x0 = F.relu(self.former_linear(x))\n    return super(LinearMultiLayerLSTMModel, self).forward(x0)\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.LinearSimpleLSTMModel","title":"<code> LinearSimpleLSTMModel            (SimpleLSTM)         </code>","text":"<p>This model is nonlinear layer + SimpleLSTM.</p> Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>class LinearSimpleLSTMModel(SimpleLSTM):\n    \"\"\"\n    This model is nonlinear layer + SimpleLSTM.\n    \"\"\"\n\n    def __init__(self, linear_size: int, **kwargs: Any):\n        \"\"\"\n\n        Parameters\n        ----------\n        linear_size\n            the number of input features for the first input linear layer\n        \"\"\"\n        super(LinearSimpleLSTMModel, self).__init__(**kwargs)\n        self.former_linear = nn.Linear(linear_size, kwargs[\"input_size\"])\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass for the LinearSimpleLSTMModel.\n\n        Args:\n            x: Input tensor which will be passed through a linear layer first.\n\n        Returns:\n            The output of the underlying SimpleLSTM model.\n        \"\"\"\n        x0 = F.relu(self.former_linear(x))\n        return super(LinearSimpleLSTMModel, self).forward(x0)\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.LinearSimpleLSTMModel.__init__","title":"<code>__init__(self, linear_size, **kwargs)</code>  <code>special</code>","text":""},{"location":"api/models/#torchhydro.models.simple_lstm.LinearSimpleLSTMModel.__init__--parameters","title":"Parameters","text":"<p>linear_size     the number of input features for the first input linear layer</p> Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>def __init__(self, linear_size: int, **kwargs: Any):\n    \"\"\"\n\n    Parameters\n    ----------\n    linear_size\n        the number of input features for the first input linear layer\n    \"\"\"\n    super(LinearSimpleLSTMModel, self).__init__(**kwargs)\n    self.former_linear = nn.Linear(linear_size, kwargs[\"input_size\"])\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.LinearSimpleLSTMModel.forward","title":"<code>forward(self, x)</code>","text":"<p>Forward pass for the LinearSimpleLSTMModel.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor which will be passed through a linear layer first.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output of the underlying SimpleLSTM model.</p> Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass for the LinearSimpleLSTMModel.\n\n    Args:\n        x: Input tensor which will be passed through a linear layer first.\n\n    Returns:\n        The output of the underlying SimpleLSTM model.\n    \"\"\"\n    x0 = F.relu(self.former_linear(x))\n    return super(LinearSimpleLSTMModel, self).forward(x0)\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.MinLSTM","title":"<code> MinLSTM            (Module)         </code>","text":"<p>Only \"parallel mode\" is supported for conciseness. use log space. Written by Yang Wang</p> <p>https://arxiv.org/pdf/2410.01201 https://github.com/axion66/minLSTM-implementation</p> <p>input shape: [batch, seq_len, in_chn] output shape: [batch,seq_len, out_chn]</p> Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>class MinLSTM(nn.Module):\n    \"\"\"\n    Only \"parallel mode\" is supported for conciseness.\n    use log space.\n    Written by Yang Wang\n\n    https://arxiv.org/pdf/2410.01201\n    https://github.com/axion66/minLSTM-implementation\n\n    input shape: [batch, seq_len, in_chn]\n    output shape: [batch,seq_len, out_chn]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        device: Optional[torch.device] = None,\n        dtype: Optional[torch.dtype] = None,\n    ):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.linear = nn.Linear(\n            input_size, hidden_size * 3, bias=False, device=device, dtype=dtype\n        )\n\n    def forward(\n        self, x_t: torch.Tensor, h_prev: Optional[torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        if h_prev is None:\n            h_prev = torch.zeros(\n                x_t.size(0), self.hidden_size, device=x_t.device, dtype=x_t.dtype\n            )\n        seq_len = x_t.shape[1]\n        f, i, h = torch.chunk(self.linear(x_t), chunks=3, dim=-1)\n        diff = F.softplus(-f) - F.softplus(-i)\n        log_f = -F.softplus(diff)\n        log_i = -F.softplus(-diff)\n        log_h_0 = self.log_g(h_prev)\n        log_tilde_h = self.log_g(h)\n        log_coeff = log_f.unsqueeze(1)\n        log_val = torch.cat([log_h_0.unsqueeze(1), (log_i + log_tilde_h)], dim=1)\n        h_t = self.parallel_scan_log(log_coeff, log_val)\n        return h_t[:, -seq_len:]\n\n    def parallel_scan_log(\n        self, log_coeffs: torch.Tensor, log_values: torch.Tensor\n    ) -&gt; torch.Tensor:\n        a_star = F.pad(torch.cumsum(log_coeffs, dim=1), (0, 0, 1, 0)).squeeze(1)\n        log_h0_plus_b_star = torch.logcumsumexp(log_values - a_star, dim=1).squeeze(1)\n        log_h = a_star + log_h0_plus_b_star\n        return torch.exp(log_h)  # will return [batch, seq + 1, chn]\n\n    def g(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return torch.where(x &gt;= 0, x + 0.5, torch.sigmoid(x))\n\n    def log_g(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return torch.where(x &gt;= 0, (F.relu(x) + 0.5).log(), -F.softplus(-x))\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.MinLSTM.forward","title":"<code>forward(self, x_t, h_prev=None)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>def forward(\n    self, x_t: torch.Tensor, h_prev: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    if h_prev is None:\n        h_prev = torch.zeros(\n            x_t.size(0), self.hidden_size, device=x_t.device, dtype=x_t.dtype\n        )\n    seq_len = x_t.shape[1]\n    f, i, h = torch.chunk(self.linear(x_t), chunks=3, dim=-1)\n    diff = F.softplus(-f) - F.softplus(-i)\n    log_f = -F.softplus(diff)\n    log_i = -F.softplus(-diff)\n    log_h_0 = self.log_g(h_prev)\n    log_tilde_h = self.log_g(h)\n    log_coeff = log_f.unsqueeze(1)\n    log_val = torch.cat([log_h_0.unsqueeze(1), (log_i + log_tilde_h)], dim=1)\n    h_t = self.parallel_scan_log(log_coeff, log_val)\n    return h_t[:, -seq_len:]\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.MultiLayerLSTM","title":"<code> MultiLayerLSTM            (Module)         </code>","text":"Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>class MultiLayerLSTM(nn.Module):\n    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        hidden_size: int,\n        num_layers: int = 1,\n        dr: float = 0.0,\n    ):\n        super(MultiLayerLSTM, self).__init__()\n        self.linearIn = nn.Linear(input_size, hidden_size)\n        self.lstm = nn.LSTM(\n            hidden_size,\n            hidden_size,\n            num_layers,\n            dropout=dr,\n        )\n        self.linearOut = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x0 = F.relu(self.linearIn(x))\n        out_lstm, (hn, cn) = self.lstm(x0)\n        return self.linearOut(out_lstm)\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.MultiLayerLSTM.forward","title":"<code>forward(self, x)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    x0 = F.relu(self.linearIn(x))\n    out_lstm, (hn, cn) = self.lstm(x0)\n    return self.linearOut(out_lstm)\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.SimpleLSTM","title":"<code> SimpleLSTM            (Module)         </code>","text":"Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>class SimpleLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, output_size: int, hidden_size: int, dr: float = 0.0\n    ):\n        super(SimpleLSTM, self).__init__()\n        self.linearIn = nn.Linear(input_size, hidden_size)\n        self.lstm = nn.LSTM(\n            hidden_size,\n            hidden_size,\n        )\n        self.dropout = nn.Dropout(p=dr)\n        self.linearOut = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x: torch.Tensor, **kwargs: Any) -&gt; torch.Tensor:\n        \"\"\"Forward pass of the SimpleLSTM model.\n\n        Args:\n            x: Input tensor.\n            **kwargs: Optional keyword arguments:\n                - seq_lengths: Sequence lengths for PackedSequence.\n                - mask: Mask tensor for manual masking.\n                - use_manual_mask: Prioritize manual masking over PackedSequence.\n\n        Returns:\n            The output tensor from the model.\n        \"\"\"\n        x0 = F.relu(self.linearIn(x))\n\n        # Extract parameters from kwargs\n        seq_lengths = kwargs.get(\"seq_lengths\", None)\n        mask = kwargs.get(\"mask\", None)\n        use_manual_mask = kwargs.get(\"use_manual_mask\", False)\n\n        # Determine processing method based on available parameters\n        if use_manual_mask and mask is not None:\n            # Use manual masking\n            out_lstm, (hn, cn) = self.lstm(x0)\n\n            # Apply mask to LSTM output\n            # Ensure mask has the correct shape for broadcasting\n            if mask.dim() == 2:  # [batch_size, seq_len]\n                # Convert to [seq_len, batch_size, 1] for seq_first format\n                mask = mask.transpose(0, 1).unsqueeze(-1)\n            elif mask.dim() == 3 and mask.size(-1) != 1:\n                # If mask is [seq_len, batch_size, features], take only the first feature\n                mask = mask[:, :, :1]\n\n            # Apply mask: set masked positions to zero\n            out_lstm = out_lstm * mask\n\n        elif seq_lengths is not None and not use_manual_mask:\n            # Use PackedSequence (original behavior)\n            packed_x = pack_padded_sequence(\n                x0, seq_lengths, batch_first=False, enforce_sorted=False\n            )\n            packed_out, (hn, cn) = self.lstm(packed_x)\n            out_lstm, _ = pad_packed_sequence(packed_out, batch_first=False)\n\n        else:\n            # Standard processing without masking\n            out_lstm, (hn, cn) = self.lstm(x0)\n\n            # Apply mask if provided (even without use_manual_mask flag)\n            if mask is not None:\n                if mask.dim() == 2:  # [batch_size, seq_len]\n                    mask = mask.transpose(0, 1).unsqueeze(-1)\n                elif mask.dim() == 3 and mask.size(-1) != 1:\n                    mask = mask[:, :, :1]\n                out_lstm = out_lstm * mask\n\n        out_lstm_dr = self.dropout(out_lstm)\n        return self.linearOut(out_lstm_dr)\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.SimpleLSTM.forward","title":"<code>forward(self, x, **kwargs)</code>","text":"<p>Forward pass of the SimpleLSTM model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>**kwargs</code> <code>Any</code> <p>Optional keyword arguments: - seq_lengths: Sequence lengths for PackedSequence. - mask: Mask tensor for manual masking. - use_manual_mask: Prioritize manual masking over PackedSequence.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor from the model.</p> Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>def forward(self, x: torch.Tensor, **kwargs: Any) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the SimpleLSTM model.\n\n    Args:\n        x: Input tensor.\n        **kwargs: Optional keyword arguments:\n            - seq_lengths: Sequence lengths for PackedSequence.\n            - mask: Mask tensor for manual masking.\n            - use_manual_mask: Prioritize manual masking over PackedSequence.\n\n    Returns:\n        The output tensor from the model.\n    \"\"\"\n    x0 = F.relu(self.linearIn(x))\n\n    # Extract parameters from kwargs\n    seq_lengths = kwargs.get(\"seq_lengths\", None)\n    mask = kwargs.get(\"mask\", None)\n    use_manual_mask = kwargs.get(\"use_manual_mask\", False)\n\n    # Determine processing method based on available parameters\n    if use_manual_mask and mask is not None:\n        # Use manual masking\n        out_lstm, (hn, cn) = self.lstm(x0)\n\n        # Apply mask to LSTM output\n        # Ensure mask has the correct shape for broadcasting\n        if mask.dim() == 2:  # [batch_size, seq_len]\n            # Convert to [seq_len, batch_size, 1] for seq_first format\n            mask = mask.transpose(0, 1).unsqueeze(-1)\n        elif mask.dim() == 3 and mask.size(-1) != 1:\n            # If mask is [seq_len, batch_size, features], take only the first feature\n            mask = mask[:, :, :1]\n\n        # Apply mask: set masked positions to zero\n        out_lstm = out_lstm * mask\n\n    elif seq_lengths is not None and not use_manual_mask:\n        # Use PackedSequence (original behavior)\n        packed_x = pack_padded_sequence(\n            x0, seq_lengths, batch_first=False, enforce_sorted=False\n        )\n        packed_out, (hn, cn) = self.lstm(packed_x)\n        out_lstm, _ = pad_packed_sequence(packed_out, batch_first=False)\n\n    else:\n        # Standard processing without masking\n        out_lstm, (hn, cn) = self.lstm(x0)\n\n        # Apply mask if provided (even without use_manual_mask flag)\n        if mask is not None:\n            if mask.dim() == 2:  # [batch_size, seq_len]\n                mask = mask.transpose(0, 1).unsqueeze(-1)\n            elif mask.dim() == 3 and mask.size(-1) != 1:\n                mask = mask[:, :, :1]\n            out_lstm = out_lstm * mask\n\n    out_lstm_dr = self.dropout(out_lstm)\n    return self.linearOut(out_lstm_dr)\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.SimpleLSTMForecast","title":"<code> SimpleLSTMForecast            (SimpleLSTM)         </code>","text":"Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>class SimpleLSTMForecast(SimpleLSTM):\n    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        hidden_size: int,\n        forecast_length: int,\n        dr: float = 0.0,\n    ):\n        super(SimpleLSTMForecast, self).__init__(\n            input_size, output_size, hidden_size, dr\n        )\n        self.forecast_length = forecast_length\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass for the SimpleLSTMForecast model.\n\n        This method calls the parent's forward method and returns only the\n        final part of the output sequence corresponding to the forecast length.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            A tensor containing the forecast part of the output sequence.\n        \"\"\"\n        # \u8c03\u7528\u7236\u7c7b\u7684forward\u65b9\u6cd5\u83b7\u53d6\u5b8c\u6574\u7684\u8f93\u51fa\n        full_output = super(SimpleLSTMForecast, self).forward(x)\n\n        return full_output[-self.forecast_length :, :, :]\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.SimpleLSTMForecast.forward","title":"<code>forward(self, x)</code>","text":"<p>Forward pass for the SimpleLSTMForecast model.</p> <p>This method calls the parent's forward method and returns only the final part of the output sequence corresponding to the forecast length.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the forecast part of the output sequence.</p> Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass for the SimpleLSTMForecast model.\n\n    This method calls the parent's forward method and returns only the\n    final part of the output sequence corresponding to the forecast length.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        A tensor containing the forecast part of the output sequence.\n    \"\"\"\n    # \u8c03\u7528\u7236\u7c7b\u7684forward\u65b9\u6cd5\u83b7\u53d6\u5b8c\u6574\u7684\u8f93\u51fa\n    full_output = super(SimpleLSTMForecast, self).forward(x)\n\n    return full_output[-self.forecast_length :, :, :]\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.SlowLSTM","title":"<code> SlowLSTM            (Module)         </code>","text":"<p>A pedagogic implementation of Hochreiter &amp; Schmidhuber: 'Long-Short Term Memory' http://www.bioinf.jku.at/publications/older/2604.pdf</p> Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>class SlowLSTM(nn.Module):\n    \"\"\"\n    A pedagogic implementation of Hochreiter &amp; Schmidhuber:\n    'Long-Short Term Memory'\n    http://www.bioinf.jku.at/publications/older/2604.pdf\n    \"\"\"\n\n    def __init__(\n        self, input_size: int, hidden_size: int, bias: bool = True, dropout: float = 0.0\n    ):\n        super(SlowLSTM, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.dropout = dropout\n        # input to hidden weights\n        self.w_xi = P(T(hidden_size, input_size))\n        self.w_xf = P(T(hidden_size, input_size))\n        self.w_xo = P(T(hidden_size, input_size))\n        self.w_xc = P(T(hidden_size, input_size))\n        # hidden to hidden weights\n        self.w_hi = P(T(hidden_size, hidden_size))\n        self.w_hf = P(T(hidden_size, hidden_size))\n        self.w_ho = P(T(hidden_size, hidden_size))\n        self.w_hc = P(T(hidden_size, hidden_size))\n        # bias terms\n        self.b_i = T(hidden_size).fill_(0)\n        self.b_f = T(hidden_size).fill_(0)\n        self.b_o = T(hidden_size).fill_(0)\n        self.b_c = T(hidden_size).fill_(0)\n\n        # Wrap biases as parameters if desired, else as variables without gradients\n        W = P if bias else (lambda x: P(x, requires_grad=False))\n        self.b_i = W(self.b_i)\n        self.b_f = W(self.b_f)\n        self.b_o = W(self.b_o)\n        self.b_c = W(self.b_c)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        std = 1.0 / math.sqrt(self.hidden_size)\n        for w in self.parameters():\n            w.data.uniform_(-std, std)\n\n    def forward(\n        self, x: torch.Tensor, hidden: Tuple[torch.Tensor, torch.Tensor]\n    ) -&gt; Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        h, c = hidden\n        h = h.view(h.size(0), -1)\n        c = c.view(h.size(0), -1)\n        x = x.view(x.size(0), -1)\n        # Linear mappings\n        i_t = th.mm(x, self.w_xi) + th.mm(h, self.w_hi) + self.b_i\n        f_t = th.mm(x, self.w_xf) + th.mm(h, self.w_hf) + self.b_f\n        o_t = th.mm(x, self.w_xo) + th.mm(h, self.w_ho) + self.b_o\n        # activations\n        i_t.sigmoid_()\n        f_t.sigmoid_()\n        o_t.sigmoid_()\n        # cell computations\n        c_t = th.mm(x, self.w_xc) + th.mm(h, self.w_hc) + self.b_c\n        c_t.tanh_()\n        c_t = th.mul(c, f_t) + th.mul(i_t, c_t)\n        h_t = th.mul(o_t, th.tanh(c_t))\n        # Reshape for compatibility\n        h_t = h_t.view(h_t.size(0), 1, -1)\n        c_t = c_t.view(c_t.size(0), 1, -1)\n        if self.dropout &gt; 0.0:\n            F.dropout(h_t, p=self.dropout, training=self.training, inplace=True)\n        return h_t, (h_t, c_t)\n\n    def sample_mask(self) -&gt; None:\n        pass\n</code></pre>"},{"location":"api/models/#torchhydro.models.simple_lstm.SlowLSTM.forward","title":"<code>forward(self, x, hidden)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/simple_lstm.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, hidden: Tuple[torch.Tensor, torch.Tensor]\n) -&gt; Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n    h, c = hidden\n    h = h.view(h.size(0), -1)\n    c = c.view(h.size(0), -1)\n    x = x.view(x.size(0), -1)\n    # Linear mappings\n    i_t = th.mm(x, self.w_xi) + th.mm(h, self.w_hi) + self.b_i\n    f_t = th.mm(x, self.w_xf) + th.mm(h, self.w_hf) + self.b_f\n    o_t = th.mm(x, self.w_xo) + th.mm(h, self.w_ho) + self.b_o\n    # activations\n    i_t.sigmoid_()\n    f_t.sigmoid_()\n    o_t.sigmoid_()\n    # cell computations\n    c_t = th.mm(x, self.w_xc) + th.mm(h, self.w_hc) + self.b_c\n    c_t.tanh_()\n    c_t = th.mul(c, f_t) + th.mul(i_t, c_t)\n    h_t = th.mul(o_t, th.tanh(c_t))\n    # Reshape for compatibility\n    h_t = h_t.view(h_t.size(0), 1, -1)\n    c_t = c_t.view(c_t.size(0), 1, -1)\n    if self.dropout &gt; 0.0:\n        F.dropout(h_t, p=self.dropout, training=self.training, inplace=True)\n    return h_t, (h_t, c_t)\n</code></pre>"},{"location":"api/models/#torchhydro.models.spplstm","title":"<code>spplstm</code>","text":"<p>Author: Xinzhuo Wu Date: 2023-09-30 1:20:18 LastEditTime: 2024-05-27 16:26:06 LastEditors: Wenyu Ouyang Description: spp lstm model FilePath:       orchhydro       orchhydro\\models\\spplstm.py Copyright (c) 2021-2022 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/models/#torchhydro.models.spplstm.SPP_LSTM_Model","title":"<code> SPP_LSTM_Model            (Module)         </code>","text":"Source code in <code>torchhydro/models/spplstm.py</code> <pre><code>class SPP_LSTM_Model(nn.Module):\n    def __init__(\n        self, hindcast_length, forecast_length, n_output, n_hidden_states, dropout\n    ):\n        super(SPP_LSTM_Model, self).__init__()\n\n        self.conv1 = TimeDistributed(\n            nn.Conv2d(\n                in_channels=1,\n                out_channels=32,\n                kernel_size=1,\n                padding=\"same\",\n            )\n        )\n\n        self.conv2 = TimeDistributed(\n            nn.Conv2d(\n                in_channels=32,\n                out_channels=16,\n                kernel_size=3,\n                padding=\"same\",\n                bias=True,\n            )\n        )\n\n        self.conv3 = TimeDistributed(\n            nn.Conv2d(\n                in_channels=16,\n                out_channels=16,\n                kernel_size=3,\n                padding=\"same\",\n                bias=True,\n            )\n        )\n\n        self.maxpool1 = TimeDistributed(nn.MaxPool2d(kernel_size=2, stride=(2, 2)))\n\n        self.conv4 = TimeDistributed(\n            nn.Conv2d(\n                in_channels=16,\n                out_channels=32,\n                kernel_size=3,\n                padding=\"same\",\n                bias=True,\n            )\n        )\n\n        self.conv5 = TimeDistributed(\n            nn.Conv2d(\n                in_channels=32,\n                out_channels=32,\n                kernel_size=3,\n                padding=\"same\",\n                bias=True,\n            )\n        )\n\n        self.maxpool2 = TimeDistributed(SppLayer([4, 2, 1]))\n\n        self.dropout = nn.Dropout(dropout)\n\n        self.lstm = nn.LSTM(\n            input_size=21 * 32, hidden_size=n_hidden_states, batch_first=True\n        )\n\n        self.dense = nn.Linear(in_features=n_hidden_states, out_features=n_output)\n\n        self.forecast_length = forecast_length\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = torch.relu(x)\n        x = self.conv2(x)\n        x = torch.relu(x)\n        x = self.conv3(x)\n        x = torch.relu(x)\n        x = self.maxpool1(x)\n        x = self.conv4(x)\n        x = torch.relu(x)\n        x = self.conv5(x)\n        x = torch.relu(x)\n        x = self.maxpool2(x)\n        x = self.dropout(x)\n        x = x.view(x.shape[0], x.shape[1], -1)\n        x, _ = self.lstm(x)\n        x = self.dense(x)\n        x = x[:, -self.forecast_length :, :]\n        return x\n</code></pre>"},{"location":"api/models/#torchhydro.models.spplstm.SPP_LSTM_Model.forward","title":"<code>forward(self, x)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/spplstm.py</code> <pre><code>def forward(self, x):\n    x = self.conv1(x)\n    x = torch.relu(x)\n    x = self.conv2(x)\n    x = torch.relu(x)\n    x = self.conv3(x)\n    x = torch.relu(x)\n    x = self.maxpool1(x)\n    x = self.conv4(x)\n    x = torch.relu(x)\n    x = self.conv5(x)\n    x = torch.relu(x)\n    x = self.maxpool2(x)\n    x = self.dropout(x)\n    x = x.view(x.shape[0], x.shape[1], -1)\n    x, _ = self.lstm(x)\n    x = self.dense(x)\n    x = x[:, -self.forecast_length :, :]\n    return x\n</code></pre>"},{"location":"api/models/#torchhydro.models.spplstm.SPP_LSTM_Model_2","title":"<code> SPP_LSTM_Model_2            (Module)         </code>","text":"Source code in <code>torchhydro/models/spplstm.py</code> <pre><code>class SPP_LSTM_Model_2(nn.Module):\n    def __init__(\n        self,\n        hindcast_length,\n        forecast_length,\n        p_n_output,\n        p_n_hidden_states,\n        p_dropout,\n        p_in_channels,\n        p_out_channels,\n        len_c=None,\n        s_hindcast_length=None,\n        s_n_output=None,\n        s_n_hidden_states=None,\n        s_dropout=None,\n        s_in_channels=None,\n        s_out_channels=None,\n    ):\n        \"\"\"Initializes the SPP_LSTM_Model_2.\n\n        A custom neural network model for handling and integrating various types\n        of meteorological and geographical data, including precipitation (p),\n        soil (s), and basin attributes (c).\n\n        Args:\n            hindcast_length: The length of the input sequence for precipitation data.\n            forecast_length: The length of the forecast period.\n            p_n_output: Output dimension for the precipitation (p) data path.\n            p_n_hidden_states: Number of hidden states in the LSTM for the precipitation path.\n            p_dropout: Dropout rate applied in the precipitation path.\n            p_in_channels: Number of input channels for the conv layer in the precipitation path.\n            p_out_channels: Number of output channels for the conv layer in the precipitation path.\n            len_c: Optional, the number of basin attribute (c) features.\n            s_hindcast_length: Optional, hindcast length for the soil (s) data path.\n            s_n_output: Optional, output dimension for the soil path.\n            s_n_hidden_states: Optional, number of hidden states for the soil path LSTM.\n            s_dropout: Optional, dropout rate for the soil path.\n            s_in_channels: Optional, input channels for the soil path conv layer.\n            s_out_channels: Optional, output channels for the soil path conv layer.\n        \"\"\"\n        super(SPP_LSTM_Model_2, self).__init__()\n        self.conv_p = nn.Conv2d(\n            in_channels=p_in_channels,\n            out_channels=p_out_channels,\n            kernel_size=(3, 3),\n            padding=\"same\",\n        )\n\n        self.leaky_relu_p = nn.LeakyReLU(0.01)\n\n        self.lstm_p = nn.LSTM(\n            input_size=p_out_channels * 5 + len_c,\n            hidden_size=p_n_hidden_states,\n            batch_first=True,\n        )\n\n        self.dropout_p = nn.Dropout(p_dropout)\n\n        self.fc_p = nn.Linear(p_n_hidden_states, p_n_output)\n\n        self.spp_p = SppLayer([2, 1])\n\n        self.p_length = hindcast_length + forecast_length\n        self.forecast_length = forecast_length\n\n        if s_hindcast_length is not None:\n            self.conv_s = nn.Conv2d(\n                in_channels=s_in_channels,\n                out_channels=s_out_channels,\n                kernel_size=(3, 3),\n                padding=\"same\",\n            )\n\n            self.leaky_relu_s = nn.LeakyReLU(0.01)\n            self.sigmoid_s = nn.Sigmoid()\n\n            self.lstm_s = nn.LSTM(\n                input_size=s_out_channels * 5,\n                hidden_size=s_n_hidden_states,\n                batch_first=True,\n            )\n\n            self.dropout_s = nn.Dropout(s_dropout)\n\n            self.fc_s = nn.Linear(s_n_hidden_states, s_n_output)\n\n            self.spp_s = SppLayer([2, 1])\n\n            self.s_length = s_hindcast_length\n\n    def forward(self, *x_lst):\n        # c and s must be None, g might be None\n        if len(x_lst) == 1:\n            x = x_lst[0]\n            x = x.view(-1, x.shape[2], x.shape[3], x.shape[4])\n            x = self.conv_p(x)\n            x = self.leaky_relu_p(x)\n            x = self.spp_p(x)\n            x = x.view(x.shape[0], -1)\n            x = x.view(int(x.shape[0] / (self.p_length)), self.p_length, -1)\n            x, _ = self.lstm_p(x)\n            x = self.dropout_p(x)\n            x = self.fc_p(x)\n        # g might be None. either c or s must be None, but not both\n        elif len(x_lst) == 2:\n            p = x_lst[0]\n            m = x_lst[1].permute(1, 0, 2)\n            # c is not None\n            if m.dim() == 3:\n                p = p.view(-1, p.shape[2], p.shape[3], p.shape[4])\n                p = self.conv_p(p)\n                p = self.leaky_relu_p(p)\n                p = self.spp_p(p)\n                p = p.view(p.shape[0], -1)\n                p = p.view(int(p.shape[0] / (self.p_length)), self.p_length, -1)\n                x = torch.cat([p, m], dim=2)\n                x, _ = self.lstm_p(x)\n                x = self.dropout_p(x)\n                x = self.fc_p(x)\n            # s is not None\n            else:\n                p = p.view(-1, p.shape[2], p.shape[3], p.shape[4])\n                p = self.conv_p(p)\n                p = self.leaky_relu_p(p)\n                p = self.spp_p(p)\n                p = p.view(p.shape[0], -1)\n                p = p.view(int(p.shape[0] / (self.p_length)), self.p_length, -1)\n                p, _ = self.lstm_p(p)\n                p = self.dropout_p(p)\n                p = self.fc_p(p)\n\n                m = m.view(-1, m.shape[2], m.shape[3], m.shape[4])\n                m = self.conv_s(m)\n                m = self.leaky_relu_s(m)\n                m = self.spp_s(m)\n                m = m.view(m.shape[0], -1)\n                m = m.view(int(m.shape[0] / (self.s_length)), self.s_length, -1)\n                m, _ = self.lstm_s(m)\n                m = m[:, -1:, :]\n                m = self.dropout_s(m)\n                m = self.fc_s(m)\n                m = self.sigmoid_s(m)\n\n                x = m * p\n        # g might be None. Both s and c are not None\n        elif len(x_lst) == 3:\n            p = x_lst[0]\n            c = x_lst[1].permute(1, 0, 2)\n            s = x_lst[2]\n\n            p = p.view(-1, p.shape[2], p.shape[3], p.shape[4])\n            p = self.conv_p(p)\n            p = self.leaky_relu_p(p)\n            p = self.spp_p(p)\n            p = p.view(p.shape[0], -1)\n            p = p.view(int(p.shape[0] / (self.p_length)), self.p_length, -1)\n            p_c = torch.cat([p, c], dim=2)\n            p_c, _ = self.lstm_p(p_c)\n            p_c = self.dropout_p(p_c)\n            p_c = self.fc_p(p_c)\n\n            s = s.view(-1, s.shape[2], s.shape[3], s.shape[4])\n            s = self.conv_s(s)\n            s = self.leaky_relu_s(s)\n            s = self.spp_s(s)\n            s = s.view(s.shape[0], -1)\n            s = s.view(int(s.shape[0] / (self.s_length)), self.s_length, -1)\n            s, _ = self.lstm_s(s)\n            s = s[:, -1:, :]\n            s = self.dropout_s(s)\n            s = self.fc_s(s)\n            s = self.sigmoid_s(s)\n\n            x = s * p_c\n        return x[:, -self.forecast_length :, :]\n</code></pre>"},{"location":"api/models/#torchhydro.models.spplstm.SPP_LSTM_Model_2.__init__","title":"<code>__init__(self, hindcast_length, forecast_length, p_n_output, p_n_hidden_states, p_dropout, p_in_channels, p_out_channels, len_c=None, s_hindcast_length=None, s_n_output=None, s_n_hidden_states=None, s_dropout=None, s_in_channels=None, s_out_channels=None)</code>  <code>special</code>","text":"<p>Initializes the SPP_LSTM_Model_2.</p> <p>A custom neural network model for handling and integrating various types of meteorological and geographical data, including precipitation (p), soil (s), and basin attributes (c).</p> <p>Parameters:</p> Name Type Description Default <code>hindcast_length</code> <p>The length of the input sequence for precipitation data.</p> required <code>forecast_length</code> <p>The length of the forecast period.</p> required <code>p_n_output</code> <p>Output dimension for the precipitation (p) data path.</p> required <code>p_n_hidden_states</code> <p>Number of hidden states in the LSTM for the precipitation path.</p> required <code>p_dropout</code> <p>Dropout rate applied in the precipitation path.</p> required <code>p_in_channels</code> <p>Number of input channels for the conv layer in the precipitation path.</p> required <code>p_out_channels</code> <p>Number of output channels for the conv layer in the precipitation path.</p> required <code>len_c</code> <p>Optional, the number of basin attribute (c) features.</p> <code>None</code> <code>s_hindcast_length</code> <p>Optional, hindcast length for the soil (s) data path.</p> <code>None</code> <code>s_n_output</code> <p>Optional, output dimension for the soil path.</p> <code>None</code> <code>s_n_hidden_states</code> <p>Optional, number of hidden states for the soil path LSTM.</p> <code>None</code> <code>s_dropout</code> <p>Optional, dropout rate for the soil path.</p> <code>None</code> <code>s_in_channels</code> <p>Optional, input channels for the soil path conv layer.</p> <code>None</code> <code>s_out_channels</code> <p>Optional, output channels for the soil path conv layer.</p> <code>None</code> Source code in <code>torchhydro/models/spplstm.py</code> <pre><code>def __init__(\n    self,\n    hindcast_length,\n    forecast_length,\n    p_n_output,\n    p_n_hidden_states,\n    p_dropout,\n    p_in_channels,\n    p_out_channels,\n    len_c=None,\n    s_hindcast_length=None,\n    s_n_output=None,\n    s_n_hidden_states=None,\n    s_dropout=None,\n    s_in_channels=None,\n    s_out_channels=None,\n):\n    \"\"\"Initializes the SPP_LSTM_Model_2.\n\n    A custom neural network model for handling and integrating various types\n    of meteorological and geographical data, including precipitation (p),\n    soil (s), and basin attributes (c).\n\n    Args:\n        hindcast_length: The length of the input sequence for precipitation data.\n        forecast_length: The length of the forecast period.\n        p_n_output: Output dimension for the precipitation (p) data path.\n        p_n_hidden_states: Number of hidden states in the LSTM for the precipitation path.\n        p_dropout: Dropout rate applied in the precipitation path.\n        p_in_channels: Number of input channels for the conv layer in the precipitation path.\n        p_out_channels: Number of output channels for the conv layer in the precipitation path.\n        len_c: Optional, the number of basin attribute (c) features.\n        s_hindcast_length: Optional, hindcast length for the soil (s) data path.\n        s_n_output: Optional, output dimension for the soil path.\n        s_n_hidden_states: Optional, number of hidden states for the soil path LSTM.\n        s_dropout: Optional, dropout rate for the soil path.\n        s_in_channels: Optional, input channels for the soil path conv layer.\n        s_out_channels: Optional, output channels for the soil path conv layer.\n    \"\"\"\n    super(SPP_LSTM_Model_2, self).__init__()\n    self.conv_p = nn.Conv2d(\n        in_channels=p_in_channels,\n        out_channels=p_out_channels,\n        kernel_size=(3, 3),\n        padding=\"same\",\n    )\n\n    self.leaky_relu_p = nn.LeakyReLU(0.01)\n\n    self.lstm_p = nn.LSTM(\n        input_size=p_out_channels * 5 + len_c,\n        hidden_size=p_n_hidden_states,\n        batch_first=True,\n    )\n\n    self.dropout_p = nn.Dropout(p_dropout)\n\n    self.fc_p = nn.Linear(p_n_hidden_states, p_n_output)\n\n    self.spp_p = SppLayer([2, 1])\n\n    self.p_length = hindcast_length + forecast_length\n    self.forecast_length = forecast_length\n\n    if s_hindcast_length is not None:\n        self.conv_s = nn.Conv2d(\n            in_channels=s_in_channels,\n            out_channels=s_out_channels,\n            kernel_size=(3, 3),\n            padding=\"same\",\n        )\n\n        self.leaky_relu_s = nn.LeakyReLU(0.01)\n        self.sigmoid_s = nn.Sigmoid()\n\n        self.lstm_s = nn.LSTM(\n            input_size=s_out_channels * 5,\n            hidden_size=s_n_hidden_states,\n            batch_first=True,\n        )\n\n        self.dropout_s = nn.Dropout(s_dropout)\n\n        self.fc_s = nn.Linear(s_n_hidden_states, s_n_output)\n\n        self.spp_s = SppLayer([2, 1])\n\n        self.s_length = s_hindcast_length\n</code></pre>"},{"location":"api/models/#torchhydro.models.spplstm.SPP_LSTM_Model_2.forward","title":"<code>forward(self, *x_lst)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/spplstm.py</code> <pre><code>def forward(self, *x_lst):\n    # c and s must be None, g might be None\n    if len(x_lst) == 1:\n        x = x_lst[0]\n        x = x.view(-1, x.shape[2], x.shape[3], x.shape[4])\n        x = self.conv_p(x)\n        x = self.leaky_relu_p(x)\n        x = self.spp_p(x)\n        x = x.view(x.shape[0], -1)\n        x = x.view(int(x.shape[0] / (self.p_length)), self.p_length, -1)\n        x, _ = self.lstm_p(x)\n        x = self.dropout_p(x)\n        x = self.fc_p(x)\n    # g might be None. either c or s must be None, but not both\n    elif len(x_lst) == 2:\n        p = x_lst[0]\n        m = x_lst[1].permute(1, 0, 2)\n        # c is not None\n        if m.dim() == 3:\n            p = p.view(-1, p.shape[2], p.shape[3], p.shape[4])\n            p = self.conv_p(p)\n            p = self.leaky_relu_p(p)\n            p = self.spp_p(p)\n            p = p.view(p.shape[0], -1)\n            p = p.view(int(p.shape[0] / (self.p_length)), self.p_length, -1)\n            x = torch.cat([p, m], dim=2)\n            x, _ = self.lstm_p(x)\n            x = self.dropout_p(x)\n            x = self.fc_p(x)\n        # s is not None\n        else:\n            p = p.view(-1, p.shape[2], p.shape[3], p.shape[4])\n            p = self.conv_p(p)\n            p = self.leaky_relu_p(p)\n            p = self.spp_p(p)\n            p = p.view(p.shape[0], -1)\n            p = p.view(int(p.shape[0] / (self.p_length)), self.p_length, -1)\n            p, _ = self.lstm_p(p)\n            p = self.dropout_p(p)\n            p = self.fc_p(p)\n\n            m = m.view(-1, m.shape[2], m.shape[3], m.shape[4])\n            m = self.conv_s(m)\n            m = self.leaky_relu_s(m)\n            m = self.spp_s(m)\n            m = m.view(m.shape[0], -1)\n            m = m.view(int(m.shape[0] / (self.s_length)), self.s_length, -1)\n            m, _ = self.lstm_s(m)\n            m = m[:, -1:, :]\n            m = self.dropout_s(m)\n            m = self.fc_s(m)\n            m = self.sigmoid_s(m)\n\n            x = m * p\n    # g might be None. Both s and c are not None\n    elif len(x_lst) == 3:\n        p = x_lst[0]\n        c = x_lst[1].permute(1, 0, 2)\n        s = x_lst[2]\n\n        p = p.view(-1, p.shape[2], p.shape[3], p.shape[4])\n        p = self.conv_p(p)\n        p = self.leaky_relu_p(p)\n        p = self.spp_p(p)\n        p = p.view(p.shape[0], -1)\n        p = p.view(int(p.shape[0] / (self.p_length)), self.p_length, -1)\n        p_c = torch.cat([p, c], dim=2)\n        p_c, _ = self.lstm_p(p_c)\n        p_c = self.dropout_p(p_c)\n        p_c = self.fc_p(p_c)\n\n        s = s.view(-1, s.shape[2], s.shape[3], s.shape[4])\n        s = self.conv_s(s)\n        s = self.leaky_relu_s(s)\n        s = self.spp_s(s)\n        s = s.view(s.shape[0], -1)\n        s = s.view(int(s.shape[0] / (self.s_length)), self.s_length, -1)\n        s, _ = self.lstm_s(s)\n        s = s[:, -1:, :]\n        s = self.dropout_s(s)\n        s = self.fc_s(s)\n        s = self.sigmoid_s(s)\n\n        x = s * p_c\n    return x[:, -self.forecast_length :, :]\n</code></pre>"},{"location":"api/models/#torchhydro.models.spplstm.SppLayer","title":"<code> SppLayer            (Module)         </code>","text":"Source code in <code>torchhydro/models/spplstm.py</code> <pre><code>class SppLayer(nn.Module):\n    def __init__(self, out_pool_size):\n        \"\"\"\n        out_pool_size: a int vector of expected output size of max pooling layer\n        \"\"\"\n        super(SppLayer, self).__init__()\n        self.out_pool_size = out_pool_size\n        self.pools = []\n        for i in range(len(out_pool_size)):\n            pool_i = nn.AdaptiveMaxPool2d(out_pool_size[i])\n            self.pools.append(pool_i)\n\n    def forward(self, previous_conv):\n        \"\"\"\n        Parameters\n        ----------\n        previous_conv\n            a tensor vector of previous convolution layer\n\n        Returns\n        -------\n        torch.Tensor\n            a tensor vector with shape [1 x n] is the concentration of multi-level pooling\n        \"\"\"\n\n        num_sample = previous_conv.size(0)\n        channel_size = previous_conv.size(1)\n        out_pool_size = self.out_pool_size\n        for i in range(len(out_pool_size)):\n            maxpool = self.pools[i]\n            x = maxpool(previous_conv)\n            if i == 0:\n                spp = x.view(num_sample, channel_size, -1)\n            else:\n                spp = torch.cat((spp, x.view(num_sample, channel_size, -1)), -1)\n        return spp\n</code></pre>"},{"location":"api/models/#torchhydro.models.spplstm.SppLayer.__init__","title":"<code>__init__(self, out_pool_size)</code>  <code>special</code>","text":"<p>out_pool_size: a int vector of expected output size of max pooling layer</p> Source code in <code>torchhydro/models/spplstm.py</code> <pre><code>def __init__(self, out_pool_size):\n    \"\"\"\n    out_pool_size: a int vector of expected output size of max pooling layer\n    \"\"\"\n    super(SppLayer, self).__init__()\n    self.out_pool_size = out_pool_size\n    self.pools = []\n    for i in range(len(out_pool_size)):\n        pool_i = nn.AdaptiveMaxPool2d(out_pool_size[i])\n        self.pools.append(pool_i)\n</code></pre>"},{"location":"api/models/#torchhydro.models.spplstm.SppLayer.forward","title":"<code>forward(self, previous_conv)</code>","text":""},{"location":"api/models/#torchhydro.models.spplstm.SppLayer.forward--parameters","title":"Parameters","text":"<p>previous_conv     a tensor vector of previous convolution layer</p>"},{"location":"api/models/#torchhydro.models.spplstm.SppLayer.forward--returns","title":"Returns","text":"<p>torch.Tensor     a tensor vector with shape [1 x n] is the concentration of multi-level pooling</p> Source code in <code>torchhydro/models/spplstm.py</code> <pre><code>def forward(self, previous_conv):\n    \"\"\"\n    Parameters\n    ----------\n    previous_conv\n        a tensor vector of previous convolution layer\n\n    Returns\n    -------\n    torch.Tensor\n        a tensor vector with shape [1 x n] is the concentration of multi-level pooling\n    \"\"\"\n\n    num_sample = previous_conv.size(0)\n    channel_size = previous_conv.size(1)\n    out_pool_size = self.out_pool_size\n    for i in range(len(out_pool_size)):\n        maxpool = self.pools[i]\n        x = maxpool(previous_conv)\n        if i == 0:\n            spp = x.view(num_sample, channel_size, -1)\n        else:\n            spp = torch.cat((spp, x.view(num_sample, channel_size, -1)), -1)\n    return spp\n</code></pre>"},{"location":"api/models/#torchhydro.models.spplstm.TimeDistributed","title":"<code> TimeDistributed            (Module)         </code>","text":"Source code in <code>torchhydro/models/spplstm.py</code> <pre><code>class TimeDistributed(nn.Module):\n    def __init__(self, layer):\n        super(TimeDistributed, self).__init__()\n        self.layer = layer\n\n    def forward(self, x):\n        outputs = []\n        for t in range(x.size(1)):\n            xt = x[:, t, :]\n            output = self.layer(xt)\n            outputs.append(output.unsqueeze(1))\n        outputs = torch.cat(outputs, dim=1)\n        return outputs\n</code></pre>"},{"location":"api/models/#torchhydro.models.spplstm.TimeDistributed.forward","title":"<code>forward(self, x)</code>","text":"<p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>torchhydro/models/spplstm.py</code> <pre><code>def forward(self, x):\n    outputs = []\n    for t in range(x.size(1)):\n        xt = x[:, t, :]\n        output = self.layer(xt)\n        outputs.append(output.unsqueeze(1))\n    outputs = torch.cat(outputs, dim=1)\n    return outputs\n</code></pre>"},{"location":"api/trainers/","title":"Trainers API","text":""},{"location":"api/trainers/#torchhydro.trainers.deep_hydro","title":"<code>deep_hydro</code>","text":"<p>Author: Wenyu Ouyang Date: 2024-04-08 18:15:48 LastEditTime: 2025-07-13 16:25:31 LastEditors: Wenyu Ouyang Description: HydroDL model class FilePath:       orchhydro       orchhydro       rainers\\deep_hydro.py Copyright (c) 2024-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydro","title":"<code> DeepHydro            (DeepHydroInterface)         </code>","text":"<p>The Base Trainer class for Hydrological Deep Learning models</p> Source code in <code>torchhydro/trainers/deep_hydro.py</code> <pre><code>class DeepHydro(DeepHydroInterface):\n    \"\"\"\n    The Base Trainer class for Hydrological Deep Learning models\n    \"\"\"\n\n    def __init__(\n        self,\n        cfgs: Dict,\n        pre_model=None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        cfgs\n            configs for the model\n        pre_model\n            a pre-trained model, if it is not None,\n            we will use its weights to initialize this model\n            by default None\n        \"\"\"\n        super().__init__(cfgs)\n        # Initialize fabric based on configuration\n        self.fabric = create_fabric_wrapper(cfgs.get(\"training_cfgs\", {}))\n        self.pre_model = pre_model\n        self.model = self.fabric.setup_module(self.load_model())\n        if cfgs[\"training_cfgs\"][\"train_mode\"]:\n            self.traindataset = self.make_dataset(\"train\")\n            if cfgs[\"data_cfgs\"][\"t_range_valid\"] is not None:\n                self.validdataset = self.make_dataset(\"valid\")\n        self.testdataset: BaseDataset = self.make_dataset(\"test\")\n\n    @property\n    def device(self):\n        \"\"\"Get the device from fabric wrapper\"\"\"\n        return self.fabric._device\n\n    def load_model(self, mode=\"train\"):\n        \"\"\"\n        Load a time series forecast model in pytorch_model_dict in model_dict_function.py\n\n        Returns\n        -------\n        object\n            model in pytorch_model_dict in model_dict_function.py\n        \"\"\"\n        if mode == \"infer\":\n            if self.weight_path is None or self.cfgs[\"model_cfgs\"][\"continue_train\"]:\n                # if no weight path is provided\n                # or weight file is provided but continue train again,\n                # we will use the trained model in the new case_dir directory\n                self.weight_path = self._get_trained_model()\n        elif mode != \"train\":\n            raise ValueError(\"Invalid mode; must be 'train' or 'infer'\")\n        model_cfgs = self.cfgs[\"model_cfgs\"]\n        model_name = model_cfgs[\"model_name\"]\n        if model_name not in pytorch_model_dict:\n            raise NotImplementedError(\n                f\"Error the model {model_name} was not found in the model dict. Please add it.\"\n            )\n        if self.pre_model is not None:\n            return self._load_pretrain_model()\n        elif self.weight_path is not None:\n            return self._load_model_from_pth()\n        else:\n            return pytorch_model_dict[model_name](**model_cfgs[\"model_hyperparam\"])\n\n    def _load_pretrain_model(self):\n        \"\"\"load a pretrained model as the initial model\"\"\"\n        return self.pre_model\n\n    def _load_model_from_pth(self):\n        weight_path = self.weight_path\n        model_cfgs = self.cfgs[\"model_cfgs\"]\n        model_name = model_cfgs[\"model_name\"]\n        model = pytorch_model_dict[model_name](**model_cfgs[\"model_hyperparam\"])\n        checkpoint = torch.load(weight_path, map_location=self.device)\n        model.load_state_dict(checkpoint)\n        print(\"Weights sucessfully loaded\")\n        return model\n\n    def make_dataset(self, is_tra_val_te: str):\n        \"\"\"\n        Initializes a pytorch dataset.\n\n        Parameters\n        ----------\n        is_tra_val_te\n            train or valid or test\n\n        Returns\n        -------\n        object\n            an object initializing from class in datasets_dict in data_dict.py\n        \"\"\"\n        data_cfgs = self.cfgs[\"data_cfgs\"]\n        dataset_name = data_cfgs[\"dataset\"]\n\n        if dataset_name in list(datasets_dict.keys()):\n            dataset = datasets_dict[dataset_name](self.cfgs, is_tra_val_te)\n        else:\n            raise NotImplementedError(\n                f\"Error the dataset {str(dataset_name)} was not found in the dataset dict. Please add it.\"\n            )\n        return dataset\n\n    def model_train(self) -&gt; None:\n        \"\"\"train a hydrological DL model\"\"\"\n        # A dictionary of the necessary parameters for training\n        training_cfgs = self.cfgs[\"training_cfgs\"]\n        # The file path to load model weights from; defaults to \"model_save\"\n        model_filepath = self.cfgs[\"data_cfgs\"][\"case_dir\"]\n        data_cfgs = self.cfgs[\"data_cfgs\"]\n        es = None\n        if training_cfgs[\"early_stopping\"]:\n            es = EarlyStopper(training_cfgs[\"patience\"])\n        criterion = self._get_loss_func(training_cfgs)\n        opt = self._get_optimizer(training_cfgs)\n        scheduler = self._get_scheduler(training_cfgs, opt)\n        max_epochs = training_cfgs[\"epochs\"]\n        start_epoch = training_cfgs[\"start_epoch\"]\n        # use PyTorch's DataLoader to load the data into batches in each epoch\n        data_loader, validation_data_loader = self._get_dataloader(\n            training_cfgs, data_cfgs\n        )\n        logger = TrainLogger(model_filepath, self.cfgs, opt)\n        for epoch in range(start_epoch, max_epochs + 1):\n            with logger.log_epoch_train(epoch) as train_logs:\n                total_loss, n_iter_ep = torch_single_train(\n                    self.model,\n                    opt,\n                    criterion,\n                    data_loader,\n                    device=self.device,\n                    which_first_tensor=training_cfgs[\"which_first_tensor\"],\n                )\n                train_logs[\"train_loss\"] = total_loss\n                train_logs[\"model\"] = self.model\n\n            valid_loss = None\n            valid_metrics = None\n            if data_cfgs[\"t_range_valid\"] is not None:\n                with logger.log_epoch_valid(epoch) as valid_logs:\n                    valid_loss, valid_metrics = self._1epoch_valid(\n                        training_cfgs, criterion, validation_data_loader, valid_logs\n                    )\n\n            self._scheduler_step(training_cfgs, scheduler, valid_loss)\n            logger.save_session_param(\n                epoch, total_loss, n_iter_ep, valid_loss, valid_metrics\n            )\n            logger.save_model_and_params(self.model, epoch, self.cfgs)\n            if es and not es.check_loss(\n                self.model,\n                valid_loss,\n                self.cfgs[\"data_cfgs\"][\"case_dir\"],\n            ):\n                print(\"Stopping model now\")\n                break\n        # logger.plot_model_structure(self.model)\n        logger.tb.close()\n\n        # return the trained model weights and bias and the epoch loss\n        return self.model.state_dict(), sum(logger.epoch_loss) / len(logger.epoch_loss)\n\n    def _get_scheduler(self, training_cfgs, opt):\n        lr_scheduler_cfg = training_cfgs[\"lr_scheduler\"]\n\n        if \"lr\" in lr_scheduler_cfg and \"lr_factor\" not in lr_scheduler_cfg:\n            scheduler = LambdaLR(opt, lr_lambda=lambda epoch: 1.0)\n        elif isinstance(lr_scheduler_cfg, dict) and all(\n            isinstance(epoch, int) for epoch in lr_scheduler_cfg\n        ):\n            # piecewise constant learning rate\n            epochs = sorted(lr_scheduler_cfg.keys())\n            values = [lr_scheduler_cfg[e] for e in epochs]\n\n            def lr_lambda(epoch):\n                idx = bisect.bisect_right(epochs, epoch) - 1\n                return 1.0 if idx &lt; 0 else values[idx]\n\n            scheduler = LambdaLR(opt, lr_lambda=lr_lambda)\n        elif \"lr_factor\" in lr_scheduler_cfg and \"lr_patience\" not in lr_scheduler_cfg:\n            scheduler = ExponentialLR(opt, gamma=lr_scheduler_cfg[\"lr_factor\"])\n        elif \"lr_factor\" in lr_scheduler_cfg:\n            scheduler = ReduceLROnPlateau(\n                opt,\n                mode=\"min\",\n                factor=lr_scheduler_cfg[\"lr_factor\"],\n                patience=lr_scheduler_cfg[\"lr_patience\"],\n            )\n        else:\n            raise ValueError(\"Invalid lr_scheduler configuration\")\n\n        return scheduler\n\n    def _scheduler_step(self, training_cfgs, scheduler, valid_loss):\n        lr_scheduler_cfg = training_cfgs[\"lr_scheduler\"]\n        required_keys = {\"lr_factor\", \"lr_patience\"}\n        if required_keys.issubset(lr_scheduler_cfg.keys()):\n            scheduler.step(valid_loss)\n        else:\n            scheduler.step()\n\n    def _1epoch_valid(\n        self, training_cfgs, criterion, validation_data_loader, valid_logs\n    ):\n        valid_obss_np, valid_preds_np, valid_loss = compute_validation(\n            self.model,\n            criterion,\n            validation_data_loader,\n            device=self.device,\n            which_first_tensor=training_cfgs[\"which_first_tensor\"],\n        )\n        valid_logs[\"valid_loss\"] = valid_loss\n        if (\n            self.cfgs[\"training_cfgs\"][\"valid_batch_mode\"] == \"test\"\n            and self.cfgs[\"training_cfgs\"][\"calc_metrics\"]\n        ):\n            # NOTE: Now we only evaluate the metrics for test-mode validation\n            target_col = self.cfgs[\"data_cfgs\"][\"target_cols\"]\n            valid_metrics = evaluate_validation(\n                validation_data_loader,\n                valid_preds_np,\n                valid_obss_np,\n                self.cfgs[\"evaluation_cfgs\"],\n                target_col,\n            )\n            valid_logs[\"valid_metrics\"] = valid_metrics\n            return valid_loss, valid_metrics\n        return valid_loss, None\n\n    def _get_trained_model(self):\n        model_loader = self.cfgs[\"evaluation_cfgs\"][\"model_loader\"]\n        model_pth_dir = self.cfgs[\"data_cfgs\"][\"case_dir\"]\n        return read_pth_from_model_loader(model_loader, model_pth_dir)\n\n    def model_evaluate(self) -&gt; Tuple[Dict, np.array, np.array]:\n        \"\"\"\n        A function to evaluate a model, called at end of training.\n\n        Returns\n        -------\n        tuple[dict, np.array, np.array]\n            eval_log, denormalized predictions and observations\n        \"\"\"\n        self.model = self.load_model(mode=\"infer\").to(self.device)\n        preds_xr, obss_xr = self.inference()\n        return preds_xr, obss_xr\n\n    def inference(self) -&gt; Tuple[xr.Dataset, xr.Dataset]:\n        \"\"\"infer using trained model and unnormalized results\"\"\"\n        data_cfgs = self.cfgs[\"data_cfgs\"]\n        training_cfgs = self.cfgs[\"training_cfgs\"]\n        test_dataloader = self._get_dataloader(training_cfgs, data_cfgs, mode=\"infer\")\n        seq_first = training_cfgs[\"which_first_tensor\"] == \"sequence\"\n        self.model.eval()\n        # here the batch is just an index of lookup table, so any batch size could be chosen\n        test_preds = []\n        obss = []\n        with torch.no_grad():\n            test_preds = []\n            obss = []\n            for i, batch in enumerate(\n                tqdm(test_dataloader, desc=\"Model inference\", unit=\"batch\")\n            ):\n                ys, pred = model_infer(\n                    seq_first,\n                    self.device,\n                    self.model,\n                    batch,\n                    variable_length_cfgs=None,\n                    return_key=(\n                        self.cfgs.get(\"evaluation_cfgs\", {})\n                        .get(\"evaluator\", {})\n                        .get(\"return_key\", None)\n                    )\n\n                )\n\n                test_preds.append(pred.cpu())\n                obss.append(ys.cpu())\n                if i % 100 == 0:\n                    torch.cuda.empty_cache()\n            pred = torch.cat(test_preds, dim=0).numpy()  # \u5728\u6700\u540e\u8f6c\u6362\u4e3anumpy\n            obs = torch.cat(obss, dim=0).numpy()  # \u5728\u6700\u540e\u8f6c\u6362\u4e3anumpy\n        if pred.ndim == 2:\n            # TODO: check\n            # the ndim is 2 meaning we use an Nto1 mode\n            # as lookup table is (basin 1's all time length, basin 2's all time length, ...)\n            # params of reshape should be (basin size, time length)\n            pred = pred.flatten().reshape(test_dataloader.test_data.y.shape[0], -1, 1)\n            obs = obs.flatten().reshape(test_dataloader.test_data.y.shape[0], -1, 1)\n        evaluation_cfgs = self.cfgs[\"evaluation_cfgs\"]\n        obs_xr, pred_xr = get_preds_to_be_eval(\n            test_dataloader,\n            evaluation_cfgs,\n            pred,\n            obs,\n        )\n        return pred_xr, obs_xr\n\n    def _get_optimizer(self, training_cfgs):\n        params_in_opt = self.model.parameters()\n        return pytorch_opt_dict[training_cfgs[\"optimizer\"]](\n            params_in_opt, **training_cfgs[\"optim_params\"]\n        )\n\n    def _get_loss_func(self, training_cfgs):\n        criterion_init_params = {}\n        if \"criterion_params\" in training_cfgs:\n            loss_param = training_cfgs[\"criterion_params\"]\n            if loss_param is not None:\n                for key in loss_param.keys():\n                    if key == \"loss_funcs\":\n                        criterion_init_params[key] = pytorch_criterion_dict[\n                            loss_param[key]\n                        ]()\n                    else:\n                        criterion_init_params[key] = loss_param[key]\n        return pytorch_criterion_dict[training_cfgs[\"criterion\"]](\n            **criterion_init_params\n        )\n\n    def _flood_event_collate_fn(self, batch):\n        \"\"\"\u81ea\u5b9a\u4e49\u7684\u6d2a\u6c34\u4e8b\u4ef6 collate \u51fd\u6570\uff0c\u786e\u4fdd\u6240\u6709\u6837\u672c\u957f\u5ea6\u4e00\u81f4\"\"\"\n\n        # \u627e\u5230\u8fd9\u4e2a\u6279\u6b21\u4e2d\u6700\u957f\u7684\u5e8f\u5217\u957f\u5ea6\n        max_len = max(tensor_data[0].shape[0] for tensor_data in batch)\n\n        # \u8c03\u6574\u6240\u6709\u6837\u672c\u5230\u76f8\u540c\u957f\u5ea6\n        processed_batch = []\n        for tensor_data in batch:\n            # \u83b7\u53d6x\u548cy\uff08\u5047\u8bbetensor_data[0]\u662fx\uff0ctensor_data[1]\u662fy\uff09\n            x = tensor_data[0]\n            y = tensor_data[1] if len(tensor_data) &gt; 1 else None\n\n            current_len = x.shape[0]\n            if current_len &lt; max_len:\n                # \u4f7f\u7528\u6700\u540e\u4e00\u4e2a\u503c\u586b\u5145x\n                padding_x = x[-1:].repeat(max_len - current_len, 1)\n                padded_x = torch.cat([x, padding_x], dim=0)\n\n                # \u5982\u679c\u6709y\uff0c\u4e5f\u8fdb\u884c\u586b\u5145\n                if y is not None:\n                    padding_y = y[-1:].repeat(max_len - current_len, 1)\n                    padded_y = torch.cat([y, padding_y], dim=0)\n                else:\n                    padded_y = None\n            else:\n                # \u5982\u679c\u66f4\u957f\u5219\u622a\u65ad\n                padded_x = x[:max_len]\n                padded_y = y[:max_len] if y is not None else None\n\n            if padded_y is not None:\n                processed_batch.append((padded_x, padded_y))\n            else:\n                processed_batch.append(padded_x)\n\n        # \u6839\u636e\u6570\u636e\u7ed3\u6784\u8fd4\u56de\u5806\u53e0\u540e\u7684\u7ed3\u679c\n        if len(processed_batch) &gt; 0 and isinstance(processed_batch[0], tuple):\n            return (\n                torch.stack([x for x, _ in processed_batch], 0),\n                torch.stack([y for _, y in processed_batch], 0)\n            )\n        else:\n            return torch.stack(processed_batch, 0)\n\n    def _get_dataloader(self, training_cfgs, data_cfgs, mode=\"train\"):\n        if mode == \"infer\":\n            _collate_fn = None\n            # Use GNN collate function for GNN datasets in inference mode\n            if hasattr(self.testdataset, '__class__') and 'GNN' in self.testdataset.__class__.__name__:\n                _collate_fn = gnn_collate_fn\n            # \u4f7f\u7528\u81ea\u5b9a\u4e49\u7684 collate \u51fd\u6570\u5904\u7406 FloodEventDataset\n            elif hasattr(self.testdataset, '__class__') and 'FloodEvent' in self.testdataset.__class__.__name__:\n                _collate_fn = self._flood_event_collate_fn\n            return DataLoader(\n                self.testdataset,\n                batch_size=training_cfgs[\"batch_size\"],\n                shuffle=False,\n                sampler=None,\n                batch_sampler=None,\n                drop_last=False,\n                timeout=0,\n                worker_init_fn=None,\n                collate_fn=_collate_fn,\n            )\n        worker_num = 0\n        pin_memory = False\n        if \"num_workers\" in training_cfgs:\n            worker_num = training_cfgs[\"num_workers\"]\n            print(f\"using {str(worker_num)} workers\")\n        if \"pin_memory\" in training_cfgs:\n            pin_memory = training_cfgs[\"pin_memory\"]\n            print(f\"Pin memory set to {str(pin_memory)}\")\n        sampler = self._get_sampler(data_cfgs, training_cfgs, self.traindataset)\n        _collate_fn = None\n        if training_cfgs[\"variable_length_cfgs\"][\"use_variable_length\"]:\n            _collate_fn = varied_length_collate_fn\n        # Use GNN collate function for GNN datasets\n        elif hasattr(self.traindataset, '__class__') and 'GNN' in self.traindataset.__class__.__name__:\n            _collate_fn = gnn_collate_fn\n        data_loader = DataLoader(\n            self.traindataset,\n            batch_size=training_cfgs[\"batch_size\"],\n            shuffle=(sampler is None),\n            sampler=sampler,\n            num_workers=worker_num,\n            pin_memory=pin_memory,\n            timeout=0,\n            collate_fn=_collate_fn,\n        )\n        if data_cfgs[\"t_range_valid\"] is not None:\n            # Use the same collate function for validation dataset\n            _val_collate_fn = None\n            if training_cfgs[\"variable_length_cfgs\"][\"use_variable_length\"]:\n                _val_collate_fn = varied_length_collate_fn\n            elif hasattr(self.validdataset, '__class__') and 'GNN' in self.validdataset.__class__.__name__:\n                _val_collate_fn = gnn_collate_fn\n\n            validation_data_loader = DataLoader(\n                self.validdataset,\n                batch_size=training_cfgs[\"batch_size\"],\n                shuffle=False,\n                num_workers=worker_num,\n                pin_memory=pin_memory,\n                timeout=0,\n                collate_fn=_val_collate_fn,\n            )\n            return data_loader, validation_data_loader\n\n        return data_loader, None\n\n    def _get_sampler(self, data_cfgs, training_cfgs, train_dataset):\n        \"\"\"\n        return data sampler based on the provided configuration and training dataset.\n\n        Parameters\n        ----------\n        data_cfgs : dict\n            Configuration dictionary containing parameters for data sampling. Expected keys are:\n            - \"sampler\": dict, containing:\n            - \"name\": str, name of the sampler to use.\n            - \"sampler_hyperparam\": dict, optional hyperparameters for the sampler.\n        training_cfgs: dict\n            Configuration dictionary containing parameters for training. Expected keys are:\n            - \"batch_size\": int, size of each batch.\n        train_dataset : Dataset\n            The training dataset object which contains the data to be sampled. Expected attributes are:\n            - ngrid: int, number of grids in the dataset.\n            - nt: int, number of time steps in the dataset.\n            - rho: int, length of the input sequence.\n            - warmup_length: int, length of the warmup period.\n            - horizon: int, length of the forecast horizon.\n\n        Returns\n        -------\n        sampler_class\n            An instance of the specified sampler class, initialized with the provided dataset and hyperparameters.\n\n        Raises\n        ------\n        NotImplementedError\n            If the specified sampler name is not found in the `data_sampler_dict`.\n        \"\"\"\n        if data_cfgs[\"sampler\"] is None:\n            return None\n        batch_size = training_cfgs[\"batch_size\"]\n        rho = train_dataset.rho\n        warmup_length = train_dataset.warmup_length\n        horizon = train_dataset.horizon\n        ngrid = train_dataset.ngrid\n        nt = train_dataset.nt\n        sampler_name = data_cfgs[\"sampler\"]\n        if sampler_name not in data_sampler_dict:\n            raise NotImplementedError(f\"Sampler {sampler_name} not implemented yet\")\n        sampler_class = data_sampler_dict[sampler_name]\n        sampler_hyperparam = {}\n        if sampler_name == \"KuaiSampler\":\n            sampler_hyperparam |= {\n                \"batch_size\": batch_size,\n                \"warmup_length\": warmup_length,\n                \"rho_horizon\": rho + horizon,\n                \"ngrid\": ngrid,\n                \"nt\": nt,\n            }\n        elif sampler_name == \"WindowLenBatchSampler\":\n            sampler_hyperparam |= {\n                \"batch_size\": batch_size,\n            }\n\n        return sampler_class(train_dataset, **sampler_hyperparam)\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydro.device","title":"<code>device</code>  <code>property</code> <code>readonly</code>","text":"<p>Get the device from fabric wrapper</p>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydro.__init__","title":"<code>__init__(self, cfgs, pre_model=None)</code>  <code>special</code>","text":""},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydro.__init__--parameters","title":"Parameters","text":"<p>cfgs     configs for the model pre_model     a pre-trained model, if it is not None,     we will use its weights to initialize this model     by default None</p> Source code in <code>torchhydro/trainers/deep_hydro.py</code> <pre><code>def __init__(\n    self,\n    cfgs: Dict,\n    pre_model=None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    cfgs\n        configs for the model\n    pre_model\n        a pre-trained model, if it is not None,\n        we will use its weights to initialize this model\n        by default None\n    \"\"\"\n    super().__init__(cfgs)\n    # Initialize fabric based on configuration\n    self.fabric = create_fabric_wrapper(cfgs.get(\"training_cfgs\", {}))\n    self.pre_model = pre_model\n    self.model = self.fabric.setup_module(self.load_model())\n    if cfgs[\"training_cfgs\"][\"train_mode\"]:\n        self.traindataset = self.make_dataset(\"train\")\n        if cfgs[\"data_cfgs\"][\"t_range_valid\"] is not None:\n            self.validdataset = self.make_dataset(\"valid\")\n    self.testdataset: BaseDataset = self.make_dataset(\"test\")\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydro.inference","title":"<code>inference(self)</code>","text":"<p>infer using trained model and unnormalized results</p> Source code in <code>torchhydro/trainers/deep_hydro.py</code> <pre><code>def inference(self) -&gt; Tuple[xr.Dataset, xr.Dataset]:\n    \"\"\"infer using trained model and unnormalized results\"\"\"\n    data_cfgs = self.cfgs[\"data_cfgs\"]\n    training_cfgs = self.cfgs[\"training_cfgs\"]\n    test_dataloader = self._get_dataloader(training_cfgs, data_cfgs, mode=\"infer\")\n    seq_first = training_cfgs[\"which_first_tensor\"] == \"sequence\"\n    self.model.eval()\n    # here the batch is just an index of lookup table, so any batch size could be chosen\n    test_preds = []\n    obss = []\n    with torch.no_grad():\n        test_preds = []\n        obss = []\n        for i, batch in enumerate(\n            tqdm(test_dataloader, desc=\"Model inference\", unit=\"batch\")\n        ):\n            ys, pred = model_infer(\n                seq_first,\n                self.device,\n                self.model,\n                batch,\n                variable_length_cfgs=None,\n                return_key=(\n                    self.cfgs.get(\"evaluation_cfgs\", {})\n                    .get(\"evaluator\", {})\n                    .get(\"return_key\", None)\n                )\n\n            )\n\n            test_preds.append(pred.cpu())\n            obss.append(ys.cpu())\n            if i % 100 == 0:\n                torch.cuda.empty_cache()\n        pred = torch.cat(test_preds, dim=0).numpy()  # \u5728\u6700\u540e\u8f6c\u6362\u4e3anumpy\n        obs = torch.cat(obss, dim=0).numpy()  # \u5728\u6700\u540e\u8f6c\u6362\u4e3anumpy\n    if pred.ndim == 2:\n        # TODO: check\n        # the ndim is 2 meaning we use an Nto1 mode\n        # as lookup table is (basin 1's all time length, basin 2's all time length, ...)\n        # params of reshape should be (basin size, time length)\n        pred = pred.flatten().reshape(test_dataloader.test_data.y.shape[0], -1, 1)\n        obs = obs.flatten().reshape(test_dataloader.test_data.y.shape[0], -1, 1)\n    evaluation_cfgs = self.cfgs[\"evaluation_cfgs\"]\n    obs_xr, pred_xr = get_preds_to_be_eval(\n        test_dataloader,\n        evaluation_cfgs,\n        pred,\n        obs,\n    )\n    return pred_xr, obs_xr\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydro.load_model","title":"<code>load_model(self, mode='train')</code>","text":"<p>Load a time series forecast model in pytorch_model_dict in model_dict_function.py</p>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydro.load_model--returns","title":"Returns","text":"<p>object     model in pytorch_model_dict in model_dict_function.py</p> Source code in <code>torchhydro/trainers/deep_hydro.py</code> <pre><code>def load_model(self, mode=\"train\"):\n    \"\"\"\n    Load a time series forecast model in pytorch_model_dict in model_dict_function.py\n\n    Returns\n    -------\n    object\n        model in pytorch_model_dict in model_dict_function.py\n    \"\"\"\n    if mode == \"infer\":\n        if self.weight_path is None or self.cfgs[\"model_cfgs\"][\"continue_train\"]:\n            # if no weight path is provided\n            # or weight file is provided but continue train again,\n            # we will use the trained model in the new case_dir directory\n            self.weight_path = self._get_trained_model()\n    elif mode != \"train\":\n        raise ValueError(\"Invalid mode; must be 'train' or 'infer'\")\n    model_cfgs = self.cfgs[\"model_cfgs\"]\n    model_name = model_cfgs[\"model_name\"]\n    if model_name not in pytorch_model_dict:\n        raise NotImplementedError(\n            f\"Error the model {model_name} was not found in the model dict. Please add it.\"\n        )\n    if self.pre_model is not None:\n        return self._load_pretrain_model()\n    elif self.weight_path is not None:\n        return self._load_model_from_pth()\n    else:\n        return pytorch_model_dict[model_name](**model_cfgs[\"model_hyperparam\"])\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydro.make_dataset","title":"<code>make_dataset(self, is_tra_val_te)</code>","text":"<p>Initializes a pytorch dataset.</p>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydro.make_dataset--parameters","title":"Parameters","text":"<p>is_tra_val_te     train or valid or test</p>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydro.make_dataset--returns","title":"Returns","text":"<p>object     an object initializing from class in datasets_dict in data_dict.py</p> Source code in <code>torchhydro/trainers/deep_hydro.py</code> <pre><code>def make_dataset(self, is_tra_val_te: str):\n    \"\"\"\n    Initializes a pytorch dataset.\n\n    Parameters\n    ----------\n    is_tra_val_te\n        train or valid or test\n\n    Returns\n    -------\n    object\n        an object initializing from class in datasets_dict in data_dict.py\n    \"\"\"\n    data_cfgs = self.cfgs[\"data_cfgs\"]\n    dataset_name = data_cfgs[\"dataset\"]\n\n    if dataset_name in list(datasets_dict.keys()):\n        dataset = datasets_dict[dataset_name](self.cfgs, is_tra_val_te)\n    else:\n        raise NotImplementedError(\n            f\"Error the dataset {str(dataset_name)} was not found in the dataset dict. Please add it.\"\n        )\n    return dataset\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydro.model_evaluate","title":"<code>model_evaluate(self)</code>","text":"<p>A function to evaluate a model, called at end of training.</p>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydro.model_evaluate--returns","title":"Returns","text":"<p>tuple[dict, np.array, np.array]     eval_log, denormalized predictions and observations</p> Source code in <code>torchhydro/trainers/deep_hydro.py</code> <pre><code>def model_evaluate(self) -&gt; Tuple[Dict, np.array, np.array]:\n    \"\"\"\n    A function to evaluate a model, called at end of training.\n\n    Returns\n    -------\n    tuple[dict, np.array, np.array]\n        eval_log, denormalized predictions and observations\n    \"\"\"\n    self.model = self.load_model(mode=\"infer\").to(self.device)\n    preds_xr, obss_xr = self.inference()\n    return preds_xr, obss_xr\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydro.model_train","title":"<code>model_train(self)</code>","text":"<p>train a hydrological DL model</p> Source code in <code>torchhydro/trainers/deep_hydro.py</code> <pre><code>def model_train(self) -&gt; None:\n    \"\"\"train a hydrological DL model\"\"\"\n    # A dictionary of the necessary parameters for training\n    training_cfgs = self.cfgs[\"training_cfgs\"]\n    # The file path to load model weights from; defaults to \"model_save\"\n    model_filepath = self.cfgs[\"data_cfgs\"][\"case_dir\"]\n    data_cfgs = self.cfgs[\"data_cfgs\"]\n    es = None\n    if training_cfgs[\"early_stopping\"]:\n        es = EarlyStopper(training_cfgs[\"patience\"])\n    criterion = self._get_loss_func(training_cfgs)\n    opt = self._get_optimizer(training_cfgs)\n    scheduler = self._get_scheduler(training_cfgs, opt)\n    max_epochs = training_cfgs[\"epochs\"]\n    start_epoch = training_cfgs[\"start_epoch\"]\n    # use PyTorch's DataLoader to load the data into batches in each epoch\n    data_loader, validation_data_loader = self._get_dataloader(\n        training_cfgs, data_cfgs\n    )\n    logger = TrainLogger(model_filepath, self.cfgs, opt)\n    for epoch in range(start_epoch, max_epochs + 1):\n        with logger.log_epoch_train(epoch) as train_logs:\n            total_loss, n_iter_ep = torch_single_train(\n                self.model,\n                opt,\n                criterion,\n                data_loader,\n                device=self.device,\n                which_first_tensor=training_cfgs[\"which_first_tensor\"],\n            )\n            train_logs[\"train_loss\"] = total_loss\n            train_logs[\"model\"] = self.model\n\n        valid_loss = None\n        valid_metrics = None\n        if data_cfgs[\"t_range_valid\"] is not None:\n            with logger.log_epoch_valid(epoch) as valid_logs:\n                valid_loss, valid_metrics = self._1epoch_valid(\n                    training_cfgs, criterion, validation_data_loader, valid_logs\n                )\n\n        self._scheduler_step(training_cfgs, scheduler, valid_loss)\n        logger.save_session_param(\n            epoch, total_loss, n_iter_ep, valid_loss, valid_metrics\n        )\n        logger.save_model_and_params(self.model, epoch, self.cfgs)\n        if es and not es.check_loss(\n            self.model,\n            valid_loss,\n            self.cfgs[\"data_cfgs\"][\"case_dir\"],\n        ):\n            print(\"Stopping model now\")\n            break\n    # logger.plot_model_structure(self.model)\n    logger.tb.close()\n\n    # return the trained model weights and bias and the epoch loss\n    return self.model.state_dict(), sum(logger.epoch_loss) / len(logger.epoch_loss)\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydroInterface","title":"<code> DeepHydroInterface            (ABC)         </code>","text":"<p>An abstract class used to handle different configurations of hydrological deep learning models + hyperparams for training, test, and predict functions. This class assumes that data is already split into test train and validation at this point.</p> Source code in <code>torchhydro/trainers/deep_hydro.py</code> <pre><code>class DeepHydroInterface(ABC):\n    \"\"\"\n    An abstract class used to handle different configurations\n    of hydrological deep learning models + hyperparams for training, test, and predict functions.\n    This class assumes that data is already split into test train and validation at this point.\n    \"\"\"\n\n    def __init__(self, cfgs: Dict):\n        \"\"\"\n        Parameters\n        ----------\n        cfgs\n            configs for initializing DeepHydro\n        \"\"\"\n\n        self._cfgs = cfgs\n\n    @property\n    def cfgs(self):\n        \"\"\"all configs\"\"\"\n        return self._cfgs\n\n    @property\n    def weight_path(self):\n        \"\"\"weight path\"\"\"\n        return self._cfgs[\"model_cfgs\"][\"weight_path\"]\n\n    @weight_path.setter\n    def weight_path(self, weight_path):\n        self._cfgs[\"model_cfgs\"][\"weight_path\"] = weight_path\n\n    @abstractmethod\n    def load_model(self, mode=\"train\") -&gt; object:\n        \"\"\"Get a Hydro DL model\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def make_dataset(self, is_tra_val_te: str) -&gt; object:\n        \"\"\"\n        Initializes a pytorch dataset.\n\n        Parameters\n        ----------\n        is_tra_val_te\n            train or valid or test\n\n        Returns\n        -------\n        object\n            a dataset class loading data from data source\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def model_train(self):\n        \"\"\"\n        Train the model\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def model_evaluate(self):\n        \"\"\"\n        Evaluate the model\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydroInterface.cfgs","title":"<code>cfgs</code>  <code>property</code> <code>readonly</code>","text":"<p>all configs</p>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydroInterface.weight_path","title":"<code>weight_path</code>  <code>property</code> <code>writable</code>","text":"<p>weight path</p>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydroInterface.__init__","title":"<code>__init__(self, cfgs)</code>  <code>special</code>","text":""},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydroInterface.__init__--parameters","title":"Parameters","text":"<p>cfgs     configs for initializing DeepHydro</p> Source code in <code>torchhydro/trainers/deep_hydro.py</code> <pre><code>def __init__(self, cfgs: Dict):\n    \"\"\"\n    Parameters\n    ----------\n    cfgs\n        configs for initializing DeepHydro\n    \"\"\"\n\n    self._cfgs = cfgs\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydroInterface.load_model","title":"<code>load_model(self, mode='train')</code>","text":"<p>Get a Hydro DL model</p> Source code in <code>torchhydro/trainers/deep_hydro.py</code> <pre><code>@abstractmethod\ndef load_model(self, mode=\"train\") -&gt; object:\n    \"\"\"Get a Hydro DL model\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydroInterface.make_dataset","title":"<code>make_dataset(self, is_tra_val_te)</code>","text":"<p>Initializes a pytorch dataset.</p>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydroInterface.make_dataset--parameters","title":"Parameters","text":"<p>is_tra_val_te     train or valid or test</p>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydroInterface.make_dataset--returns","title":"Returns","text":"<p>object     a dataset class loading data from data source</p> Source code in <code>torchhydro/trainers/deep_hydro.py</code> <pre><code>@abstractmethod\ndef make_dataset(self, is_tra_val_te: str) -&gt; object:\n    \"\"\"\n    Initializes a pytorch dataset.\n\n    Parameters\n    ----------\n    is_tra_val_te\n        train or valid or test\n\n    Returns\n    -------\n    object\n        a dataset class loading data from data source\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydroInterface.model_evaluate","title":"<code>model_evaluate(self)</code>","text":"<p>Evaluate the model</p> Source code in <code>torchhydro/trainers/deep_hydro.py</code> <pre><code>@abstractmethod\ndef model_evaluate(self):\n    \"\"\"\n    Evaluate the model\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.DeepHydroInterface.model_train","title":"<code>model_train(self)</code>","text":"<p>Train the model</p> Source code in <code>torchhydro/trainers/deep_hydro.py</code> <pre><code>@abstractmethod\ndef model_train(self):\n    \"\"\"\n    Train the model\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.FedLearnHydro","title":"<code> FedLearnHydro            (DeepHydro)         </code>","text":"<p>Federated Learning Hydrological DL model</p> Source code in <code>torchhydro/trainers/deep_hydro.py</code> <pre><code>class FedLearnHydro(DeepHydro):\n    \"\"\"Federated Learning Hydrological DL model\"\"\"\n\n    def __init__(self, cfgs: Dict):\n        super().__init__(cfgs)\n        # a user group which is a dict where the keys are the user index\n        # and the values are the corresponding data for each of those users\n        train_dataset = self.traindataset\n        fl_hyperparam = self.cfgs[\"model_cfgs\"][\"fl_hyperparam\"]\n        # sample training data amongst users\n        if fl_hyperparam[\"fl_sample\"] == \"basin\":\n            # Sample a basin for a user\n            user_groups = fl_sample_basin(train_dataset)\n        elif fl_hyperparam[\"fl_sample\"] == \"region\":\n            # Sample a region for a user\n            user_groups = fl_sample_region(train_dataset)\n        else:\n            raise NotImplementedError()\n        self.user_groups = user_groups\n\n    @property\n    def num_users(self):\n        \"\"\"number of users in federated learning\"\"\"\n        return len(self.user_groups)\n\n    def model_train(self) -&gt; None:\n        # BUILD MODEL\n        global_model = self.model\n\n        # copy weights\n        global_weights = global_model.state_dict()\n\n        # Training\n        train_loss, train_accuracy = [], []\n        print_every = 2\n\n        training_cfgs = self.cfgs[\"training_cfgs\"]\n        model_cfgs = self.cfgs[\"model_cfgs\"]\n        max_epochs = training_cfgs[\"epochs\"]\n        start_epoch = training_cfgs[\"start_epoch\"]\n        fl_hyperparam = model_cfgs[\"fl_hyperparam\"]\n        # total rounds in a FL system is max_epochs\n        for epoch in tqdm(range(start_epoch, max_epochs + 1)):\n            local_weights, local_losses = [], []\n            print(f\"\\n | Global Training Round : {epoch} |\\n\")\n\n            global_model.train()\n            m = max(int(fl_hyperparam[\"fl_frac\"] * self.num_users), 1)\n            # randomly select m users, they will be the clients in this round\n            idxs_users = np.random.choice(range(self.num_users), m, replace=False)\n\n            for idx in idxs_users:\n                # each user will be used to train the model locally\n                # user_gourps[idx] means the idx of dataset for a user\n                user_cfgs = self._get_a_user_cfgs(idx)\n                local_model = DeepHydro(\n                    user_cfgs,\n                    pre_model=copy.deepcopy(global_model),\n                )\n                w, loss = local_model.model_train()\n                local_weights.append(copy.deepcopy(w))\n                local_losses.append(copy.deepcopy(loss))\n\n            # update global weights\n            global_weights = average_weights(local_weights)\n\n            # update global weights\n            global_model.load_state_dict(global_weights)\n\n            loss_avg = sum(local_losses) / len(local_losses)\n            train_loss.append(loss_avg)\n\n            # Calculate avg training accuracy over all users at every epoch\n            list_acc = []\n            global_model.eval()\n            for c in range(self.num_users):\n                one_user_cfg = self._get_a_user_cfgs(c)\n                local_model = DeepHydro(\n                    one_user_cfg,\n                    pre_model=global_model,\n                )\n                acc, _, _ = local_model.model_evaluate()\n                list_acc.append(acc)\n            values = [list(d.values())[0][0] for d in list_acc]\n            filtered_values = [v for v in values if not np.isnan(v)]\n            train_accuracy.append(sum(filtered_values) / len(filtered_values))\n\n            # print global training loss after every 'i' rounds\n            if (epoch + 1) % print_every == 0:\n                print(f\" \\nAvg Training Stats after {epoch+1} global rounds:\")\n                print(f\"Training Loss : {np.mean(np.array(train_loss))}\")\n                print(\"Train Accuracy: {:.2f}% \\n\".format(100 * train_accuracy[-1]))\n\n    def _get_a_user_cfgs(self, idx):\n        \"\"\"To get a user's configs for local training\"\"\"\n        user = self.user_groups[idx]\n\n        # update data_cfgs\n        # Use defaultdict to collect dates for each basin\n        basin_dates = defaultdict(list)\n\n        for _, (basin, time) in user.items():\n            basin_dates[basin].append(time)\n\n        # Initialize a list to store distinct basins\n        basins = []\n\n        # for each basin, we can find its date range\n        date_ranges = {}\n        for basin, times in basin_dates.items():\n            basins.append(basin)\n            date_ranges[basin] = (np.min(times), np.max(times))\n        # get the longest date range\n        longest_date_range = max(date_ranges.values(), key=lambda x: x[1] - x[0])\n        # transform the date range of numpy data into string\n        longest_date_range = [\n            np.datetime_as_string(dt, unit=\"D\") for dt in longest_date_range\n        ]\n        user_cfgs = copy.deepcopy(self.cfgs)\n        # update data_cfgs\n        update_nested_dict(\n            user_cfgs, [\"data_cfgs\", \"t_range_train\"], longest_date_range\n        )\n        # for local training in FL, we don't need a validation set\n        update_nested_dict(user_cfgs, [\"data_cfgs\", \"t_range_valid\"], None)\n        # for local training in FL, we don't need a test set, but we should set one to avoid error\n        update_nested_dict(user_cfgs, [\"data_cfgs\", \"t_range_test\"], longest_date_range)\n        update_nested_dict(user_cfgs, [\"data_cfgs\", \"object_ids\"], basins)\n\n        # update training_cfgs\n        # we also need to update some training params for local training from FL settings\n        update_nested_dict(\n            user_cfgs,\n            [\"training_cfgs\", \"epochs\"],\n            user_cfgs[\"model_cfgs\"][\"fl_hyperparam\"][\"fl_local_ep\"],\n        )\n        update_nested_dict(\n            user_cfgs,\n            [\"evaluation_cfgs\", \"test_epoch\"],\n            user_cfgs[\"model_cfgs\"][\"fl_hyperparam\"][\"fl_local_ep\"],\n        )\n        # don't need to save model weights for local training\n        update_nested_dict(\n            user_cfgs,\n            [\"training_cfgs\", \"save_epoch\"],\n            None,\n        )\n        # there are two settings for batch size in configs, we need to update both of them\n        update_nested_dict(\n            user_cfgs,\n            [\"training_cfgs\", \"batch_size\"],\n            user_cfgs[\"model_cfgs\"][\"fl_hyperparam\"][\"fl_local_bs\"],\n        )\n        update_nested_dict(\n            user_cfgs,\n            [\"data_cfgs\", \"batch_size\"],\n            user_cfgs[\"model_cfgs\"][\"fl_hyperparam\"][\"fl_local_bs\"],\n        )\n\n        # update model_cfgs finally\n        # For local model, its model_type is Normal\n        update_nested_dict(user_cfgs, [\"model_cfgs\", \"model_type\"], \"Normal\")\n        update_nested_dict(\n            user_cfgs,\n            [\"model_cfgs\", \"fl_hyperparam\"],\n            None,\n        )\n        return user_cfgs\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.FedLearnHydro.num_users","title":"<code>num_users</code>  <code>property</code> <code>readonly</code>","text":"<p>number of users in federated learning</p>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.FedLearnHydro.model_train","title":"<code>model_train(self)</code>","text":"<p>train a hydrological DL model</p> Source code in <code>torchhydro/trainers/deep_hydro.py</code> <pre><code>def model_train(self) -&gt; None:\n    # BUILD MODEL\n    global_model = self.model\n\n    # copy weights\n    global_weights = global_model.state_dict()\n\n    # Training\n    train_loss, train_accuracy = [], []\n    print_every = 2\n\n    training_cfgs = self.cfgs[\"training_cfgs\"]\n    model_cfgs = self.cfgs[\"model_cfgs\"]\n    max_epochs = training_cfgs[\"epochs\"]\n    start_epoch = training_cfgs[\"start_epoch\"]\n    fl_hyperparam = model_cfgs[\"fl_hyperparam\"]\n    # total rounds in a FL system is max_epochs\n    for epoch in tqdm(range(start_epoch, max_epochs + 1)):\n        local_weights, local_losses = [], []\n        print(f\"\\n | Global Training Round : {epoch} |\\n\")\n\n        global_model.train()\n        m = max(int(fl_hyperparam[\"fl_frac\"] * self.num_users), 1)\n        # randomly select m users, they will be the clients in this round\n        idxs_users = np.random.choice(range(self.num_users), m, replace=False)\n\n        for idx in idxs_users:\n            # each user will be used to train the model locally\n            # user_gourps[idx] means the idx of dataset for a user\n            user_cfgs = self._get_a_user_cfgs(idx)\n            local_model = DeepHydro(\n                user_cfgs,\n                pre_model=copy.deepcopy(global_model),\n            )\n            w, loss = local_model.model_train()\n            local_weights.append(copy.deepcopy(w))\n            local_losses.append(copy.deepcopy(loss))\n\n        # update global weights\n        global_weights = average_weights(local_weights)\n\n        # update global weights\n        global_model.load_state_dict(global_weights)\n\n        loss_avg = sum(local_losses) / len(local_losses)\n        train_loss.append(loss_avg)\n\n        # Calculate avg training accuracy over all users at every epoch\n        list_acc = []\n        global_model.eval()\n        for c in range(self.num_users):\n            one_user_cfg = self._get_a_user_cfgs(c)\n            local_model = DeepHydro(\n                one_user_cfg,\n                pre_model=global_model,\n            )\n            acc, _, _ = local_model.model_evaluate()\n            list_acc.append(acc)\n        values = [list(d.values())[0][0] for d in list_acc]\n        filtered_values = [v for v in values if not np.isnan(v)]\n        train_accuracy.append(sum(filtered_values) / len(filtered_values))\n\n        # print global training loss after every 'i' rounds\n        if (epoch + 1) % print_every == 0:\n            print(f\" \\nAvg Training Stats after {epoch+1} global rounds:\")\n            print(f\"Training Loss : {np.mean(np.array(train_loss))}\")\n            print(\"Train Accuracy: {:.2f}% \\n\".format(100 * train_accuracy[-1]))\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.TransLearnHydro","title":"<code> TransLearnHydro            (DeepHydro)         </code>","text":"Source code in <code>torchhydro/trainers/deep_hydro.py</code> <pre><code>class TransLearnHydro(DeepHydro):\n    def __init__(self, cfgs: Dict, pre_model=None):\n        super().__init__(cfgs, pre_model)\n\n    def load_model(self, mode=\"train\"):\n        \"\"\"Load model for transfer learning\"\"\"\n        model_cfgs = self.cfgs[\"model_cfgs\"]\n        if self.weight_path is None and self.pre_model is None:\n            raise NotImplementedError(\n                \"For transfer learning, we need a pre-trained model\"\n            )\n        if mode == \"train\":\n            model = super().load_model(mode)\n        elif mode == \"infer\":\n            self.weight_path = self._get_trained_model()\n            model = self._load_model_from_pth()\n            model.to(self.device)\n        if (\n            \"weight_path_add\" in model_cfgs\n            and \"freeze_params\" in model_cfgs[\"weight_path_add\"]\n        ):\n            freeze_params = model_cfgs[\"weight_path_add\"][\"freeze_params\"]\n            for param in freeze_params:\n                exec(f\"model.{param}.requires_grad = False\")\n        return model\n\n    def _load_model_from_pth(self):\n        weight_path = self.weight_path\n        model_cfgs = self.cfgs[\"model_cfgs\"]\n        model_name = model_cfgs[\"model_name\"]\n        model = pytorch_model_dict[model_name](**model_cfgs[\"model_hyperparam\"])\n        checkpoint = torch.load(weight_path, map_location=self.device)\n        if \"weight_path_add\" in model_cfgs:\n            if \"excluded_layers\" in model_cfgs[\"weight_path_add\"]:\n                # delete some layers from source model if we don't need them\n                excluded_layers = model_cfgs[\"weight_path_add\"][\"excluded_layers\"]\n                for layer in excluded_layers:\n                    del checkpoint[layer]\n                print(\"sucessfully deleted layers\")\n            else:\n                print(\"directly loading identically-named layers of source model\")\n        model.load_state_dict(checkpoint, strict=False)\n        print(\"Weights sucessfully loaded\")\n        return model\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.deep_hydro.TransLearnHydro.load_model","title":"<code>load_model(self, mode='train')</code>","text":"<p>Load model for transfer learning</p> Source code in <code>torchhydro/trainers/deep_hydro.py</code> <pre><code>def load_model(self, mode=\"train\"):\n    \"\"\"Load model for transfer learning\"\"\"\n    model_cfgs = self.cfgs[\"model_cfgs\"]\n    if self.weight_path is None and self.pre_model is None:\n        raise NotImplementedError(\n            \"For transfer learning, we need a pre-trained model\"\n        )\n    if mode == \"train\":\n        model = super().load_model(mode)\n    elif mode == \"infer\":\n        self.weight_path = self._get_trained_model()\n        model = self._load_model_from_pth()\n        model.to(self.device)\n    if (\n        \"weight_path_add\" in model_cfgs\n        and \"freeze_params\" in model_cfgs[\"weight_path_add\"]\n    ):\n        freeze_params = model_cfgs[\"weight_path_add\"][\"freeze_params\"]\n        for param in freeze_params:\n            exec(f\"model.{param}.requires_grad = False\")\n    return model\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper","title":"<code>fabric_wrapper</code>","text":"<p>Author: Wenyu Ouyang Date: 2023-07-25 16:47:19 LastEditTime: 2025-06-17 10:39:32 LastEditors: Wenyu Ouyang Description: Lightning Fabric wrapper for debugging and distributed training FilePath:       orchhydro       orchhydro       rainers\fabric_wrapper.py Copyright (c) 2025-2026 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper","title":"<code> FabricWrapper        </code>","text":"<p>A wrapper class that can switch between Lightning Fabric and normal PyTorch operations based on configuration settings.</p> <p>TODO: the fabric wrapper is not fully used for parallel training yet</p> Source code in <code>torchhydro/trainers/fabric_wrapper.py</code> <pre><code>class FabricWrapper:\n    \"\"\"\n    A wrapper class that can switch between Lightning Fabric and normal PyTorch operations\n    based on configuration settings.\n\n    TODO: the fabric wrapper is not fully used for parallel training yet\n    \"\"\"\n\n    def __init__(self, use_fabric: bool = True, fabric_config: Optional[Dict] = None):\n        \"\"\"\n        Initialize the Fabric wrapper.\n\n        Parameters\n        ----------\n        use_fabric : bool\n            Whether to use Lightning Fabric or normal PyTorch operations\n        fabric_config : Optional[Dict]\n            Configuration for Fabric (devices, strategy, etc.)\n        \"\"\"\n        self.use_fabric = use_fabric\n        self.fabric_config = fabric_config or {}\n        self._fabric: Optional[Any] = None\n        self._device: Optional[torch.device] = None\n\n        if self.use_fabric:\n            self._init_fabric()\n        else:\n            self._init_pytorch()\n\n    def _init_fabric(self) -&gt; None:\n        \"\"\"Initialize Lightning Fabric\"\"\"\n        try:\n            import lightning as L\n\n            # Default fabric configuration\n            default_config = {\n                \"accelerator\": \"auto\",\n                \"devices\": \"auto\",\n                \"strategy\": \"auto\",\n                \"precision\": \"32-true\",\n            }\n\n            # Update with user config\n            default_config.update(self.fabric_config)\n\n            self._fabric = L.Fabric(**default_config)\n            print(\"\u2705 Lightning Fabric initialized successfully\")\n\n        except ImportError:\n            print(\"\u274c Lightning not found, falling back to normal PyTorch\")\n            self.use_fabric = False\n            self._init_pytorch()\n\n    def _init_pytorch(self) -&gt; None:\n        \"\"\"Initialize normal PyTorch setup\"\"\"\n        self.device_num = self.fabric_config[\"devices\"]\n        #self.device_num = [0]\n        self._device = get_the_device(self.device_num)\n        print(f\"\u2705 Normal PyTorch initialized, using device: {self._device}\")\n\n    def setup_module(self, model: torch.nn.Module) -&gt; torch.nn.Module:\n        \"\"\"Setup model for training\"\"\"\n        if self.use_fabric:\n            return self._fabric.setup_module(model)\n        else:\n            return model.to(self._device)\n\n    def setup_optimizers(\n        self, optimizer: torch.optim.Optimizer\n    ) -&gt; torch.optim.Optimizer:\n        \"\"\"Setup optimizer\"\"\"\n        if self.use_fabric:\n            return self._fabric.setup_optimizers(optimizer)\n        else:\n            return optimizer\n\n    def setup_dataloaders(\n        self, *dataloaders: torch.utils.data.DataLoader\n    ) -&gt; Tuple[torch.utils.data.DataLoader, ...]:\n        \"\"\"Setup dataloaders\"\"\"\n        if self.use_fabric:\n            return self._fabric.setup_dataloaders(*dataloaders)\n        else:\n            return dataloaders\n\n    def save(self, path: str, state_dict: Dict[str, Any]) -&gt; None:\n        \"\"\"Save model state\"\"\"\n        if self.use_fabric:\n            self._fabric.save(path, state_dict)\n        else:\n            torch.save(state_dict, path)\n\n    def load(self, path: str, model: Optional[torch.nn.Module] = None) -&gt; Any:\n        \"\"\"Load model state\"\"\"\n        if self.use_fabric:\n            return self._fabric.load(path, model)\n        else:\n            return torch.load(path, map_location=self._device)\n\n    def load_raw(self, path: str, model: torch.nn.Module) -&gt; None:\n        \"\"\"Load raw model weights\"\"\"\n        if self.use_fabric:\n            checkpoint = self._fabric.load(path)\n            model.load_state_dict(checkpoint)\n        else:\n            checkpoint = torch.load(path, map_location=self._device)\n            model.load_state_dict(checkpoint)\n\n    def launch(self, fn: Optional[Any] = None, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Launch training function\"\"\"\n        if self.use_fabric:\n            if fn is None:\n                # This is called without a function, just launch fabric\n                return self._fabric.launch()\n            else:\n                return self._fabric.launch(fn, *args, **kwargs)\n        else:\n            # Normal PyTorch, just call the function directly\n            if fn is not None:\n                return fn(*args, **kwargs)\n            else:\n                return None\n\n    def backward(self, loss: torch.Tensor) -&gt; None:\n        \"\"\"Backward pass\"\"\"\n        if self.use_fabric:\n            self._fabric.backward(loss)\n        else:\n            loss.backward()\n\n    def clip_gradients(\n        self,\n        model: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        max_norm: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Clip gradients\"\"\"\n        if self.use_fabric:\n            self._fabric.clip_gradients(model, optimizer, max_norm=max_norm)\n        else:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n\n    @property\n    def device(self) -&gt; torch.device:\n        \"\"\"Get current device\"\"\"\n        if self.use_fabric:\n            return self._fabric.device\n        else:\n            return self._device\n\n    @property\n    def local_rank(self) -&gt; int:\n        \"\"\"Get local rank\"\"\"\n        if self.use_fabric:\n            return self._fabric.local_rank\n        else:\n            return 0\n\n    @property\n    def global_rank(self) -&gt; int:\n        \"\"\"Get global rank\"\"\"\n        if self.use_fabric:\n            return self._fabric.global_rank\n        else:\n            return 0\n\n    @property\n    def world_size(self) -&gt; int:\n        \"\"\"Get world size\"\"\"\n        if self.use_fabric:\n            return self._fabric.world_size\n        else:\n            return 1\n\n    def barrier(self) -&gt; None:\n        \"\"\"Synchronization barrier\"\"\"\n        if self.use_fabric:\n            self._fabric.barrier()\n        else:\n            pass  # No barrier needed for single process\n\n    def print(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Print only on rank 0\"\"\"\n        if self.use_fabric:\n            self._fabric.print(*args, **kwargs)\n        else:\n            print(*args, **kwargs)\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper.device","title":"<code>device: device</code>  <code>property</code> <code>readonly</code>","text":"<p>Get current device</p>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper.global_rank","title":"<code>global_rank: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Get global rank</p>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper.local_rank","title":"<code>local_rank: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Get local rank</p>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper.world_size","title":"<code>world_size: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Get world size</p>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper.__init__","title":"<code>__init__(self, use_fabric=True, fabric_config=None)</code>  <code>special</code>","text":"<p>Initialize the Fabric wrapper.</p>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper.__init__--parameters","title":"Parameters","text":"<p>use_fabric : bool     Whether to use Lightning Fabric or normal PyTorch operations fabric_config : Optional[Dict]     Configuration for Fabric (devices, strategy, etc.)</p> Source code in <code>torchhydro/trainers/fabric_wrapper.py</code> <pre><code>def __init__(self, use_fabric: bool = True, fabric_config: Optional[Dict] = None):\n    \"\"\"\n    Initialize the Fabric wrapper.\n\n    Parameters\n    ----------\n    use_fabric : bool\n        Whether to use Lightning Fabric or normal PyTorch operations\n    fabric_config : Optional[Dict]\n        Configuration for Fabric (devices, strategy, etc.)\n    \"\"\"\n    self.use_fabric = use_fabric\n    self.fabric_config = fabric_config or {}\n    self._fabric: Optional[Any] = None\n    self._device: Optional[torch.device] = None\n\n    if self.use_fabric:\n        self._init_fabric()\n    else:\n        self._init_pytorch()\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper.backward","title":"<code>backward(self, loss)</code>","text":"<p>Backward pass</p> Source code in <code>torchhydro/trainers/fabric_wrapper.py</code> <pre><code>def backward(self, loss: torch.Tensor) -&gt; None:\n    \"\"\"Backward pass\"\"\"\n    if self.use_fabric:\n        self._fabric.backward(loss)\n    else:\n        loss.backward()\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper.barrier","title":"<code>barrier(self)</code>","text":"<p>Synchronization barrier</p> Source code in <code>torchhydro/trainers/fabric_wrapper.py</code> <pre><code>def barrier(self) -&gt; None:\n    \"\"\"Synchronization barrier\"\"\"\n    if self.use_fabric:\n        self._fabric.barrier()\n    else:\n        pass  # No barrier needed for single process\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper.clip_gradients","title":"<code>clip_gradients(self, model, optimizer, max_norm=1.0)</code>","text":"<p>Clip gradients</p> Source code in <code>torchhydro/trainers/fabric_wrapper.py</code> <pre><code>def clip_gradients(\n    self,\n    model: torch.nn.Module,\n    optimizer: torch.optim.Optimizer,\n    max_norm: float = 1.0,\n) -&gt; None:\n    \"\"\"Clip gradients\"\"\"\n    if self.use_fabric:\n        self._fabric.clip_gradients(model, optimizer, max_norm=max_norm)\n    else:\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper.launch","title":"<code>launch(self, fn=None, *args, **kwargs)</code>","text":"<p>Launch training function</p> Source code in <code>torchhydro/trainers/fabric_wrapper.py</code> <pre><code>def launch(self, fn: Optional[Any] = None, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Launch training function\"\"\"\n    if self.use_fabric:\n        if fn is None:\n            # This is called without a function, just launch fabric\n            return self._fabric.launch()\n        else:\n            return self._fabric.launch(fn, *args, **kwargs)\n    else:\n        # Normal PyTorch, just call the function directly\n        if fn is not None:\n            return fn(*args, **kwargs)\n        else:\n            return None\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper.load","title":"<code>load(self, path, model=None)</code>","text":"<p>Load model state</p> Source code in <code>torchhydro/trainers/fabric_wrapper.py</code> <pre><code>def load(self, path: str, model: Optional[torch.nn.Module] = None) -&gt; Any:\n    \"\"\"Load model state\"\"\"\n    if self.use_fabric:\n        return self._fabric.load(path, model)\n    else:\n        return torch.load(path, map_location=self._device)\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper.load_raw","title":"<code>load_raw(self, path, model)</code>","text":"<p>Load raw model weights</p> Source code in <code>torchhydro/trainers/fabric_wrapper.py</code> <pre><code>def load_raw(self, path: str, model: torch.nn.Module) -&gt; None:\n    \"\"\"Load raw model weights\"\"\"\n    if self.use_fabric:\n        checkpoint = self._fabric.load(path)\n        model.load_state_dict(checkpoint)\n    else:\n        checkpoint = torch.load(path, map_location=self._device)\n        model.load_state_dict(checkpoint)\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper.print","title":"<code>print(self, *args, **kwargs)</code>","text":"<p>Print only on rank 0</p> Source code in <code>torchhydro/trainers/fabric_wrapper.py</code> <pre><code>def print(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Print only on rank 0\"\"\"\n    if self.use_fabric:\n        self._fabric.print(*args, **kwargs)\n    else:\n        print(*args, **kwargs)\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper.save","title":"<code>save(self, path, state_dict)</code>","text":"<p>Save model state</p> Source code in <code>torchhydro/trainers/fabric_wrapper.py</code> <pre><code>def save(self, path: str, state_dict: Dict[str, Any]) -&gt; None:\n    \"\"\"Save model state\"\"\"\n    if self.use_fabric:\n        self._fabric.save(path, state_dict)\n    else:\n        torch.save(state_dict, path)\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper.setup_dataloaders","title":"<code>setup_dataloaders(self, *dataloaders)</code>","text":"<p>Setup dataloaders</p> Source code in <code>torchhydro/trainers/fabric_wrapper.py</code> <pre><code>def setup_dataloaders(\n    self, *dataloaders: torch.utils.data.DataLoader\n) -&gt; Tuple[torch.utils.data.DataLoader, ...]:\n    \"\"\"Setup dataloaders\"\"\"\n    if self.use_fabric:\n        return self._fabric.setup_dataloaders(*dataloaders)\n    else:\n        return dataloaders\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper.setup_module","title":"<code>setup_module(self, model)</code>","text":"<p>Setup model for training</p> Source code in <code>torchhydro/trainers/fabric_wrapper.py</code> <pre><code>def setup_module(self, model: torch.nn.Module) -&gt; torch.nn.Module:\n    \"\"\"Setup model for training\"\"\"\n    if self.use_fabric:\n        return self._fabric.setup_module(model)\n    else:\n        return model.to(self._device)\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.FabricWrapper.setup_optimizers","title":"<code>setup_optimizers(self, optimizer)</code>","text":"<p>Setup optimizer</p> Source code in <code>torchhydro/trainers/fabric_wrapper.py</code> <pre><code>def setup_optimizers(\n    self, optimizer: torch.optim.Optimizer\n) -&gt; torch.optim.Optimizer:\n    \"\"\"Setup optimizer\"\"\"\n    if self.use_fabric:\n        return self._fabric.setup_optimizers(optimizer)\n    else:\n        return optimizer\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.create_fabric_wrapper","title":"<code>create_fabric_wrapper(training_cfgs)</code>","text":"<p>Create a fabric wrapper based on training configuration.</p>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.create_fabric_wrapper--parameters","title":"Parameters","text":"<p>training_cfgs : Dict     Training configuration dictionary</p>"},{"location":"api/trainers/#torchhydro.trainers.fabric_wrapper.create_fabric_wrapper--returns","title":"Returns","text":"<p>FabricWrapper     Initialized fabric wrapper</p> Source code in <code>torchhydro/trainers/fabric_wrapper.py</code> <pre><code>def create_fabric_wrapper(training_cfgs: Dict) -&gt; FabricWrapper:\n    \"\"\"\n    Create a fabric wrapper based on training configuration.\n\n    Parameters\n    ----------\n    training_cfgs : Dict\n        Training configuration dictionary\n\n    Returns\n    -------\n    FabricWrapper\n        Initialized fabric wrapper\n    \"\"\"\n    # Check if we should use fabric\n    fabric_strategy = training_cfgs.get(\"fabric_strategy\")\n    use_fabric = fabric_strategy is not None\n\n    # Check if we have multiple devices\n    devices = training_cfgs.get(\"device\", [0])\n    if isinstance(devices, list) and len(devices) == 1 and use_fabric:\n        print(\"\ud83d\udcf1 Single device detected - we can disable Fabric\")\n        use_fabric = False\n\n    # Fabric configuration\n    fabric_config = {\n        \"devices\": devices if isinstance(devices, list) else [devices],\n        \"strategy\": fabric_strategy,\n        \"precision\": training_cfgs.get(\"precision\", \"32-true\"),\n        \"accelerator\": training_cfgs.get(\"accelerator\", \"auto\"),\n    }\n\n    return FabricWrapper(use_fabric=use_fabric, fabric_config=fabric_config)\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.resulter","title":"<code>resulter</code>","text":""},{"location":"api/trainers/#torchhydro.trainers.resulter.Resulter","title":"<code> Resulter        </code>","text":"Source code in <code>torchhydro/trainers/resulter.py</code> <pre><code>class Resulter:\n    def __init__(self, cfgs) -&gt; None:\n        self.cfgs = cfgs\n        self.result_dir = cfgs[\"data_cfgs\"][\"case_dir\"]\n        if not os.path.exists(self.result_dir):\n            os.makedirs(self.result_dir)\n\n    @property\n    def pred_name(self):\n        return f\"epoch{str(self.chosen_trained_epoch)}flow_pred\"\n\n    @property\n    def obs_name(self):\n        return f\"epoch{str(self.chosen_trained_epoch)}flow_obs\"\n\n    @property\n    def chosen_trained_epoch(self):\n        model_loader = self.cfgs[\"evaluation_cfgs\"][\"model_loader\"]\n        if model_loader[\"load_way\"] == \"specified\":\n            epoch_name = str(model_loader[\"test_epoch\"])\n        elif model_loader[\"load_way\"] == \"best\":\n            # NOTE: TO make it consistent with the name in case of model_loader[\"load_way\"] == \"pth\", the name have to be \"best_model.pth\"\n            epoch_name = \"best_model.pth\"\n        elif model_loader[\"load_way\"] == \"latest\":\n            epoch_name = str(self.cfgs[\"training_cfgs\"][\"epochs\"])\n        elif model_loader[\"load_way\"] == \"pth\":\n            epoch_name = model_loader[\"pth_path\"].split(os.sep)[-1]\n        else:\n            raise ValueError(\"Invalid load_way\")\n        return epoch_name\n\n    def save_cfg(self, cfgs):\n        # save the cfgs after training\n        # update the cfgs with the latest one\n        self.cfgs = cfgs\n        param_file_exist = any(\n            (\n                fnmatch.fnmatch(file, \"*.json\")\n                and \"_stat\" not in file  # statistics json file\n                and \"_dict\" not in file  # data cache json file\n            )\n            for file in os.listdir(self.result_dir)\n        )\n        if not param_file_exist:\n            # although we save params log during training, but sometimes we directly evaluate a model\n            # so here we still save params log if param file does not exist\n            # no param file was saved yet, here we save data and params setting\n            save_model_params_log(cfgs, self.result_dir)\n\n    def save_result(self, pred, obs):\n        \"\"\"\n        save the pred value of testing period and obs value\n\n        Parameters\n        ----------\n        pred\n            predictions\n        obs\n            observations\n        pred_name\n            the file name of predictions\n        obs_name\n            the file name of observations\n\n        Returns\n        -------\n        None\n        \"\"\"\n        save_dir = self.result_dir\n        flow_pred_file = os.path.join(save_dir, self.pred_name)\n        flow_obs_file = os.path.join(save_dir, self.obs_name)\n        max_len = max(len(basin) for basin in pred.basin.values)\n        encoding = {\"basin\": {\"dtype\": f\"U{max_len}\"}}\n        pred.to_netcdf(flow_pred_file + \".nc\", encoding=encoding)\n        obs.to_netcdf(flow_obs_file + \".nc\", encoding=encoding)\n\n    def eval_result(self, preds_xr, obss_xr):\n        # types of observations\n        target_col = self.cfgs[\"data_cfgs\"][\"target_cols\"]\n        evaluation_metrics = self.cfgs[\"evaluation_cfgs\"][\"metrics\"]\n        basin_ids = self.cfgs[\"data_cfgs\"][\"object_ids\"]\n        test_path = self.cfgs[\"data_cfgs\"][\"case_dir\"]\n        # Assume object_ids like ['changdian_61561']\n        # fill_nan: \"no\" means ignoring the NaN value;\n        #           \"sum\" means calculate the sum of the following values in the NaN locations.\n        #           For example, observations are [1, nan, nan, 2], and predictions are [0.3, 0.3, 0.3, 1.5].\n        #           Then, \"no\" means [1, 2] v.s. [0.3, 1.5] while \"sum\" means [1, 2] v.s. [0.3 + 0.3 + 0.3, 1.5].\n        #           If it is a str, then all target vars use same fill_nan method;\n        #           elif it is a list, each for a var\n        fill_nan = self.cfgs[\"evaluation_cfgs\"][\"fill_nan\"]\n        #  Then evaluate the model metrics\n        if type(fill_nan) is list and len(fill_nan) != len(target_col):\n            raise ValueError(\"length of fill_nan must be equal to target_col's\")\n        for i, col in enumerate(target_col):\n            eval_log = {}\n            obs = obss_xr[col].to_numpy()\n            pred = preds_xr[col].to_numpy()\n\n            eval_log = calculate_and_record_metrics(\n                obs,\n                pred,\n                evaluation_metrics,\n                col,\n                fill_nan[i] if isinstance(fill_nan, list) else fill_nan,\n                eval_log,\n            )\n            # Create pandas DataFrames from eval_log for each target variable (e.g., streamflow)\n            # Create a dictionary to hold the data for the DataFrame\n            data = {}\n            # Iterate over metrics in eval_log\n            for metric, values in eval_log.items():\n                # Remove 'of streamflow' (or similar) from the metric name\n                clean_metric = metric.replace(f\"of {col}\", \"\").strip()\n\n                # Add the cleaned metric to the data dictionary\n                data[clean_metric] = values\n\n            # Create a DataFrame using object_ids as the index and metrics as columns\n            df = pd.DataFrame(data, index=basin_ids)\n\n            # Define the output file name based on the target variable\n            output_file = os.path.join(test_path, f\"metric_{col}.csv\")\n\n            # Save the DataFrame to a CSV file\n            df.to_csv(output_file, index_label=\"basin_id\")\n\n        # Finally, try to explain model behaviour using shap\n        is_shap = self.cfgs[\"evaluation_cfgs\"][\"explainer\"] == \"shap\"\n        if is_shap:\n            shap_summary_plot(self.model, self.traindataset, self.testdataset)\n            # deep_explain_model_summary_plot(self.model, test_data)\n            # deep_explain_model_heatmap(self.model, test_data)\n\n    def _convert_streamflow_units(self, ds):\n        \"\"\"convert the streamflow units to m^3/s\n\n        Parameters\n        ----------\n        pred : np.array\n            predictions\n\n        Returns\n        -------\n        \"\"\"\n        data_cfgs = self.cfgs[\"data_cfgs\"]\n        source_name = data_cfgs[\"source_cfgs\"][\"source_name\"]\n        source_path = data_cfgs[\"source_cfgs\"][\"source_path\"]\n        other_settings = data_cfgs[\"source_cfgs\"].get(\"other_settings\", {})\n        data_source = data_sources_dict[source_name](source_path, **other_settings)\n        basin_id = data_cfgs[\"object_ids\"]\n        # NOTE: all datasource should have read_area method\n        basin_area = data_source.read_area(basin_id)\n        target_unit = \"m^3/s\"\n        # Get the flow variable name dynamically from config instead of hardcoding \"streamflow\"\n        # NOTE: the first target variable must be the flow variable\n        var_flow = self.cfgs[\"data_cfgs\"][\"target_cols\"][0]\n        streamflow_ds = ds[[var_flow]]\n        ds_ = streamflow_unit_conv(\n            streamflow_ds, basin_area, target_unit=target_unit, inverse=True\n        )\n        new_ds = ds.copy(deep=True)\n        new_ds[var_flow] = ds_[var_flow]\n        return new_ds\n\n    def load_result(self, convert_flow_unit=False) -&gt; Tuple[np.array, np.array]:\n        \"\"\"load the pred value of testing period and obs value\"\"\"\n        save_dir = self.result_dir\n        pred_file = os.path.join(save_dir, self.pred_name + \".nc\")\n        obs_file = os.path.join(save_dir, self.obs_name + \".nc\")\n        pred = xr.open_dataset(pred_file)\n        obs = xr.open_dataset(obs_file)\n        if convert_flow_unit:\n            pred = self._convert_streamflow_units(pred)\n            obs = self._convert_streamflow_units(obs)\n        return pred, obs\n\n    def save_intermediate_results(self, **kwargs):\n        \"\"\"Load model weights and deal with some intermediate results\"\"\"\n        is_cell_states = kwargs.get(\"is_cell_states\", False)\n        is_pbm_params = kwargs.get(\"is_pbm_params\", False)\n        cfgs = self.cfgs\n        cfgs[\"training_cfgs\"][\"train_mode\"] = False\n        training_cfgs = cfgs[\"training_cfgs\"]\n        seq_first = training_cfgs[\"which_first_tensor\"] == \"sequence\"\n        if is_cell_states:\n            # TODO: not support return_cell_states yet\n            return cellstates_when_inference(seq_first, data_cfgs, pred)\n        if is_pbm_params:\n            self._save_pbm_params(cfgs, seq_first)\n\n    def _save_pbm_params(self, cfgs, seq_first):\n        training_cfgs = cfgs[\"training_cfgs\"]\n        model_loader = cfgs[\"evaluation_cfgs\"][\"model_loader\"]\n        model_pth_dir = cfgs[\"data_cfgs\"][\"case_dir\"]\n        weight_path = read_pth_from_model_loader(model_loader, model_pth_dir)\n        cfgs[\"model_cfgs\"][\"weight_path\"] = weight_path\n        cfgs[\"training_cfgs\"][\"device\"] = [0] if torch.cuda.is_available() else [-1]\n        deephydro = DeepHydro(cfgs)\n        device = deephydro.device\n        dl_model = deephydro.model.dl_model\n        pb_model = deephydro.model.pb_model\n        param_func = deephydro.model.param_func\n        # TODO: check for dplnnmodule model\n        param_test_way = deephydro.model.param_test_way\n        test_dataloader = DataLoader(\n            deephydro.testdataset,\n            batch_size=training_cfgs[\"batch_size\"],\n            shuffle=False,\n            sampler=None,\n            batch_sampler=None,\n            drop_last=False,\n            timeout=0,\n            worker_init_fn=None,\n        )\n        deephydro.model.eval()\n        # here the batch is just an index of lookup table, so any batch size could be chosen\n        params_lst = []\n        with torch.no_grad():\n            for batch in test_dataloader:\n                ys, gen = model_infer(seq_first, device, dl_model, batch)\n                # we set all params' values in [0, 1] and will scale them when forwarding\n                if param_func == \"clamp\":\n                    params_ = torch.clamp(gen, min=0.0, max=1.0)\n                elif param_func == \"sigmoid\":\n                    params_ = F.sigmoid(gen)\n                else:\n                    raise NotImplementedError(\n                        \"We don't provide this way to limit parameters' range!! Please choose sigmoid or clamp\"\n                    )\n                # just get one-period values, here we use the final period's values\n                params = params_[:, -1, :]\n                params_lst.append(params)\n        pb_params = reduce(lambda a, b: torch.cat((a, b), dim=0), params_lst)\n        # trans tensor to pandas dataframe\n        sites = deephydro.cfgs[\"data_cfgs\"][\"object_ids\"]\n        params_names = pb_model.params_names\n        params_df = pd.DataFrame(\n            pb_params.cpu().numpy(), columns=params_names, index=sites\n        )\n        save_param_file = os.path.join(\n            model_pth_dir, f\"pb_params_{int(time.time())}.csv\"\n        )\n        params_df.to_csv(save_param_file, index_label=\"GAGE_ID\")\n\n    def read_tensorboard_log(self, **kwargs):\n        \"\"\"read tensorboard log files\"\"\"\n        is_scalar = kwargs.get(\"is_scalar\", False)\n        is_histogram = kwargs.get(\"is_histogram\", False)\n        log_dir = self.cfgs[\"data_cfgs\"][\"case_dir\"]\n        if is_scalar:\n            scalar_file = os.path.join(log_dir, \"tb_scalars.csv\")\n            if not os.path.exists(scalar_file):\n                reader = SummaryReader(log_dir)\n                df_scalar = reader.scalars\n                df_scalar.to_csv(scalar_file, index=False)\n            else:\n                df_scalar = pd.read_csv(scalar_file)\n        if is_histogram:\n            histogram_file = os.path.join(log_dir, \"tb_histograms.csv\")\n            if not os.path.exists(histogram_file):\n                reader = SummaryReader(log_dir)\n                df_histogram = reader.histograms\n                df_histogram.to_csv(histogram_file, index=False)\n            else:\n                df_histogram = pd.read_csv(histogram_file)\n        if is_scalar and is_histogram:\n            return df_scalar, df_histogram\n        elif is_scalar:\n            return df_scalar\n        elif is_histogram:\n            return df_histogram\n\n    # TODO: the following code is not finished yet\n    def load_ensemble_result(\n        self, save_dirs, test_epoch, flow_unit=\"m3/s\", basin_areas=None\n    ) -&gt; Tuple[np.array, np.array]:\n        \"\"\"\n        load ensemble mean value\n\n        Parameters\n        ----------\n        save_dirs\n        test_epoch\n        flow_unit\n            default is m3/s, if it is not m3/s, transform the results\n        basin_areas\n            if unit is mm/day it will be used, default is None\n\n        Returns\n        -------\n\n        \"\"\"\n        preds = []\n        obss = []\n        for save_dir in save_dirs:\n            pred_i, obs_i = self.load_result(save_dir, test_epoch)\n            if pred_i.ndim == 3 and pred_i.shape[-1] == 1:\n                pred_i = pred_i.reshape(pred_i.shape[0], pred_i.shape[1])\n                obs_i = obs_i.reshape(obs_i.shape[0], obs_i.shape[1])\n            preds.append(pred_i)\n            obss.append(obs_i)\n        preds_np = np.array(preds)\n        obss_np = np.array(obss)\n        pred_mean = np.mean(preds_np, axis=0)\n        obs_mean = np.mean(obss_np, axis=0)\n        if flow_unit == \"mm/day\":\n            if basin_areas is None:\n                raise ArithmeticError(\"No basin areas we cannot calculate\")\n            basin_areas = np.repeat(basin_areas, obs_mean.shape[1], axis=0).reshape(\n                obs_mean.shape\n            )\n            obs_mean = obs_mean * basin_areas * 1e-3 * 1e6 / 86400\n            pred_mean = pred_mean * basin_areas * 1e-3 * 1e6 / 86400\n        elif flow_unit == \"m3/s\":\n            pass\n        elif flow_unit == \"ft3/s\":\n            obs_mean = obs_mean / 35.314666721489\n            pred_mean = pred_mean / 35.314666721489\n        return pred_mean, obs_mean\n\n    def eval_ensemble_result(\n        self,\n        save_dirs,\n        test_epoch,\n        return_value=False,\n        flow_unit=\"m3/s\",\n        basin_areas=None,\n    ) -&gt; Tuple[np.array, np.array]:\n        \"\"\"calculate statistics for ensemble results\n\n        Parameters\n        ----------\n        save_dirs : _type_\n            where the results save\n        test_epoch : _type_\n            we name the results files with the test_epoch\n        return_value : bool, optional\n            if True, return (inds_df, pred_mean, obs_mean), by default False\n        flow_unit : str, optional\n            arg for load_ensemble_result, by default \"m3/s\"\n        basin_areas : _type_, optional\n            arg for load_ensemble_result, by default None\n\n        Returns\n        -------\n        Tuple[np.array, np.array]\n            inds_df or (inds_df, pred_mean, obs_mean)\n        \"\"\"\n        pred_mean, obs_mean = self.load_ensemble_result(\n            save_dirs, test_epoch, flow_unit=flow_unit, basin_areas=basin_areas\n        )\n        inds = stat_error(obs_mean, pred_mean)\n        inds_df = pd.DataFrame(inds)\n        return (inds_df, pred_mean, obs_mean) if return_value else inds_df\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.resulter.Resulter.eval_ensemble_result","title":"<code>eval_ensemble_result(self, save_dirs, test_epoch, return_value=False, flow_unit='m3/s', basin_areas=None)</code>","text":"<p>calculate statistics for ensemble results</p>"},{"location":"api/trainers/#torchhydro.trainers.resulter.Resulter.eval_ensemble_result--parameters","title":"Parameters","text":"<p>save_dirs : type     where the results save test_epoch : type     we name the results files with the test_epoch return_value : bool, optional     if True, return (inds_df, pred_mean, obs_mean), by default False flow_unit : str, optional     arg for load_ensemble_result, by default \"m3/s\" basin_areas : type, optional     arg for load_ensemble_result, by default None</p>"},{"location":"api/trainers/#torchhydro.trainers.resulter.Resulter.eval_ensemble_result--returns","title":"Returns","text":"<p>Tuple[np.array, np.array]     inds_df or (inds_df, pred_mean, obs_mean)</p> Source code in <code>torchhydro/trainers/resulter.py</code> <pre><code>def eval_ensemble_result(\n    self,\n    save_dirs,\n    test_epoch,\n    return_value=False,\n    flow_unit=\"m3/s\",\n    basin_areas=None,\n) -&gt; Tuple[np.array, np.array]:\n    \"\"\"calculate statistics for ensemble results\n\n    Parameters\n    ----------\n    save_dirs : _type_\n        where the results save\n    test_epoch : _type_\n        we name the results files with the test_epoch\n    return_value : bool, optional\n        if True, return (inds_df, pred_mean, obs_mean), by default False\n    flow_unit : str, optional\n        arg for load_ensemble_result, by default \"m3/s\"\n    basin_areas : _type_, optional\n        arg for load_ensemble_result, by default None\n\n    Returns\n    -------\n    Tuple[np.array, np.array]\n        inds_df or (inds_df, pred_mean, obs_mean)\n    \"\"\"\n    pred_mean, obs_mean = self.load_ensemble_result(\n        save_dirs, test_epoch, flow_unit=flow_unit, basin_areas=basin_areas\n    )\n    inds = stat_error(obs_mean, pred_mean)\n    inds_df = pd.DataFrame(inds)\n    return (inds_df, pred_mean, obs_mean) if return_value else inds_df\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.resulter.Resulter.load_ensemble_result","title":"<code>load_ensemble_result(self, save_dirs, test_epoch, flow_unit='m3/s', basin_areas=None)</code>","text":"<p>load ensemble mean value</p>"},{"location":"api/trainers/#torchhydro.trainers.resulter.Resulter.load_ensemble_result--parameters","title":"Parameters","text":"<p>save_dirs test_epoch flow_unit     default is m3/s, if it is not m3/s, transform the results basin_areas     if unit is mm/day it will be used, default is None</p>"},{"location":"api/trainers/#torchhydro.trainers.resulter.Resulter.load_ensemble_result--returns","title":"Returns","text":"Source code in <code>torchhydro/trainers/resulter.py</code> <pre><code>def load_ensemble_result(\n    self, save_dirs, test_epoch, flow_unit=\"m3/s\", basin_areas=None\n) -&gt; Tuple[np.array, np.array]:\n    \"\"\"\n    load ensemble mean value\n\n    Parameters\n    ----------\n    save_dirs\n    test_epoch\n    flow_unit\n        default is m3/s, if it is not m3/s, transform the results\n    basin_areas\n        if unit is mm/day it will be used, default is None\n\n    Returns\n    -------\n\n    \"\"\"\n    preds = []\n    obss = []\n    for save_dir in save_dirs:\n        pred_i, obs_i = self.load_result(save_dir, test_epoch)\n        if pred_i.ndim == 3 and pred_i.shape[-1] == 1:\n            pred_i = pred_i.reshape(pred_i.shape[0], pred_i.shape[1])\n            obs_i = obs_i.reshape(obs_i.shape[0], obs_i.shape[1])\n        preds.append(pred_i)\n        obss.append(obs_i)\n    preds_np = np.array(preds)\n    obss_np = np.array(obss)\n    pred_mean = np.mean(preds_np, axis=0)\n    obs_mean = np.mean(obss_np, axis=0)\n    if flow_unit == \"mm/day\":\n        if basin_areas is None:\n            raise ArithmeticError(\"No basin areas we cannot calculate\")\n        basin_areas = np.repeat(basin_areas, obs_mean.shape[1], axis=0).reshape(\n            obs_mean.shape\n        )\n        obs_mean = obs_mean * basin_areas * 1e-3 * 1e6 / 86400\n        pred_mean = pred_mean * basin_areas * 1e-3 * 1e6 / 86400\n    elif flow_unit == \"m3/s\":\n        pass\n    elif flow_unit == \"ft3/s\":\n        obs_mean = obs_mean / 35.314666721489\n        pred_mean = pred_mean / 35.314666721489\n    return pred_mean, obs_mean\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.resulter.Resulter.load_result","title":"<code>load_result(self, convert_flow_unit=False)</code>","text":"<p>load the pred value of testing period and obs value</p> Source code in <code>torchhydro/trainers/resulter.py</code> <pre><code>def load_result(self, convert_flow_unit=False) -&gt; Tuple[np.array, np.array]:\n    \"\"\"load the pred value of testing period and obs value\"\"\"\n    save_dir = self.result_dir\n    pred_file = os.path.join(save_dir, self.pred_name + \".nc\")\n    obs_file = os.path.join(save_dir, self.obs_name + \".nc\")\n    pred = xr.open_dataset(pred_file)\n    obs = xr.open_dataset(obs_file)\n    if convert_flow_unit:\n        pred = self._convert_streamflow_units(pred)\n        obs = self._convert_streamflow_units(obs)\n    return pred, obs\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.resulter.Resulter.read_tensorboard_log","title":"<code>read_tensorboard_log(self, **kwargs)</code>","text":"<p>read tensorboard log files</p> Source code in <code>torchhydro/trainers/resulter.py</code> <pre><code>def read_tensorboard_log(self, **kwargs):\n    \"\"\"read tensorboard log files\"\"\"\n    is_scalar = kwargs.get(\"is_scalar\", False)\n    is_histogram = kwargs.get(\"is_histogram\", False)\n    log_dir = self.cfgs[\"data_cfgs\"][\"case_dir\"]\n    if is_scalar:\n        scalar_file = os.path.join(log_dir, \"tb_scalars.csv\")\n        if not os.path.exists(scalar_file):\n            reader = SummaryReader(log_dir)\n            df_scalar = reader.scalars\n            df_scalar.to_csv(scalar_file, index=False)\n        else:\n            df_scalar = pd.read_csv(scalar_file)\n    if is_histogram:\n        histogram_file = os.path.join(log_dir, \"tb_histograms.csv\")\n        if not os.path.exists(histogram_file):\n            reader = SummaryReader(log_dir)\n            df_histogram = reader.histograms\n            df_histogram.to_csv(histogram_file, index=False)\n        else:\n            df_histogram = pd.read_csv(histogram_file)\n    if is_scalar and is_histogram:\n        return df_scalar, df_histogram\n    elif is_scalar:\n        return df_scalar\n    elif is_histogram:\n        return df_histogram\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.resulter.Resulter.save_intermediate_results","title":"<code>save_intermediate_results(self, **kwargs)</code>","text":"<p>Load model weights and deal with some intermediate results</p> Source code in <code>torchhydro/trainers/resulter.py</code> <pre><code>def save_intermediate_results(self, **kwargs):\n    \"\"\"Load model weights and deal with some intermediate results\"\"\"\n    is_cell_states = kwargs.get(\"is_cell_states\", False)\n    is_pbm_params = kwargs.get(\"is_pbm_params\", False)\n    cfgs = self.cfgs\n    cfgs[\"training_cfgs\"][\"train_mode\"] = False\n    training_cfgs = cfgs[\"training_cfgs\"]\n    seq_first = training_cfgs[\"which_first_tensor\"] == \"sequence\"\n    if is_cell_states:\n        # TODO: not support return_cell_states yet\n        return cellstates_when_inference(seq_first, data_cfgs, pred)\n    if is_pbm_params:\n        self._save_pbm_params(cfgs, seq_first)\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.resulter.Resulter.save_result","title":"<code>save_result(self, pred, obs)</code>","text":"<p>save the pred value of testing period and obs value</p>"},{"location":"api/trainers/#torchhydro.trainers.resulter.Resulter.save_result--parameters","title":"Parameters","text":"<p>pred     predictions obs     observations pred_name     the file name of predictions obs_name     the file name of observations</p>"},{"location":"api/trainers/#torchhydro.trainers.resulter.Resulter.save_result--returns","title":"Returns","text":"<p>None</p> Source code in <code>torchhydro/trainers/resulter.py</code> <pre><code>def save_result(self, pred, obs):\n    \"\"\"\n    save the pred value of testing period and obs value\n\n    Parameters\n    ----------\n    pred\n        predictions\n    obs\n        observations\n    pred_name\n        the file name of predictions\n    obs_name\n        the file name of observations\n\n    Returns\n    -------\n    None\n    \"\"\"\n    save_dir = self.result_dir\n    flow_pred_file = os.path.join(save_dir, self.pred_name)\n    flow_obs_file = os.path.join(save_dir, self.obs_name)\n    max_len = max(len(basin) for basin in pred.basin.values)\n    encoding = {\"basin\": {\"dtype\": f\"U{max_len}\"}}\n    pred.to_netcdf(flow_pred_file + \".nc\", encoding=encoding)\n    obs.to_netcdf(flow_obs_file + \".nc\", encoding=encoding)\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_logger","title":"<code>train_logger</code>","text":"<p>Author: Wenyu Ouyang Date: 2021-12-31 11:08:29 LastEditTime: 2025-11-07 08:36:50 LastEditors: Wenyu Ouyang Description: Training function for DL models FilePath:       orchhydro       orchhydro       rainers rain_logger.py Copyright (c) 2021-2022 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/trainers/#torchhydro.trainers.train_logger.TrainLogger","title":"<code> TrainLogger        </code>","text":"Source code in <code>torchhydro/trainers/train_logger.py</code> <pre><code>class TrainLogger:\n    def __init__(self, model_filepath, params, opt):\n        self.training_cfgs = params[\"training_cfgs\"]\n        self.data_cfgs = params[\"data_cfgs\"]\n        self.evaluation_cfgs = params[\"evaluation_cfgs\"]\n        self.model_cfgs = params[\"model_cfgs\"]\n        self.opt = opt\n        self.training_save_dir = model_filepath\n        self.tb = SummaryWriter(self.training_save_dir)\n        self.session_params = []\n        self.train_time = []\n        # log loss for each epoch\n        self.epoch_loss = []\n        # reload previous logs if continue_train is True and weight_path is not None\n        if (\n            self.model_cfgs[\"continue_train\"]\n            and self.model_cfgs[\"weight_path\"] is not None\n        ):\n            the_logger_file = get_lastest_logger_file_in_a_dir(self.training_save_dir)\n            if the_logger_file is not None:\n                with open(the_logger_file, \"r\") as f:\n                    logs = json.load(f)\n            start_epoch = self.training_cfgs[\"start_epoch\"]\n            # read the logs before start_epoch and load them to session_params, train_time, epoch_loss\n            for log in logs[\"run\"]:\n                if log[\"epoch\"] &lt; start_epoch:\n                    self.session_params.append(log)\n                    self.train_time.append(log[\"train_time\"])\n                    self.epoch_loss.append(float(log[\"train_loss\"]))\n\n    def save_session_param(\n        self, epoch, total_loss, n_iter_ep, valid_loss=None, valid_metrics=None\n    ):\n        if valid_metrics is None:\n            if valid_loss is None:\n                epoch_params = {\n                    \"epoch\": epoch,\n                    \"train_loss\": str(total_loss),\n                    \"iter_num\": n_iter_ep,\n                }\n            else:\n                epoch_params = {\n                    \"epoch\": epoch,\n                    \"train_loss\": str(total_loss),\n                    \"validation_loss\": str(valid_loss),\n                    \"iter_num\": n_iter_ep,\n                }\n        else:\n            epoch_params = {\n                \"epoch\": epoch,\n                \"train_loss\": str(total_loss),\n                \"validation_loss\": str(valid_loss),\n                \"validation_metric\": valid_metrics,\n                \"iter_num\": n_iter_ep,\n            }\n        epoch_params[\"train_time\"] = self.train_time[epoch - 1]\n        self.session_params.append(epoch_params)\n\n    @contextmanager\n    def log_epoch_train(self, epoch):\n        start_time = time.time()\n        logs = {}\n        # here content in the 'with' block will be performed after yeild\n        yield logs\n        total_loss = logs[\"train_loss\"]\n        elapsed_time = time.time() - start_time\n        lr = self.opt.param_groups[0][\"lr\"]\n        log_str = \"Epoch {} Loss {:.4f} time {:.2f} lr {}\".format(\n            epoch, total_loss, elapsed_time, lr\n        )\n        print(log_str)\n        model = logs[\"model\"]\n        print(model)\n        self.tb.add_scalar(\"Loss\", total_loss, epoch)\n        # self.plot_hist_img(model, epoch)\n        self.train_time.append(log_str)\n        self.epoch_loss.append(total_loss)\n\n    @contextmanager\n    def log_epoch_valid(self, epoch):\n        logs = {}\n        yield logs\n        valid_loss = logs[\"valid_loss\"]\n        if (\n            self.training_cfgs[\"valid_batch_mode\"] == \"test\"\n            and self.training_cfgs[\"calc_metrics\"]\n        ):\n            # NOTE: Now we only evaluate the metrics for test-mode validation\n            valid_metrics = logs[\"valid_metrics\"]\n            val_log = \"Epoch {} Valid Loss {:.4f} Valid Metric {}\".format(\n                epoch, valid_loss, valid_metrics\n            )\n            print(val_log)\n            self.tb.add_scalar(\"ValidLoss\", valid_loss, epoch)\n            target_col = self.data_cfgs[\"target_cols\"]\n            evaluation_metrics = self.evaluation_cfgs[\"metrics\"]\n            for i in range(len(target_col)):\n                for evaluation_metric in evaluation_metrics:\n                    self.tb.add_scalar(\n                        f\"Valid{target_col[i]}{evaluation_metric}mean\",\n                        np.nanmean(\n                            valid_metrics[f\"{evaluation_metric} of {target_col[i]}\"]\n                        ),\n                        epoch,\n                    )\n                    self.tb.add_scalar(\n                        f\"Valid{target_col[i]}{evaluation_metric}median\",\n                        np.nanmedian(\n                            valid_metrics[f\"{evaluation_metric} of {target_col[i]}\"]\n                        ),\n                        epoch,\n                    )\n        else:\n            val_log = \"Epoch {} Valid Loss {:.4f} \".format(epoch, valid_loss)\n            print(val_log)\n            self.tb.add_scalar(\"ValidLoss\", valid_loss, epoch)\n\n    def save_model_and_params(self, model, epoch, params):\n        final_epoch = params[\"training_cfgs\"][\"epochs\"]\n        save_epoch = params[\"training_cfgs\"][\"save_epoch\"]\n        if save_epoch is None or save_epoch == 0 and epoch != final_epoch:\n            return\n        if (save_epoch &gt; 0 and epoch % save_epoch == 0) or epoch == final_epoch:\n            # save for save_epoch\n            model_file = os.path.join(\n                self.training_save_dir, f\"model_Ep{str(epoch)}.pth\"\n            )\n            save_model(model, model_file)\n        if epoch == final_epoch:\n            self._save_final_epoch(params, model)\n\n    def _save_final_epoch(self, params, model):\n        # In final epoch, we save the model and params in case_dir\n        final_path = params[\"data_cfgs\"][\"case_dir\"]\n        params[\"run\"] = self.session_params\n        time_stamp = datetime.now().strftime(\"%d_%B_%Y%I_%M%p\")\n        model_save_path = os.path.join(final_path, f\"{time_stamp}_model.pth\")\n        save_model(model, model_save_path)\n        save_model_params_log(params, final_path)\n        # also save one for a training directory for one hyperparameter setting\n        save_model_params_log(params, self.training_save_dir)\n\n    def plot_hist_img(self, model, global_step):\n        for tag, parm in model.named_parameters():\n            self.tb.add_histogram(\n                f\"{tag}_hist\", parm.detach().cpu().numpy(), global_step\n            )\n            if len(parm.shape) == 2:\n                img_format = \"HW\"\n                if parm.shape[0] &gt; parm.shape[1]:\n                    img_format = \"WH\"\n                    self.tb.add_image(\n                        f\"{tag}_img\",\n                        parm.detach().cpu().numpy(),\n                        global_step,\n                        dataformats=img_format,\n                    )\n\n    def plot_model_structure(self, model):\n        \"\"\"plot model structure in tensorboard\n\n        TODO: This function is not working as expected. It should be rewritten.\n\n        Parameters\n        ----------\n        model :\n            torch model\n        \"\"\"\n        # input4modelplot = torch.randn(\n        #     self.training_cfgs[\"batch_size\"],\n        #     self.training_cfgs[\"hindcast_length\"],\n        #     # self.model_cfgs[\"model_hyperparam\"][\"n_input_features\"],\n        #     self.model_cfgs[\"model_hyperparam\"][\"input_size\"],\n        # )\n        if self.data_cfgs[\"model_mode\"] == \"single\":\n            input4modelplot = [\n                torch.randn(\n                    self.training_cfgs[\"batch_size\"],\n                    self.training_cfgs[\"hindcast_length\"],\n                    self.data_cfgs[\"input_features\"] - 1,\n                ),\n                torch.randn(\n                    self.training_cfgs[\"batch_size\"],\n                    self.training_cfgs[\"hindcast_length\"],\n                    self.data_cfgs[\"cnn_size\"],\n                ),\n                torch.rand(\n                    self.training_cfgs[\"batch_size\"],\n                    1,\n                    self.data_cfgs[\"output_features\"],\n                ),\n            ]\n        else:\n            input4modelplot = [\n                torch.randn(\n                    self.training_cfgs[\"batch_size\"],\n                    self.training_cfgs[\"hindcast_length\"],\n                    self.data_cfgs[\"input_features\"],\n                ),\n                torch.randn(\n                    self.training_cfgs[\"batch_size\"],\n                    self.training_cfgs[\"hindcast_length\"],\n                    self.data_cfgs[\"input_size_encoder2\"],\n                ),\n                torch.rand(\n                    self.training_cfgs[\"batch_size\"],\n                    1,\n                    self.data_cfgs[\"output_features\"],\n                ),\n            ]\n        self.tb.add_graph(model, input4modelplot)\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_logger.TrainLogger.plot_model_structure","title":"<code>plot_model_structure(self, model)</code>","text":"<p>plot model structure in tensorboard</p> <p>TODO: This function is not working as expected. It should be rewritten.</p>"},{"location":"api/trainers/#torchhydro.trainers.train_logger.TrainLogger.plot_model_structure--parameters","title":"Parameters","text":"<p>model :     torch model</p> Source code in <code>torchhydro/trainers/train_logger.py</code> <pre><code>def plot_model_structure(self, model):\n    \"\"\"plot model structure in tensorboard\n\n    TODO: This function is not working as expected. It should be rewritten.\n\n    Parameters\n    ----------\n    model :\n        torch model\n    \"\"\"\n    # input4modelplot = torch.randn(\n    #     self.training_cfgs[\"batch_size\"],\n    #     self.training_cfgs[\"hindcast_length\"],\n    #     # self.model_cfgs[\"model_hyperparam\"][\"n_input_features\"],\n    #     self.model_cfgs[\"model_hyperparam\"][\"input_size\"],\n    # )\n    if self.data_cfgs[\"model_mode\"] == \"single\":\n        input4modelplot = [\n            torch.randn(\n                self.training_cfgs[\"batch_size\"],\n                self.training_cfgs[\"hindcast_length\"],\n                self.data_cfgs[\"input_features\"] - 1,\n            ),\n            torch.randn(\n                self.training_cfgs[\"batch_size\"],\n                self.training_cfgs[\"hindcast_length\"],\n                self.data_cfgs[\"cnn_size\"],\n            ),\n            torch.rand(\n                self.training_cfgs[\"batch_size\"],\n                1,\n                self.data_cfgs[\"output_features\"],\n            ),\n        ]\n    else:\n        input4modelplot = [\n            torch.randn(\n                self.training_cfgs[\"batch_size\"],\n                self.training_cfgs[\"hindcast_length\"],\n                self.data_cfgs[\"input_features\"],\n            ),\n            torch.randn(\n                self.training_cfgs[\"batch_size\"],\n                self.training_cfgs[\"hindcast_length\"],\n                self.data_cfgs[\"input_size_encoder2\"],\n            ),\n            torch.rand(\n                self.training_cfgs[\"batch_size\"],\n                1,\n                self.data_cfgs[\"output_features\"],\n            ),\n        ]\n    self.tb.add_graph(model, input4modelplot)\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_utils","title":"<code>train_utils</code>","text":"<p>Author: Wenyu Ouyang Date: 2024-04-08 18:16:26 LastEditTime: 2025-11-06 13:48:48 LastEditors: Wenyu Ouyang Description: Some basic functions for training FilePath:       orchhydro       orchhydro       rainers rain_utils.py Copyright (c) 2024-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.EarlyStopper","title":"<code> EarlyStopper        </code>","text":"Source code in <code>torchhydro/trainers/train_utils.py</code> <pre><code>class EarlyStopper(object):\n    def __init__(\n        self,\n        patience: int,\n        min_delta: float = 0.0,\n        cumulative_delta: bool = False,\n    ):\n        \"\"\"\n        EarlyStopping handler can be used to stop the training if no improvement after a given number of events.\n\n        Parameters\n        ----------\n        patience\n            Number of events to wait if no improvement and then stop the training.\n        min_delta\n            A minimum increase in the score to qualify as an improvement,\n            i.e. an increase of less than or equal to `min_delta`, will count as no improvement.\n        cumulative_delta\n            It True, `min_delta` defines an increase since the last `patience` reset, otherwise,\n        it defines an increase after the last event. Default value is False.\n        \"\"\"\n\n        if patience &lt; 1:\n            raise ValueError(\"Argument patience should be positive integer.\")\n\n        if min_delta &lt; 0.0:\n            raise ValueError(\"Argument min_delta should not be a negative number.\")\n\n        self.patience = patience\n        self.min_delta = min_delta\n        self.cumulative_delta = cumulative_delta\n        self.counter = 0\n        self.best_score = None\n\n    def check_loss(self, model, validation_loss, save_dir) -&gt; bool:\n        score = validation_loss\n        if self.best_score is None:\n            self.save_model_checkpoint(model, save_dir)\n            self.best_score = score\n\n        elif score + self.min_delta &gt;= self.best_score:\n            self.counter += 1\n            print(\"Epochs without Model Update:\", self.counter)\n            if self.counter &gt;= self.patience:\n                return False\n        else:\n            self.save_model_checkpoint(model, save_dir)\n            print(\"Model Update\")\n            self.best_score = score\n            self.counter = 0\n        return True\n\n    def save_model_checkpoint(self, model, save_dir):\n        torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.EarlyStopper.__init__","title":"<code>__init__(self, patience, min_delta=0.0, cumulative_delta=False)</code>  <code>special</code>","text":"<p>EarlyStopping handler can be used to stop the training if no improvement after a given number of events.</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.EarlyStopper.__init__--parameters","title":"Parameters","text":"<p>patience     Number of events to wait if no improvement and then stop the training. min_delta     A minimum increase in the score to qualify as an improvement,     i.e. an increase of less than or equal to <code>min_delta</code>, will count as no improvement. cumulative_delta     It True, <code>min_delta</code> defines an increase since the last <code>patience</code> reset, otherwise, it defines an increase after the last event. Default value is False.</p> Source code in <code>torchhydro/trainers/train_utils.py</code> <pre><code>def __init__(\n    self,\n    patience: int,\n    min_delta: float = 0.0,\n    cumulative_delta: bool = False,\n):\n    \"\"\"\n    EarlyStopping handler can be used to stop the training if no improvement after a given number of events.\n\n    Parameters\n    ----------\n    patience\n        Number of events to wait if no improvement and then stop the training.\n    min_delta\n        A minimum increase in the score to qualify as an improvement,\n        i.e. an increase of less than or equal to `min_delta`, will count as no improvement.\n    cumulative_delta\n        It True, `min_delta` defines an increase since the last `patience` reset, otherwise,\n    it defines an increase after the last event. Default value is False.\n    \"\"\"\n\n    if patience &lt; 1:\n        raise ValueError(\"Argument patience should be positive integer.\")\n\n    if min_delta &lt; 0.0:\n        raise ValueError(\"Argument min_delta should not be a negative number.\")\n\n    self.patience = patience\n    self.min_delta = min_delta\n    self.cumulative_delta = cumulative_delta\n    self.counter = 0\n    self.best_score = None\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.average_weights","title":"<code>average_weights(w)</code>","text":"<p>Returns the average of the weights.</p> Source code in <code>torchhydro/trainers/train_utils.py</code> <pre><code>def average_weights(w):\n    \"\"\"\n    Returns the average of the weights.\n    \"\"\"\n    w_avg = copy.deepcopy(w[0])\n    for key in w_avg.keys():\n        for i in range(1, len(w)):\n            w_avg[key] += w[i][key]\n        w_avg[key] = torch.div(w_avg[key], len(w))\n    return w_avg\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.cellstates_when_inference","title":"<code>cellstates_when_inference(seq_first, data_cfgs, pred)</code>","text":"<p>get cell states when inference</p> Source code in <code>torchhydro/trainers/train_utils.py</code> <pre><code>def cellstates_when_inference(seq_first, data_cfgs, pred):\n    \"\"\"get cell states when inference\"\"\"\n    cs_out = (\n        cs_cat_lst.detach().cpu().numpy().swapaxes(0, 1)\n        if seq_first\n        else cs_cat_lst.detach().cpu().numpy()\n    )\n    cs_out_lst = [cs_out]\n    cell_state = reduce(lambda a, b: np.vstack((a, b)), cs_out_lst)\n    np.save(os.path.join(data_cfgs[\"case_dir\"], \"cell_states.npy\"), cell_state)\n    # model.zero_grad()\n    torch.cuda.empty_cache()\n    return pred, cell_state\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.compute_loss","title":"<code>compute_loss(labels, output, criterion, **kwargs)</code>","text":"<p>Function for computing the loss</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.compute_loss--parameters","title":"Parameters","text":"<p>labels     The real values for the target. Shape can be variable but should follow (batch_size, time) output     The output of the model criterion     loss function validation_dataset     Only passed when unscaling of data is needed. m     defaults to 1</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.compute_loss--returns","title":"Returns","text":"<p>torch.Tensor     the computed loss</p> Source code in <code>torchhydro/trainers/train_utils.py</code> <pre><code>def compute_loss(\n    labels: torch.Tensor, output: torch.Tensor, criterion, **kwargs\n) -&gt; torch.Tensor:\n    \"\"\"\n    Function for computing the loss\n\n    Parameters\n    ----------\n    labels\n        The real values for the target. Shape can be variable but should follow (batch_size, time)\n    output\n        The output of the model\n    criterion\n        loss function\n    validation_dataset\n        Only passed when unscaling of data is needed.\n    m\n        defaults to 1\n\n    Returns\n    -------\n    torch.Tensor\n        the computed loss\n    \"\"\"\n    # a = np.sum(output.cpu().detach().numpy(),axis=1)/len(output)\n    # b=[]\n    # for i in a:\n    #     b.append([i.tolist()])\n    # output = torch.tensor(b, requires_grad=True).to(torch.device(\"cuda\"))\n\n    if isinstance(criterion, GaussianLoss):\n        if len(output[0].shape) &gt; 2:\n            g_loss = GaussianLoss(output[0][:, :, 0], output[1][:, :, 0])\n        else:\n            g_loss = GaussianLoss(output[0][:, 0], output[1][:, 0])\n        return g_loss(labels)\n    if isinstance(criterion, FloodBaseLoss):\n        # labels has one more column than output, which is the flood mask\n        # so we need to remove the last column of labels to get targets\n        flood_mask = labels[:, :, -1:]  # Extract flood mask from last column\n        targets = labels[:, :, :-1]  # Extract targets (remove last column)\n        return criterion(output, targets, flood_mask)\n    if (\n        isinstance(output, torch.Tensor)\n        and len(labels.shape) != len(output.shape)\n        and len(labels.shape) &gt; 1\n    ):\n        if labels.shape[1] == output.shape[1]:\n            labels = labels.unsqueeze(2)\n        else:\n            labels = labels.unsqueeze(0)\n    assert labels.shape == output.shape\n    return criterion(output, labels.float())\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.compute_validation","title":"<code>compute_validation(model, criterion, data_loader, device=None, **kwargs)</code>","text":"<p>Function to compute the validation loss metrics</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.compute_validation--parameters","title":"Parameters","text":"<p>model     the trained model criterion     torch.nn.modules.loss dataloader     The data-loader of either validation or test-data device     torch.device</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.compute_validation--returns","title":"Returns","text":"<p>tuple     validation observations (numpy array), predictions (numpy array) and the loss of validation</p> Source code in <code>torchhydro/trainers/train_utils.py</code> <pre><code>def compute_validation(\n    model,\n    criterion,\n    data_loader: DataLoader,\n    device: torch.device = None,\n    **kwargs,\n):\n    \"\"\"\n    Function to compute the validation loss metrics\n\n    Parameters\n    ----------\n    model\n        the trained model\n    criterion\n        torch.nn.modules.loss\n    dataloader\n        The data-loader of either validation or test-data\n    device\n        torch.device\n\n    Returns\n    -------\n    tuple\n        validation observations (numpy array), predictions (numpy array) and the loss of validation\n    \"\"\"\n    model.eval()\n    seq_first = kwargs[\"which_first_tensor\"] != \"batch\"\n    obs = []\n    preds = []\n    valid_loss = 0.0\n    obs_final = None\n    pred_final = None\n    with torch.no_grad():\n        iter_num = 0\n        for batch in tqdm(data_loader, desc=\"Evaluating\", total=len(data_loader)):\n            trg, output = model_infer(seq_first, device, model, batch)\n            obs.append(trg)\n            preds.append(output)\n            valid_loss_ = compute_loss(trg, output, criterion)\n            if torch.isnan(valid_loss_):\n                # for not-train mode, we may get all nan data for trg\n                # so we skip this batch\n                continue\n                print(\"NAN loss detected, skipping this batch\")\n            valid_loss = valid_loss + valid_loss_.item()\n            iter_num = iter_num + 1\n\n            # For flood datasets, remove the flood_mask column from observations\n            # to match the prediction dimensions for evaluation\n            trg_for_eval = (\n                trg[:, :, :-1] if isinstance(criterion, FloodBaseLoss) else trg\n            )\n            # clear memory to save GPU memory\n            if obs_final is None:\n                obs_final = trg_for_eval.detach().cpu()\n                pred_final = output.detach().cpu()\n            else:\n                obs_final = torch.cat([obs_final, trg_for_eval.detach().cpu()], dim=0)\n                pred_final = torch.cat([pred_final, output.detach().cpu()], dim=0)\n            del trg, output\n            torch.cuda.empty_cache()\n    valid_loss = valid_loss / iter_num\n    y_obs = obs_final.numpy()\n    y_pred = pred_final.numpy()\n    return y_obs, y_pred, valid_loss\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.evaluate_validation","title":"<code>evaluate_validation(validation_data_loader, output, labels, evaluation_cfgs, target_col)</code>","text":"<p>calculate metrics for validation</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.evaluate_validation--parameters","title":"Parameters","text":"<p>output     model output labels     model target evaluation_cfgs     evaluation configs target_col     target columns</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.evaluate_validation--returns","title":"Returns","text":"<p>tuple     metrics</p> Source code in <code>torchhydro/trainers/train_utils.py</code> <pre><code>def evaluate_validation(\n    validation_data_loader,\n    output,\n    labels,\n    evaluation_cfgs,\n    target_col,\n):\n    \"\"\"\n    calculate metrics for validation\n\n    Parameters\n    ----------\n    output\n        model output\n    labels\n        model target\n    evaluation_cfgs\n        evaluation configs\n    target_col\n        target columns\n\n    Returns\n    -------\n    tuple\n        metrics\n    \"\"\"\n    fill_nan = evaluation_cfgs[\"fill_nan\"]\n    if isinstance(fill_nan, list) and len(fill_nan) != len(target_col):\n        raise ValueError(\"Length of fill_nan must be equal to length of target_col.\")\n    eval_log = {}\n    evaluation_metrics = evaluation_cfgs[\"metrics\"]\n    obss_xr, preds_xr = get_preds_to_be_eval(\n        validation_data_loader,\n        evaluation_cfgs,\n        output,\n        labels,\n    )\n    # obss_xr_list\n    # preds_xr_list\n    # if type()\n    # for i in range(obs.shape[0]): # \u7b2c\u51e0\u4e2a\u9884\u89c1\u671f\n    ## obs_ = obs[i]\n    if isinstance(obss_xr, list):\n        obss_xr_list = obss_xr\n        preds_xr_list = preds_xr\n        for horizon_idx in range(len(obss_xr_list)):\n            obss_xr = obss_xr_list[horizon_idx]\n            preds_xr = preds_xr_list[horizon_idx]\n            for i, col in enumerate(target_col):\n                obs = obss_xr[col].to_numpy()\n                pred = preds_xr[col].to_numpy()\n                # eval_log will be updated rather than completely replaced, no need to use eval_log[\"key\"]\n                eval_log = calculate_and_record_metrics(\n                    obs,\n                    pred,\n                    evaluation_metrics,\n                    col,\n                    fill_nan[i] if isinstance(fill_nan, list) else fill_nan,\n                    eval_log,\n                    horizon_idx + 1,\n                )\n        return eval_log\n\n    for i, col in enumerate(target_col):\n        obs = obss_xr[col].to_numpy()\n        pred = preds_xr[col].to_numpy()\n        # eval_log will be updated rather than completely replaced, no need to use eval_log[\"key\"]\n        eval_log = calculate_and_record_metrics(\n            obs,\n            pred,\n            evaluation_metrics,\n            col,\n            fill_nan[i] if isinstance(fill_nan, list) else fill_nan,\n            eval_log,\n        )\n    return eval_log\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.get_lastest_logger_file_in_a_dir","title":"<code>get_lastest_logger_file_in_a_dir(dir_path)</code>","text":"<p>Get the last logger file in a directory</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.get_lastest_logger_file_in_a_dir--parameters","title":"Parameters","text":"<p>dir_path : str     the directory</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.get_lastest_logger_file_in_a_dir--returns","title":"Returns","text":"<p>str     the path of the logger file</p> Source code in <code>torchhydro/trainers/train_utils.py</code> <pre><code>def get_lastest_logger_file_in_a_dir(dir_path):\n    \"\"\"Get the last logger file in a directory\n\n    Parameters\n    ----------\n    dir_path : str\n        the directory\n\n    Returns\n    -------\n    str\n        the path of the logger file\n    \"\"\"\n    pattern = r\"^\\d{1,2}_[A-Za-z]+_\\d{6}_\\d{2}(AM|PM)\\.json$\"\n    pth_files_lst = [\n        os.path.join(dir_path, file)\n        for file in os.listdir(dir_path)\n        if re.match(pattern, file)\n    ]\n    return get_latest_file_in_a_lst(pth_files_lst)\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.get_latest_pbm_param_file","title":"<code>get_latest_pbm_param_file(param_dir)</code>","text":"<p>Get the latest parameter file of physics-based models in the current directory.</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.get_latest_pbm_param_file--parameters","title":"Parameters","text":"<p>param_dir : str     The directory of parameter files.</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.get_latest_pbm_param_file--returns","title":"Returns","text":"<p>str     The latest parameter file.</p> Source code in <code>torchhydro/trainers/train_utils.py</code> <pre><code>def get_latest_pbm_param_file(param_dir):\n    \"\"\"Get the latest parameter file of physics-based models in the current directory.\n\n    Parameters\n    ----------\n    param_dir : str\n        The directory of parameter files.\n\n    Returns\n    -------\n    str\n        The latest parameter file.\n    \"\"\"\n    param_file_lst = [\n        os.path.join(param_dir, f)\n        for f in os.listdir(param_dir)\n        if f.startswith(\"pb_params\") and f.endswith(\".csv\")\n    ]\n    param_files = [Path(f) for f in param_file_lst]\n    param_file_names_lst = [param_file.stem.split(\"_\") for param_file in param_files]\n    ctimes = [\n        int(param_file_names[param_file_names.index(\"params\") + 1])\n        for param_file_names in param_file_names_lst\n    ]\n    return param_files[ctimes.index(max(ctimes))] if ctimes else None\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.get_latest_tensorboard_event_file","title":"<code>get_latest_tensorboard_event_file(log_dir)</code>","text":"<p>Get the latest event file in the log_dir directory.</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.get_latest_tensorboard_event_file--parameters","title":"Parameters","text":"<p>log_dir : str     The directory where the event files are stored.</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.get_latest_tensorboard_event_file--returns","title":"Returns","text":"<p>str     The latest event file.</p> Source code in <code>torchhydro/trainers/train_utils.py</code> <pre><code>def get_latest_tensorboard_event_file(log_dir):\n    \"\"\"Get the latest event file in the log_dir directory.\n\n    Parameters\n    ----------\n    log_dir : str\n        The directory where the event files are stored.\n\n    Returns\n    -------\n    str\n        The latest event file.\n    \"\"\"\n    event_file_lst = [\n        os.path.join(log_dir, f) for f in os.listdir(log_dir) if f.startswith(\"events\")\n    ]\n    event_files = [Path(f) for f in event_file_lst]\n    event_file_names_lst = [event_file.stem.split(\".\") for event_file in event_files]\n    ctimes = [\n        int(event_file_names[event_file_names.index(\"tfevents\") + 1])\n        for event_file_names in event_file_names_lst\n    ]\n    return event_files[ctimes.index(max(ctimes))]\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.get_masked_tensors","title":"<code>get_masked_tensors(variable_length_cfgs, batch, seq_first)</code>","text":"<p>Get the mask for the data</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.get_masked_tensors--parameters","title":"Parameters","text":"<p>variable_length_cfgs : dict     The variable length configuration batch : tuple or list     The batch data from collate_fn or dataset seq_first : bool     Whether the data is in sequence first format</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.get_masked_tensors--returns","title":"Returns","text":"<p>tuple     For standard datasets: (xs, ys, xs_mask, ys_mask, xs_lens, ys_lens)     For GNN datasets: (xs, ys, edge_index, edge_weight, xs_mask, ys_mask, xs_lens, ys_lens)     For GNN with batch vector: (xs, ys, edge_index, edge_weight, batch_vector, xs_mask, ys_mask, xs_lens, ys_lens)</p> Source code in <code>torchhydro/trainers/train_utils.py</code> <pre><code>def get_masked_tensors(variable_length_cfgs, batch, seq_first):\n    \"\"\"Get the mask for the data\n\n    Parameters\n    ----------\n    variable_length_cfgs : dict\n        The variable length configuration\n    batch : tuple or list\n        The batch data from collate_fn or dataset\n    seq_first : bool\n        Whether the data is in sequence first format\n\n    Returns\n    -------\n    tuple\n        For standard datasets: (xs, ys, xs_mask, ys_mask, xs_lens, ys_lens)\n        For GNN datasets: (xs, ys, edge_index, edge_weight, xs_mask, ys_mask, xs_lens, ys_lens)\n        For GNN with batch vector: (xs, ys, edge_index, edge_weight, batch_vector, xs_mask, ys_mask, xs_lens, ys_lens)\n    \"\"\"\n    xs_mask = None\n    ys_mask = None\n    xs_lens = None\n    ys_lens = None\n    edge_index = None\n    edge_weight = None\n\n    if variable_length_cfgs is None:\n        # Check batch length to determine format\n        if len(batch) == 5:\n            # GNN batch with batch_vector: [sxc, y, edge_index, edge_weight, batch_vector]\n            xs, ys, edge_index, edge_weight, batch_vector = batch\n            return (\n                xs,\n                ys,\n                edge_index,\n                edge_weight,\n                batch_vector,\n                xs_mask,\n                ys_mask,\n                xs_lens,\n                ys_lens,\n            )\n        elif len(batch) == 4:\n            # GNN batch: [sxc, y, edge_index, edge_weight]\n            xs, ys, edge_index, edge_weight = batch\n            return xs, ys, edge_index, edge_weight, xs_mask, ys_mask, xs_lens, ys_lens\n        else:\n            # Standard batch: [xs, ys]\n            xs, ys = batch[0], batch[1]\n            return xs, ys, xs_mask, ys_mask, xs_lens, ys_lens\n\n    if variable_length_cfgs.get(\"use_variable_length\", False):\n        # When using variable length training, batch comes from varied_length_collate_fn\n        # which returns [xs_pad, ys_pad, xs_lens, ys_lens, xs_mask, ys_mask]\n        if len(batch) &gt;= 6:\n            xs, ys, xs_lens, ys_lens, xs_mask_bool, ys_mask_bool = batch[:6]\n        else:\n            # Fallback: treat as regular batch with first two elements\n            xs, ys = batch[0], batch[1]\n            xs_lens = ys_lens = xs_mask_bool = ys_mask_bool = None\n\n        if xs_mask_bool is None and ys_mask_bool is None:\n            # sometime even you choose to use variable length training, the batch data may still be fixed length\n            # so we need to return the batch data directly\n            return xs, ys, xs_mask_bool, ys_mask_bool, xs_lens, ys_lens\n        # Convert masks to the format expected by model (float tensor with shape [..., 1])\n        xs_mask = xs_mask_bool.unsqueeze(-1).float()  # [batch, seq, 1]\n        ys_mask = ys_mask_bool.unsqueeze(-1).float()  # [batch, seq, 1]\n\n        # Convert to appropriate format for model if needed\n        if seq_first:\n            xs_mask = xs_mask.transpose(0, 1)  # [seq, batch, 1]\n            ys_mask = ys_mask.transpose(0, 1)  # [seq, batch, 1]\n    else:\n        # Check batch length to determine format\n        if len(batch) == 5:\n            # GNN batch with batch_vector: [sxc, y, edge_index, edge_weight, batch_vector]\n            xs, ys, edge_index, edge_weight, batch_vector = batch\n        elif len(batch) == 4:\n            # GNN batch: [sxc, y, edge_index, edge_weight]\n            xs, ys, edge_index, edge_weight = batch\n        else:\n            # Standard batch: [xs, ys]\n            xs, ys = batch[0], batch[1]\n\n    # Return appropriate format based on what we have\n    if edge_index is not None and edge_weight is not None:\n        return (\n            xs,\n            ys,\n            edge_index,\n            edge_weight,\n            batch_vector,\n            xs_mask,\n            ys_mask,\n            xs_lens,\n            ys_lens,\n        )\n    else:\n        return xs, ys, xs_mask, ys_mask, xs_lens, ys_lens\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.get_preds_to_be_eval","title":"<code>get_preds_to_be_eval(valorte_data_loader, evaluation_cfgs, output, labels)</code>","text":"<p>Get prediction results prepared for evaluation: the denormalized data without metrics by different eval ways</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.get_preds_to_be_eval--parameters","title":"Parameters","text":"<p>valorte_data_loader : DataLoader     validation or test data loader evaluation_cfgs : dict     evaluation configs output : np.ndarray     model output labels : np.ndarray     model target</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.get_preds_to_be_eval--returns","title":"Returns","text":"<p>tuple     description</p> Source code in <code>torchhydro/trainers/train_utils.py</code> <pre><code>def get_preds_to_be_eval(\n    valorte_data_loader,\n    evaluation_cfgs,\n    output,\n    labels,\n):\n    \"\"\"\n    Get prediction results prepared for evaluation:\n    the denormalized data without metrics by different eval ways\n\n    Parameters\n    ----------\n    valorte_data_loader : DataLoader\n        validation or test data loader\n    evaluation_cfgs : dict\n        evaluation configs\n    output : np.ndarray\n        model output\n    labels : np.ndarray\n        model target\n\n    Returns\n    -------\n    tuple\n        _description_\n    \"\"\"\n    evaluator = evaluation_cfgs[\"evaluator\"]\n    # this test_rolling means how we perform prediction during testing\n    test_rolling = evaluation_cfgs[\"rolling\"]\n    batch_size = valorte_data_loader.batch_size\n    target_scaler = valorte_data_loader.dataset.target_scaler\n    target_data = target_scaler.data_target\n    rho = valorte_data_loader.dataset.rho\n    horizon = valorte_data_loader.dataset.horizon\n    warmup_length = valorte_data_loader.dataset.warmup_length\n    hindcast_output_window = target_scaler.data_cfgs[\"hindcast_output_window\"]\n    nf = valorte_data_loader.dataset.noutputvar  # number of features\n    # number of time steps after warmup as outputs typically don't include warmup period\n    nt = valorte_data_loader.dataset.nt - warmup_length\n    basin_num = len(target_data.basin)\n    data_shape = (basin_num, nt, nf)\n    if evaluator[\"eval_way\"] == \"once\":\n        stride = evaluator[\"stride\"]\n        if stride &gt; 0:\n            if horizon != stride:\n                raise NotImplementedError(\n                    \"horizon should be equal to stride in evaluator if you chose eval_way to be once, or else you need to change the eval_way to be 1pace or rolling\"\n                )\n            obs = _rolling_preds_for_once_eval(\n                (basin_num, horizon, nf),\n                rho,\n                evaluation_cfgs[\"forecast_length\"],\n                stride,\n                hindcast_output_window,\n                target_data.reshape(basin_num, horizon, nf),\n            )\n            pred = _rolling_preds_for_once_eval(\n                (basin_num, horizon, nf),\n                rho,\n                evaluation_cfgs[\"forecast_length\"],\n                stride,\n                hindcast_output_window,\n                output.reshape(batch_size, horizon, nf),\n            )\n        else:\n            if test_rolling &gt; 0:\n                raise RuntimeError(\n                    \"please set rolling to 0 when you chose eval way as once and stride=0\"\n                )\n            obs = labels.reshape(basin_num, -1, nf)\n            pred = output.reshape(basin_num, -1, nf)\n    elif evaluator[\"eval_way\"] == \"1pace\":\n        if test_rolling &lt; 1:\n            raise NotImplementedError(\n                \"rolling should be larger than 0 if you chose eval_way to be 1pace\"\n            )\n        pace_idx = evaluator[\"pace_idx\"]\n        # stride = evaluator.get(\"stride\", 1)\n        # for 1pace with pace_idx meaning which value of output was chosen to show\n        # 1st, we need to transpose data to 4-dim to show the whole data\n\n        # TODO:check should we select which def\n        pred = _recover_samples_to_basin(output, valorte_data_loader, pace_idx)\n        obs = _recover_samples_to_basin(labels, valorte_data_loader, pace_idx)\n\n    elif evaluator[\"eval_way\"] == \"rolling\":\n        # \u83b7\u53d6\u6eda\u52a8\u9884\u6d4b\u6240\u9700\u7684\u53c2\u6570\n        stride = evaluator.get(\"stride\", 1)\n        if stride != 1:\n            raise NotImplementedError(\n                \"if stride is not equal to 1, we think it is meaningless\"\n            )\n        # \u91cd\u7ec4\u9884\u6d4b\u7ed3\u679c\u548c\u89c2\u6d4b\u503c\n        basin_num = len(target_data.basin)\n\n        # \u65b0\u589e\uff1a\u6839\u636e\u914d\u7f6e\u9009\u62e9\u4e0d\u540c\u7684\u6570\u636e\u7ec4\u7ec7\u65b9\u5f0f\n        recover_mode = evaluator.get(\"recover_mode\", \"bybasins\")\n        stride = evaluator.get(\"stride\", 1)\n        data_shape = (basin_num, nt, nf)\n\n        if recover_mode == \"bybasins\":\n\n            pred = _recover_samples_to_4d_by_basins(\n                data_shape,\n                valorte_data_loader,\n                stride,\n                hindcast_output_window,\n                output,\n            )\n            obs = _recover_samples_to_4d_by_basins(\n                data_shape,\n                valorte_data_loader,\n                stride,\n                hindcast_output_window,\n                labels,\n            )\n        elif recover_mode == \"byforecast\":\n            pred = _recover_samples_to_4d_by_forecast(\n                data_shape,\n                valorte_data_loader,\n                stride,\n                hindcast_output_window,\n                output,  # samples, seq_length, nf\n            )\n            obs = _recover_samples_to_4d_by_forecast(\n                data_shape,\n                valorte_data_loader,\n                stride,\n                hindcast_output_window,\n                labels,\n            )\n        elif recover_mode == \"byensembles\":\n            pred = _recover_samples_to_3d_by_4d_ensembles(\n                data_shape,\n                valorte_data_loader,\n                stride,\n                hindcast_output_window,\n                output,\n            )\n            obs = _recover_samples_to_3d_by_4d_ensembles(\n                data_shape,\n                valorte_data_loader,\n                stride,\n                hindcast_output_window,\n                labels,\n            )\n        else:\n            raise ValueError(\n                f\"Unsupported recover_mode: {recover_mode}, must be 'bybasins' or 'byforecast' or 'byensembles'\"\n            )\n    elif evaluator[\"eval_way\"] == \"floodevent\":\n        # For flood event evaluation, stride is not typically used, but we set it to 1 for consistency\n        stride = evaluator.get(\"stride\", 1)\n        pred = _recover_samples_to_continuous_by_floodevent(\n            data_shape,\n            valorte_data_loader,\n            stride,\n            hindcast_output_window,\n            output,\n        )\n        obs = _recover_samples_to_continuous_by_floodevent(\n            data_shape,\n            valorte_data_loader,\n            stride,\n            hindcast_output_window,\n            labels,\n        )\n    else:\n        raise ValueError(\"eval_way should be rolling or 1pace\")\n\n    # pace_idx = np.nan\n    recover_mode = evaluator.get(\"recover_mode\")\n    valte_dataset = valorte_data_loader.dataset\n    # \u68c0\u67e5\u6570\u636e\u7ef4\u5ea6\u5e76\u8fdb\u884c\u9002\u5f53\u5904\u7406\n    if pred.ndim == 4:\n        # \u5982\u679c\u662f\u56db\u7ef4\u6570\u636e\uff0c\u9700\u8981\u6839\u636e\u8bc4\u4f30\u65b9\u5f0f\u9009\u62e9\u5408\u9002\u7684\u5904\u7406\u65b9\u6cd5\n        if evaluator[\"eval_way\"] == \"1pace\" and \"pace_idx\" in evaluator:\n            # \u5bf9\u4e8e1pace\u6a21\u5f0f\uff0c\u9009\u62e9\u7279\u5b9a\u7684\u9884\u6d4b\u6b65\u957f\n            pace_idx = evaluator[\"pace_idx\"]\n            # \u9009\u62e9\u7279\u5b9a\u9884\u6d4b\u6b65\u957f\u7684\u6570\u636e\n            pred_3d = pred[:, :, pace_idx, :]\n            obs_3d = obs[:, :, pace_idx, :]\n            preds_xr = valte_dataset.denormalize(pred_3d, pace_idx)\n            obss_xr = valte_dataset.denormalize(obs_3d, pace_idx)\n        elif evaluator[\"eval_way\"] == \"rolling\" and recover_mode == \"byforecast\":\n            # \u5bf9\u4e8ebyforecast\u6a21\u5f0f\uff0c\u9700\u8981\u7279\u6b8a\u5904\u7406\n            # \u521b\u5efa\u4e00\u4e2a\u5217\u8868\u5b58\u50a8\u6bcf\u4e2a\u9884\u6d4b\u6b65\u957f\u7684\u7ed3\u679c\n            preds_xr_list = []\n            obss_xr_list = []\n            for i in range(pred.shape[2]):\n                pred_3d = pred[:, :, i, :]\n                obs_3d = obs[:, :, i, :]\n                the_array_pred_ = np.full(target_data.shape, np.nan)\n                the_array_obs_ = np.full(target_data.shape, np.nan)\n                start = rho + i  # TODO:need check\n                end = start + pred_3d.shape[1]\n                assert end &lt;= the_array_pred_.shape[1]\n                the_array_pred_[:, start:end, :] = pred_3d\n                the_array_obs_[:, start:end, :] = obs_3d\n                preds_xr_list.append(valte_dataset.denormalize(the_array_pred_, i))\n                obss_xr_list.append(valte_dataset.denormalize(the_array_obs_, i))\n            # \u5408\u5e76\u7ed3\u679c\n            # preds_xr = xr.concat(preds_xr_list, dim=\"horizon\")\n            # obss_xr = xr.concat(obss_xr_list, dim=\"horizon\")\n            return obss_xr_list, preds_xr_list\n        elif evaluator[\"eval_way\"] == \"rolling\" and recover_mode == \"bybasins\":\n            # \u5bf9\u4e8e\u5176\u4ed6\u60c5\u51b5\uff0c\u53ef\u4ee5\u8003\u8651\u5c06\u56db\u7ef4\u6570\u636e\u8f6c\u6362\u4e3a\u4e09\u7ef4\n            # \u4f8b\u5982\uff0c\u53d6\u6700\u540e\u4e00\u4e2a\u9884\u6d4b\u6b65\u957f\n            preds_xr_list = []\n            obss_xr_list = []\n            for i in range(pred.shape[0]):\n                pred_3d = pred[i, :, :, :]\n                obs_3d = obs[i, :, :, :]\n                selected_data = target_scaler.data_target\n                the_array_pred_ = np.full(selected_data.shape, np.nan)\n                the_array_obs_ = np.full(selected_data.shape, np.nan)\n                start = rho  # TODO:need check\n                end = start + pred_3d.shape[1]  # \u81ea\u52a8\u8ba1\u7b97\u586b\u5145\u7684\u7ed3\u675f\u4f4d\u7f6e\n\n                # \u68c0\u67e5\u662f\u5426\u8d8a\u754c\uff08\u53ef\u9009\uff09\n                assert end &lt;= the_array_pred_.shape[1]  # \"\u586b\u5145\u8303\u56f4\u8d85\u51fa\u76ee\u6807\u6570\u7ec4\u7684\u8fb9\u754c\"\n\n                # \u6267\u884c\u586b\u5145\n                the_array_pred_[:, start:end, :] = pred_3d\n                the_array_obs_[:, start:end, :] = obs_3d\n\n                preds_xr = valte_dataset.denormalize(the_array_pred_, -1)\n                obss_xr = valte_dataset.denormalize(the_array_obs_, -1)\n    else:\n        # for 3d data, directly process\n        # TODO: maybe need more test for the pace_idx case\n        preds_xr = valte_dataset.denormalize(pred)\n        obss_xr = valte_dataset.denormalize(obs)\n\n    def _align_and_order(_obs, _pred):\n        # \u5bf9\u9f50\u5230\u516c\u5171 (basin,time,variable) \u7684\u4ea4\u96c6\uff0c\u907f\u514d outer \u5f15\u5165 NaN\n        _obs, _pred = xr.align(_obs, _pred, join=\"inner\")\n        # time \u7ef4\u4e3a\u7a7a\uff08\u65e0\u4ea4\u96c6\uff09\u65f6\u76f4\u63a5\u629b\u9519\uff0c\u907f\u514d\u8fdb\u5165 nanmean\n        if _obs.sizes.get(\"time\", 0) == 0:\n            raise ValueError(\n                \"No overlapping timestamps between observations and predictions \"\n                f\"(obs.time len={_obs.sizes.get('time',0)}, pred.time len={_pred.sizes.get('time',0)}).\"\n            )\n        # \u6309\u65f6\u95f4\u6392\u5e8f\uff08\u4fdd\u9669\uff09\n        if \"time\" in _obs.dims:\n            _obs = _obs.sortby(\"time\")\n        if \"time\" in _pred.dims:\n            _pred = _pred.sortby(\"time\")\n        # \u89c4\u8303\u7ef4\u5ea6\u987a\u5e8f\uff08\u82e5\u5b58\u5728\uff09\n        wanted = [d for d in (\"basin\", \"time\", \"variable\") if d in _obs.dims]\n        _obs = _obs.transpose(*wanted, missing_dims=\"ignore\")\n        _pred = _pred.transpose(*wanted, missing_dims=\"ignore\")\n        return _obs, _pred\n\n    # \u5355\u5bf9\u8c61 vs \u5217\u8868\u5206\u522b\u5904\u7406\n    if preds_xr is not None and obss_xr is not None:\n        obss_xr, preds_xr = _align_and_order(obss_xr, preds_xr)\n        return obss_xr, preds_xr\n\n    elif preds_xr_list is not None and obss_xr_list is not None:\n        obss_aligned, preds_aligned = [], []\n        for _o, _p in zip(obss_xr_list, preds_xr_list):\n            _o2, _p2 = _align_and_order(_o, _p)\n            obss_aligned.append(_o2)\n            preds_aligned.append(_p2)\n        return obss_aligned, preds_aligned\n\n    else:\n        # \u7406\u8bba\u4e0d\u5e94\u8d70\u5230\u8fd9\n        raise RuntimeError(\"Failed to build preds_xr / obss_xr for evaluation.\")\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.gnn_collate_fn","title":"<code>gnn_collate_fn(batch)</code>","text":"<p>Custom collate function for GNN datasets that handles variable-sized graphs.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <p>A list of samples, where each sample is a tuple of (sxc, y, edge_index, edge_weight).</p> required <p>Returns:</p> Type Description <code>A list containing batched tensors</code> <p>[batched_sxc, batched_y, batched_edge_index, batched_edge_weight, batch_vector]</p> Source code in <code>torchhydro/trainers/train_utils.py</code> <pre><code>def gnn_collate_fn(batch):\n    \"\"\"Custom collate function for GNN datasets that handles variable-sized graphs.\n\n    Args:\n        batch: A list of samples, where each sample is a tuple of\n            (sxc, y, edge_index, edge_weight).\n\n    Returns:\n        A list containing batched tensors:\n        [batched_sxc, batched_y, batched_edge_index, batched_edge_weight, batch_vector]\n    \"\"\"\n    import torch\n\n    if len(batch) == 0:\n        return []\n\n    # Unpack the batch\n    sxc_list, y_list, edge_index_list, edge_weight_list = zip(*batch)\n\n    # Batch the target values (y) - these should have the same shape\n    batched_y = torch.stack(y_list, dim=0)  # [batch_size, forecast_length, output_dim]\n\n    # Find the maximum number of nodes in this batch\n    max_num_nodes = max(sxc.shape[0] for sxc in sxc_list)\n\n    # Get dimensions\n    batch_size = len(sxc_list)\n    seq_length = sxc_list[0].shape[1]\n    feature_dim = sxc_list[0].shape[2]\n\n    # Create padded tensor for node features\n    batched_sxc = torch.zeros(batch_size, max_num_nodes, seq_length, feature_dim)\n\n    # Create batched edge indices and weights\n    # For each graph in the batch, we need to offset node indices\n    batched_edge_index = []\n    batched_edge_weight = []\n    batch_vector = []\n    node_offset = 0\n    for i, (sxc, edge_index, edge_weight) in enumerate(\n        zip(sxc_list, edge_index_list, edge_weight_list)\n    ):\n        num_nodes = sxc.shape[0]\n        # Fill the padded tensor with actual node features\n        batched_sxc[i, :num_nodes] = sxc\n        # For edge indices, we need to offset by node_offset to make them unique across batch\n        if edge_index.numel() &gt; 0:\n            if edge_index.max() &gt;= num_nodes:\n                print(\n                    f\"Warning: Graph {i} has edge indices {edge_index.max().item()} &gt;= num_nodes {num_nodes}\"\n                )\n                valid_mask = (edge_index[0] &lt; num_nodes) &amp; (edge_index[1] &lt; num_nodes)\n                edge_index = edge_index[:, valid_mask]\n                edge_weight = edge_weight[valid_mask]\n            if edge_index.numel() &gt; 0:\n                offset_edge_index = edge_index + node_offset\n                batched_edge_index.append(offset_edge_index)\n                batched_edge_weight.append(edge_weight)\n        # batch_vector: for each node in this graph, assign batch index i\n        batch_vector.append(torch.full((num_nodes,), i, dtype=torch.long))\n        node_offset += num_nodes\n    # Concatenate edge indices and weights if they exist\n    if batched_edge_index:\n        batched_edge_index = torch.cat(batched_edge_index, dim=1)  # [2, total_edges]\n        batched_edge_weight = torch.cat(batched_edge_weight, dim=0)  # [total_edges]\n    else:\n        batched_edge_index = torch.empty((2, 0), dtype=torch.long)\n        batched_edge_weight = torch.empty(0)\n    batch_vector = torch.cat(batch_vector, dim=0)  # [total_nodes]\n    return [\n        batched_sxc,\n        batched_y,\n        batched_edge_index,\n        batched_edge_weight,\n        batch_vector,\n    ]\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.model_infer","title":"<code>model_infer(seq_first, device, model, batch, variable_length_cfgs=None, return_key=None)</code>","text":"<p>Unified model inference function with variable length support</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.model_infer--parameters","title":"Parameters","text":"<p>seq_first : bool     if True, the input data is sequence first device : torch.device     cpu or gpu model : torch.nn.Module     the model batch : tuple or list     batch data from collate_fn or dataset variable_length_cfgs : dict, optional     variable length configuration containing mask settings return_key : str, optional     when model returns a dict, choose which key (e.g., \"f2\") to return.     if None, defaults to the last frequency (max key).</p> Source code in <code>torchhydro/trainers/train_utils.py</code> <pre><code>def model_infer(\n    seq_first, device, model, batch, variable_length_cfgs=None, return_key=None\n):\n    \"\"\"\n    Unified model inference function with variable length support\n\n    Parameters\n    ----------\n    seq_first : bool\n        if True, the input data is sequence first\n    device : torch.device\n        cpu or gpu\n    model : torch.nn.Module\n        the model\n    batch : tuple or list\n        batch data from collate_fn or dataset\n    variable_length_cfgs : dict, optional\n        variable length configuration containing mask settings\n    return_key : str, optional\n        when model returns a dict, choose which key (e.g., \"f2\") to return.\n        if None, defaults to the last frequency (max key).\n    \"\"\"\n    result = get_masked_tensors(variable_length_cfgs, batch, seq_first)\n\n    # --- unpack inputs ---\n    if len(result) == 9:\n        (\n            xs,\n            ys,\n            edge_index,\n            edge_weight,\n            batch_vector,\n            xs_mask,\n            ys_mask,\n            xs_lens,\n            ys_lens,\n        ) = result\n    elif len(result) == 8:\n        xs, ys, edge_index, edge_weight, xs_mask, ys_mask, xs_lens, ys_lens = result\n        batch_vector = None\n    else:\n        xs, ys, xs_mask, ys_mask, xs_lens, ys_lens = result\n        edge_index = edge_weight = batch_vector = None\n\n    # --- move xs to device ---\n    if isinstance(xs, list):\n        xs = [\n            (\n                x.permute(1, 0, 2).to(device)\n                if seq_first and x.ndim == 3\n                else x.to(device)\n            )\n            for x in xs\n        ]\n    else:\n        xs = [\n            (\n                xs.permute(1, 0, 2).to(device)\n                if seq_first and xs.ndim == 3\n                else xs.to(device)\n            )\n        ]\n\n    # --- move ys to device ---\n    if ys is not None:\n        ys = (\n            ys.permute(1, 0, 2).to(device)\n            if seq_first and ys.ndim == 3\n            else ys.to(device)\n        )\n\n    # --- move graph data ---\n    if edge_index is not None:\n        edge_index = edge_index.to(device)\n    if edge_weight is not None:\n        edge_weight = edge_weight.to(device)\n    if batch_vector is not None:\n        batch_vector = batch_vector.to(device)\n\n    # --- forward ---\n    if xs_mask is not None and ys_mask is not None:\n        if edge_index is not None and edge_weight is not None:\n            output = model(\n                *xs,\n                edge_index=edge_index,\n                edge_weight=edge_weight,\n                batch_vector=batch_vector,\n                mask=xs_mask,\n                seq_lengths=xs_lens,\n            )\n        else:\n            output = model(*xs, mask=xs_mask, seq_lengths=xs_lens)\n    else:\n        if edge_index is not None and edge_weight is not None:\n            output = model(\n                *xs,\n                edge_index=edge_index,\n                edge_weight=edge_weight,\n                batch_vector=batch_vector,\n            )\n        else:\n            output = model(*xs)\n\n    # --- handle model outputs ---\n    if isinstance(output, tuple):\n        output = output[0]\n\n    if isinstance(output, dict):\n        # \u9ed8\u8ba4\u53d6\u6700\u9ad8\u9891\u7684\u8f93\u51fa\n        if return_key is None:\n            return_key = sorted(output.keys())[-1]  # e.g., \"f2\"\n        if return_key not in output:\n            raise KeyError(\n                f\"Model returned keys {list(output.keys())}, but return_key='{return_key}' not found\"\n            )\n        output = output[return_key]\n\n    if ys_mask is not None:\n        ys = ys.masked_fill(ys_mask == 0, torch.nan)\n\n    # --- seq_first transpose back ---\n    if seq_first:\n        output = output.transpose(0, 1)\n        if ys is not None:\n            ys = ys.transpose(0, 1)\n\n    return ys, output\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.torch_single_train","title":"<code>torch_single_train(model, opt, criterion, data_loader, device=None, **kwargs)</code>","text":"<p>Training function for one epoch</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.torch_single_train--parameters","title":"Parameters","text":"<p>model     a PyTorch model inherit from nn.Module opt     optimizer function from PyTorch optim.Optimizer criterion     loss function data_loader     object for loading data to the model device     where we put the tensors and models</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.torch_single_train--returns","title":"Returns","text":"<p>tuple(torch.Tensor, int)     loss of this epoch and number of all iterations</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.torch_single_train--raises","title":"Raises","text":"<p>ValueError     if nan exits, raise a ValueError</p> Source code in <code>torchhydro/trainers/train_utils.py</code> <pre><code>def torch_single_train(\n    model,\n    opt: optim.Optimizer,\n    criterion,\n    data_loader: DataLoader,\n    device=None,\n    **kwargs,\n):\n    \"\"\"\n    Training function for one epoch\n\n    Parameters\n    ----------\n    model\n        a PyTorch model inherit from nn.Module\n    opt\n        optimizer function from PyTorch optim.Optimizer\n    criterion\n        loss function\n    data_loader\n        object for loading data to the model\n    device\n        where we put the tensors and models\n\n    Returns\n    -------\n    tuple(torch.Tensor, int)\n        loss of this epoch and number of all iterations\n\n    Raises\n    --------\n    ValueError\n        if nan exits, raise a ValueError\n    \"\"\"\n    # we will set model.eval() in the validation function so here we should set model.train()\n    model.train()\n    n_iter_ep = 0\n    running_loss = 0.0\n    which_first_tensor = kwargs[\"which_first_tensor\"]\n    seq_first = which_first_tensor != \"batch\"\n    variable_length_cfgs = data_loader.dataset.training_cfgs.get(\n        \"variable_length_cfgs\", None\n    )\n    pbar = tqdm(data_loader)\n\n    for _, batch in enumerate(pbar):\n        # mask handling is already done inside model_infer function\n        trg, output = model_infer(seq_first, device, model, batch, variable_length_cfgs)\n        loss = compute_loss(trg, output, criterion, **kwargs)\n        if loss &gt; 100:\n            print(\"Warning: high loss detected\")\n        if torch.isnan(loss):\n            raise ValueError(\"nan loss detected\")\n            # continue\n        loss.backward()  # Backpropagate to compute the current gradient\n        opt.step()  # Update network parameters based on gradients\n        model.zero_grad()  # clear gradient\n        if loss == float(\"inf\"):\n            raise ValueError(\n                \"Error infinite loss detected. Try normalizing data or performing interpolation\"\n            )\n        running_loss += loss.item()\n        n_iter_ep += 1\n    if n_iter_ep == 0:\n        raise ValueError(\n            \"All batch computations of loss result in NAN. Please check the data.\"\n        )\n    total_loss = running_loss / float(n_iter_ep)\n    return total_loss, n_iter_ep\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.varied_length_collate_fn","title":"<code>varied_length_collate_fn(batch)</code>","text":"<p>Collate function for variable length training</p> <p>This function is automatically used by DataLoader when variable_length_cfgs[\"use_variable_length\"] is True. It pads sequences to the same length and generates corresponding masks.</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.varied_length_collate_fn--parameters","title":"Parameters","text":"<p>batch : list of tuples     The batch data after the dataset getitem method</p>"},{"location":"api/trainers/#torchhydro.trainers.train_utils.varied_length_collate_fn--returns","title":"Returns","text":"<p>list     [xs_pad, ys_pad, xs_lens, ys_lens, xs_mask, ys_mask]     - xs_pad: padded input sequences [batch, max_seq_len, input_dim]     - ys_pad: padded output sequences [batch, max_seq_len, output_dim]     - xs_lens: original sequence lengths for input     - ys_lens: original sequence lengths for output     - xs_mask: valid position mask for input [batch, max_seq_len]     - ys_mask: valid position mask for output [batch, max_seq_len]</p> Source code in <code>torchhydro/trainers/train_utils.py</code> <pre><code>def varied_length_collate_fn(batch):\n    \"\"\"Collate function for variable length training\n\n    This function is automatically used by DataLoader when variable_length_cfgs[\"use_variable_length\"] is True.\n    It pads sequences to the same length and generates corresponding masks.\n\n    Parameters\n    ----------\n    batch : list of tuples\n        The batch data after the dataset __getitem__ method\n\n    Returns\n    -------\n    list\n        [xs_pad, ys_pad, xs_lens, ys_lens, xs_mask, ys_mask]\n        - xs_pad: padded input sequences [batch, max_seq_len, input_dim]\n        - ys_pad: padded output sequences [batch, max_seq_len, output_dim]\n        - xs_lens: original sequence lengths for input\n        - ys_lens: original sequence lengths for output\n        - xs_mask: valid position mask for input [batch, max_seq_len]\n        - ys_mask: valid position mask for output [batch, max_seq_len]\n    \"\"\"\n\n    xs, ys = zip(*batch)\n    # sometimes x is a tuple like in dpl dataset, then we can get the shape of the first element as the length\n    xs_lens = [x[0].shape[0] if type(x) in [tuple, list] else x.shape[0] for x in xs]\n    ys_lens = [y[0].shape[0] if type(y) in [tuple, list] else y.shape[0] for y in ys]\n    # if all ys_lens are the same, use default collate_fn to create tensors\n    if len(set(ys_lens)) == 1 and len(set(xs_lens)) == 1:\n        xs_tensor = default_collate(xs)\n        ys_tensor = default_collate(ys)\n        return [xs_tensor, ys_tensor, None, None, None, None]\n\n    # pad the batch data with padding value 0\n    xs_pad = pad_sequence(xs, batch_first=True, padding_value=0)\n    ys_pad = pad_sequence(ys, batch_first=True, padding_value=0)\n\n    # generate the mask for the batch data\n    # xs_mask: [batch_size, max_seq_len] or [batch_size, max_seq_len, 1]\n    batch_size = len(xs_lens)\n    max_xs_len = max(xs_lens)\n    max_ys_len = max(ys_lens)\n\n    # create the mask for the input sequence (True for valid positions, False for padding positions)\n    xs_mask = torch.zeros(batch_size, max_xs_len, dtype=torch.bool)\n    for i, length in enumerate(xs_lens):\n        xs_mask[i, :length] = True\n\n    # create the mask for the output sequence\n    ys_mask = torch.zeros(batch_size, max_ys_len, dtype=torch.bool)\n    for i, length in enumerate(ys_lens):\n        ys_mask[i, :length] = True\n\n    return [\n        xs_pad,\n        ys_pad,\n        xs_lens,\n        ys_lens,\n        xs_mask,\n        ys_mask,\n    ]\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.trainer","title":"<code>trainer</code>","text":"<p>Author: Wenyu Ouyang Date: 2021-12-05 11:21:58 LastEditTime: 2025-11-08 16:02:09 LastEditors: Wenyu Ouyang Description: Main function for training and testing FilePath:       orchhydro       orchhydro       rainers rainer.py Copyright (c) 2021-2022 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/trainers/#torchhydro.trainers.trainer.ensemble_train_and_evaluate","title":"<code>ensemble_train_and_evaluate(cfgs)</code>","text":"<p>Function to train and test for ensemble models</p>"},{"location":"api/trainers/#torchhydro.trainers.trainer.ensemble_train_and_evaluate--parameters","title":"Parameters","text":"<p>cfgs     Dictionary containing all configs needed to run the model</p>"},{"location":"api/trainers/#torchhydro.trainers.trainer.ensemble_train_and_evaluate--returns","title":"Returns","text":"<p>None</p> Source code in <code>torchhydro/trainers/trainer.py</code> <pre><code>def ensemble_train_and_evaluate(cfgs: Dict):\n    \"\"\"\n    Function to train and test for ensemble models\n\n    Parameters\n    ----------\n    cfgs\n        Dictionary containing all configs needed to run the model\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # for basins and models\n    ensemble = cfgs[\"training_cfgs\"][\"ensemble\"]\n    if not ensemble:\n        raise ValueError(\n            \"ensemble should be True, otherwise should use train_and_evaluate rather than ensemble_train_and_evaluate\"\n        )\n    ensemble_items = cfgs[\"training_cfgs\"][\"ensemble_items\"]\n    number_of_items = len(ensemble_items)\n    if number_of_items == 0:\n        raise ValueError(\"ensemble_items should not be empty\")\n    keys_list = list(ensemble_items.keys())\n    if \"kfold\" in keys_list:\n        _trans_kfold_to_periods(cfgs, ensemble_items, \"kfold\")\n    _nested_loop_train_and_evaluate(keys_list, 0, ensemble_items, cfgs)\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.trainer.set_random_seed","title":"<code>set_random_seed(seed)</code>","text":"<p>Set a random seed to guarantee the reproducibility</p>"},{"location":"api/trainers/#torchhydro.trainers.trainer.set_random_seed--parameters","title":"Parameters","text":"<p>seed     a number</p>"},{"location":"api/trainers/#torchhydro.trainers.trainer.set_random_seed--returns","title":"Returns","text":"<p>None</p> Source code in <code>torchhydro/trainers/trainer.py</code> <pre><code>def set_random_seed(seed):\n    \"\"\"\n    Set a random seed to guarantee the reproducibility\n\n    Parameters\n    ----------\n    seed\n        a number\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # print(\"Random seed:\", seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n</code></pre>"},{"location":"api/trainers/#torchhydro.trainers.trainer.train_and_evaluate","title":"<code>train_and_evaluate(cfgs)</code>","text":"<p>Function to train and test a Model</p>"},{"location":"api/trainers/#torchhydro.trainers.trainer.train_and_evaluate--parameters","title":"Parameters","text":"<p>cfgs     Dictionary containing all configs needed to run the model</p>"},{"location":"api/trainers/#torchhydro.trainers.trainer.train_and_evaluate--returns","title":"Returns","text":"<p>None</p> Source code in <code>torchhydro/trainers/trainer.py</code> <pre><code>def train_and_evaluate(cfgs: Dict):\n    \"\"\"\n    Function to train and test a Model\n\n    Parameters\n    ----------\n    cfgs\n        Dictionary containing all configs needed to run the model\n\n    Returns\n    -------\n    None\n    \"\"\"\n    random_seed = cfgs[\"training_cfgs\"][\"random_seed\"]\n    set_random_seed(random_seed)\n    resulter = Resulter(cfgs)\n    deephydro = _get_deep_hydro(cfgs)\n    # if train_mode is False, we only evaluate the model\n    train_mode = deephydro.cfgs[\"training_cfgs\"][\"train_mode\"]\n    # but if train_mode is True, we still need some conditions to train the model\n    continue_train = deephydro.cfgs[\"model_cfgs\"][\"continue_train\"]\n    is_transfer_learning = deephydro.cfgs[\"model_cfgs\"][\"model_type\"] == \"TransLearn\"\n    is_train = train_mode and (\n        (deephydro.weight_path is not None and (continue_train or is_transfer_learning))\n        or (deephydro.weight_path is None)\n    )\n    if is_train:\n        deephydro.model_train()\n    preds, obss = deephydro.model_evaluate()\n    resulter.save_cfg(deephydro.cfgs)\n    resulter.save_result(preds, obss)\n    resulter.eval_result(preds, obss)\n</code></pre>"}]}